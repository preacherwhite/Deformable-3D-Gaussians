{
  "best_metric": 10.7188,
  "best_model_checkpoint": "./results/checkpoint-2846",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2846,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007027406886858749,
      "grad_norm": 1.6860004663467407,
      "learning_rate": 9.993675333801827e-05,
      "loss": -1.2921,
      "step": 10
    },
    {
      "epoch": 0.014054813773717497,
      "grad_norm": 1.5223720073699951,
      "learning_rate": 9.98664792691497e-05,
      "loss": -1.3807,
      "step": 20
    },
    {
      "epoch": 0.02108222066057625,
      "grad_norm": 0.8838928937911987,
      "learning_rate": 9.97962052002811e-05,
      "loss": -1.4051,
      "step": 30
    },
    {
      "epoch": 0.028109627547434995,
      "grad_norm": 0.9422695636749268,
      "learning_rate": 9.97259311314125e-05,
      "loss": -1.4302,
      "step": 40
    },
    {
      "epoch": 0.035137034434293744,
      "grad_norm": 2.1020593643188477,
      "learning_rate": 9.965565706254392e-05,
      "loss": -1.4926,
      "step": 50
    },
    {
      "epoch": 0.0421644413211525,
      "grad_norm": 1.212904453277588,
      "learning_rate": 9.958538299367534e-05,
      "loss": -1.5129,
      "step": 60
    },
    {
      "epoch": 0.049191848208011243,
      "grad_norm": 1.2417880296707153,
      "learning_rate": 9.951510892480675e-05,
      "loss": -1.5735,
      "step": 70
    },
    {
      "epoch": 0.05621925509486999,
      "grad_norm": 1.4859538078308105,
      "learning_rate": 9.944483485593816e-05,
      "loss": -1.657,
      "step": 80
    },
    {
      "epoch": 0.06324666198172874,
      "grad_norm": 2.792626142501831,
      "learning_rate": 9.937456078706959e-05,
      "loss": -1.772,
      "step": 90
    },
    {
      "epoch": 0.07027406886858749,
      "grad_norm": 4.062569618225098,
      "learning_rate": 9.930428671820099e-05,
      "loss": -2.0051,
      "step": 100
    },
    {
      "epoch": 0.07730147575544624,
      "grad_norm": 10.54062557220459,
      "learning_rate": 9.924104005621925e-05,
      "loss": -2.3975,
      "step": 110
    },
    {
      "epoch": 0.084328882642305,
      "grad_norm": Infinity,
      "learning_rate": 9.918482080112439e-05,
      "loss": -2.6963,
      "step": 120
    },
    {
      "epoch": 0.09135628952916373,
      "grad_norm": 11.13524341583252,
      "learning_rate": 9.91145467322558e-05,
      "loss": -2.8551,
      "step": 130
    },
    {
      "epoch": 0.09838369641602249,
      "grad_norm": 27.765460968017578,
      "learning_rate": 9.904427266338721e-05,
      "loss": -3.1458,
      "step": 140
    },
    {
      "epoch": 0.10541110330288124,
      "grad_norm": 50.406158447265625,
      "learning_rate": 9.897399859451863e-05,
      "loss": -3.0714,
      "step": 150
    },
    {
      "epoch": 0.11243851018973998,
      "grad_norm": 7.153792858123779,
      "learning_rate": 9.890372452565004e-05,
      "loss": -3.0842,
      "step": 160
    },
    {
      "epoch": 0.11946591707659873,
      "grad_norm": 19.641448974609375,
      "learning_rate": 9.883345045678145e-05,
      "loss": -3.6879,
      "step": 170
    },
    {
      "epoch": 0.12649332396345747,
      "grad_norm": Infinity,
      "learning_rate": 9.877020379479973e-05,
      "loss": -3.393,
      "step": 180
    },
    {
      "epoch": 0.13352073085031624,
      "grad_norm": 50.81373596191406,
      "learning_rate": 9.869992972593114e-05,
      "loss": -3.653,
      "step": 190
    },
    {
      "epoch": 0.14054813773717498,
      "grad_norm": 45.54872512817383,
      "learning_rate": 9.862965565706254e-05,
      "loss": -3.8452,
      "step": 200
    },
    {
      "epoch": 0.14757554462403374,
      "grad_norm": 20.347394943237305,
      "learning_rate": 9.855938158819396e-05,
      "loss": -4.0533,
      "step": 210
    },
    {
      "epoch": 0.15460295151089248,
      "grad_norm": 25.921110153198242,
      "learning_rate": 9.848910751932538e-05,
      "loss": -4.2427,
      "step": 220
    },
    {
      "epoch": 0.16163035839775122,
      "grad_norm": 13.012264251708984,
      "learning_rate": 9.841883345045678e-05,
      "loss": -4.3694,
      "step": 230
    },
    {
      "epoch": 0.16865776528461,
      "grad_norm": 60.69106674194336,
      "learning_rate": 9.83485593815882e-05,
      "loss": -4.5216,
      "step": 240
    },
    {
      "epoch": 0.17568517217146873,
      "grad_norm": 18.223241806030273,
      "learning_rate": 9.827828531271961e-05,
      "loss": -4.592,
      "step": 250
    },
    {
      "epoch": 0.18271257905832747,
      "grad_norm": 16.20248794555664,
      "learning_rate": 9.820801124385103e-05,
      "loss": -4.5475,
      "step": 260
    },
    {
      "epoch": 0.18973998594518623,
      "grad_norm": 54.024654388427734,
      "learning_rate": 9.813773717498243e-05,
      "loss": -4.8331,
      "step": 270
    },
    {
      "epoch": 0.19676739283204497,
      "grad_norm": 21.992366790771484,
      "learning_rate": 9.806746310611385e-05,
      "loss": -4.6675,
      "step": 280
    },
    {
      "epoch": 0.2037947997189037,
      "grad_norm": Infinity,
      "learning_rate": 9.800421644413213e-05,
      "loss": -4.5446,
      "step": 290
    },
    {
      "epoch": 0.21082220660576248,
      "grad_norm": 58.36254119873047,
      "learning_rate": 9.793394237526353e-05,
      "loss": -4.593,
      "step": 300
    },
    {
      "epoch": 0.21784961349262122,
      "grad_norm": 33.2662467956543,
      "learning_rate": 9.786366830639495e-05,
      "loss": -4.8962,
      "step": 310
    },
    {
      "epoch": 0.22487702037947996,
      "grad_norm": 43.39387512207031,
      "learning_rate": 9.779339423752636e-05,
      "loss": -4.9067,
      "step": 320
    },
    {
      "epoch": 0.23190442726633873,
      "grad_norm": 31.868099212646484,
      "learning_rate": 9.772312016865777e-05,
      "loss": -4.8554,
      "step": 330
    },
    {
      "epoch": 0.23893183415319746,
      "grad_norm": 32.99828338623047,
      "learning_rate": 9.765284609978918e-05,
      "loss": -4.7624,
      "step": 340
    },
    {
      "epoch": 0.24595924104005623,
      "grad_norm": 22.345361709594727,
      "learning_rate": 9.75825720309206e-05,
      "loss": -4.3312,
      "step": 350
    },
    {
      "epoch": 0.25298664792691494,
      "grad_norm": 19.150009155273438,
      "learning_rate": 9.751229796205202e-05,
      "loss": -5.3253,
      "step": 360
    },
    {
      "epoch": 0.2600140548137737,
      "grad_norm": 73.74205017089844,
      "learning_rate": 9.744202389318342e-05,
      "loss": -4.8292,
      "step": 370
    },
    {
      "epoch": 0.2670414617006325,
      "grad_norm": 66.38163757324219,
      "learning_rate": 9.737174982431483e-05,
      "loss": -5.1897,
      "step": 380
    },
    {
      "epoch": 0.27406886858749124,
      "grad_norm": 49.51173782348633,
      "learning_rate": 9.730147575544625e-05,
      "loss": -5.6284,
      "step": 390
    },
    {
      "epoch": 0.28109627547434995,
      "grad_norm": 35.72504806518555,
      "learning_rate": 9.723120168657765e-05,
      "loss": -5.4332,
      "step": 400
    },
    {
      "epoch": 0.2881236823612087,
      "grad_norm": 35.065677642822266,
      "learning_rate": 9.716092761770907e-05,
      "loss": -5.3912,
      "step": 410
    },
    {
      "epoch": 0.2951510892480675,
      "grad_norm": 64.38773345947266,
      "learning_rate": 9.709065354884049e-05,
      "loss": -5.3189,
      "step": 420
    },
    {
      "epoch": 0.3021784961349262,
      "grad_norm": 48.378482818603516,
      "learning_rate": 9.702037947997189e-05,
      "loss": -5.4603,
      "step": 430
    },
    {
      "epoch": 0.30920590302178497,
      "grad_norm": 23.19247055053711,
      "learning_rate": 9.69501054111033e-05,
      "loss": -5.4158,
      "step": 440
    },
    {
      "epoch": 0.31623330990864373,
      "grad_norm": 64.5969467163086,
      "learning_rate": 9.687983134223472e-05,
      "loss": -5.1215,
      "step": 450
    },
    {
      "epoch": 0.32326071679550245,
      "grad_norm": 44.53466033935547,
      "learning_rate": 9.680955727336614e-05,
      "loss": -5.9027,
      "step": 460
    },
    {
      "epoch": 0.3302881236823612,
      "grad_norm": 30.518274307250977,
      "learning_rate": 9.673928320449754e-05,
      "loss": -5.459,
      "step": 470
    },
    {
      "epoch": 0.33731553056922,
      "grad_norm": 101.69618225097656,
      "learning_rate": 9.666900913562896e-05,
      "loss": -5.2915,
      "step": 480
    },
    {
      "epoch": 0.3443429374560787,
      "grad_norm": 71.34317016601562,
      "learning_rate": 9.659873506676038e-05,
      "loss": -4.9095,
      "step": 490
    },
    {
      "epoch": 0.35137034434293746,
      "grad_norm": 84.34651184082031,
      "learning_rate": 9.652846099789178e-05,
      "loss": -5.6687,
      "step": 500
    },
    {
      "epoch": 0.3583977512297962,
      "grad_norm": 42.30650329589844,
      "learning_rate": 9.64581869290232e-05,
      "loss": -5.7226,
      "step": 510
    },
    {
      "epoch": 0.36542515811665494,
      "grad_norm": 79.91998291015625,
      "learning_rate": 9.638791286015461e-05,
      "loss": -5.7669,
      "step": 520
    },
    {
      "epoch": 0.3724525650035137,
      "grad_norm": 63.55778503417969,
      "learning_rate": 9.631763879128603e-05,
      "loss": -5.9739,
      "step": 530
    },
    {
      "epoch": 0.37947997189037247,
      "grad_norm": 23.731168746948242,
      "learning_rate": 9.624736472241743e-05,
      "loss": -5.8008,
      "step": 540
    },
    {
      "epoch": 0.3865073787772312,
      "grad_norm": 58.560874938964844,
      "learning_rate": 9.617709065354885e-05,
      "loss": -5.2713,
      "step": 550
    },
    {
      "epoch": 0.39353478566408995,
      "grad_norm": 48.000518798828125,
      "learning_rate": 9.610681658468026e-05,
      "loss": -5.8234,
      "step": 560
    },
    {
      "epoch": 0.4005621925509487,
      "grad_norm": 64.36412048339844,
      "learning_rate": 9.603654251581167e-05,
      "loss": -5.5284,
      "step": 570
    },
    {
      "epoch": 0.4075895994378074,
      "grad_norm": 53.271366119384766,
      "learning_rate": 9.596626844694308e-05,
      "loss": -5.5427,
      "step": 580
    },
    {
      "epoch": 0.4146170063246662,
      "grad_norm": 79.79280090332031,
      "learning_rate": 9.58959943780745e-05,
      "loss": -5.9536,
      "step": 590
    },
    {
      "epoch": 0.42164441321152496,
      "grad_norm": 150.04550170898438,
      "learning_rate": 9.582572030920592e-05,
      "loss": -5.8571,
      "step": 600
    },
    {
      "epoch": 0.42867182009838367,
      "grad_norm": 93.87496948242188,
      "learning_rate": 9.575544624033732e-05,
      "loss": -6.0092,
      "step": 610
    },
    {
      "epoch": 0.43569922698524244,
      "grad_norm": 117.1666259765625,
      "learning_rate": 9.568517217146874e-05,
      "loss": -6.087,
      "step": 620
    },
    {
      "epoch": 0.4427266338721012,
      "grad_norm": 50.84917449951172,
      "learning_rate": 9.561489810260015e-05,
      "loss": -6.1588,
      "step": 630
    },
    {
      "epoch": 0.4497540407589599,
      "grad_norm": 51.93496322631836,
      "learning_rate": 9.554462403373155e-05,
      "loss": -6.3485,
      "step": 640
    },
    {
      "epoch": 0.4567814476458187,
      "grad_norm": 32.84553527832031,
      "learning_rate": 9.547434996486297e-05,
      "loss": -6.2964,
      "step": 650
    },
    {
      "epoch": 0.46380885453267745,
      "grad_norm": 60.058799743652344,
      "learning_rate": 9.540407589599439e-05,
      "loss": -6.2155,
      "step": 660
    },
    {
      "epoch": 0.4708362614195362,
      "grad_norm": 32.49545669555664,
      "learning_rate": 9.533380182712579e-05,
      "loss": -6.1921,
      "step": 670
    },
    {
      "epoch": 0.47786366830639493,
      "grad_norm": 27.035213470458984,
      "learning_rate": 9.526352775825721e-05,
      "loss": -6.3931,
      "step": 680
    },
    {
      "epoch": 0.4848910751932537,
      "grad_norm": 81.44160461425781,
      "learning_rate": 9.519325368938862e-05,
      "loss": -6.3278,
      "step": 690
    },
    {
      "epoch": 0.49191848208011246,
      "grad_norm": 158.89794921875,
      "learning_rate": 9.512297962052004e-05,
      "loss": -6.2272,
      "step": 700
    },
    {
      "epoch": 0.4989458889669712,
      "grad_norm": 130.32418823242188,
      "learning_rate": 9.505270555165144e-05,
      "loss": -6.2297,
      "step": 710
    },
    {
      "epoch": 0.5059732958538299,
      "grad_norm": 74.09990692138672,
      "learning_rate": 9.498243148278286e-05,
      "loss": -6.4421,
      "step": 720
    },
    {
      "epoch": 0.5130007027406887,
      "grad_norm": 49.37168884277344,
      "learning_rate": 9.491215741391428e-05,
      "loss": -6.165,
      "step": 730
    },
    {
      "epoch": 0.5200281096275474,
      "grad_norm": 75.5392074584961,
      "learning_rate": 9.484188334504568e-05,
      "loss": -6.5732,
      "step": 740
    },
    {
      "epoch": 0.5270555165144062,
      "grad_norm": 45.11758041381836,
      "learning_rate": 9.47716092761771e-05,
      "loss": -6.3273,
      "step": 750
    },
    {
      "epoch": 0.534082923401265,
      "grad_norm": 95.66844940185547,
      "learning_rate": 9.470133520730851e-05,
      "loss": -6.5303,
      "step": 760
    },
    {
      "epoch": 0.5411103302881237,
      "grad_norm": 59.14422607421875,
      "learning_rate": 9.463106113843993e-05,
      "loss": -6.4698,
      "step": 770
    },
    {
      "epoch": 0.5481377371749825,
      "grad_norm": 68.3644790649414,
      "learning_rate": 9.456078706957133e-05,
      "loss": -6.5819,
      "step": 780
    },
    {
      "epoch": 0.5551651440618411,
      "grad_norm": 72.96343994140625,
      "learning_rate": 9.449754040758961e-05,
      "loss": -6.4701,
      "step": 790
    },
    {
      "epoch": 0.5621925509486999,
      "grad_norm": 47.393516540527344,
      "learning_rate": 9.442726633872103e-05,
      "loss": -6.5356,
      "step": 800
    },
    {
      "epoch": 0.5692199578355587,
      "grad_norm": 99.93370819091797,
      "learning_rate": 9.435699226985243e-05,
      "loss": -6.5243,
      "step": 810
    },
    {
      "epoch": 0.5762473647224174,
      "grad_norm": 168.73931884765625,
      "learning_rate": 9.428671820098383e-05,
      "loss": -6.6569,
      "step": 820
    },
    {
      "epoch": 0.5832747716092762,
      "grad_norm": 50.17271041870117,
      "learning_rate": 9.421644413211526e-05,
      "loss": -6.8741,
      "step": 830
    },
    {
      "epoch": 0.590302178496135,
      "grad_norm": 78.69847869873047,
      "learning_rate": 9.414617006324666e-05,
      "loss": -6.6368,
      "step": 840
    },
    {
      "epoch": 0.5973295853829936,
      "grad_norm": 89.92803192138672,
      "learning_rate": 9.407589599437808e-05,
      "loss": -6.7147,
      "step": 850
    },
    {
      "epoch": 0.6043569922698524,
      "grad_norm": 277.0621337890625,
      "learning_rate": 9.40056219255095e-05,
      "loss": -6.0667,
      "step": 860
    },
    {
      "epoch": 0.6113843991567112,
      "grad_norm": 86.7811508178711,
      "learning_rate": 9.393534785664091e-05,
      "loss": -6.646,
      "step": 870
    },
    {
      "epoch": 0.6184118060435699,
      "grad_norm": 64.3265151977539,
      "learning_rate": 9.386507378777232e-05,
      "loss": -6.1169,
      "step": 880
    },
    {
      "epoch": 0.6254392129304287,
      "grad_norm": 246.79415893554688,
      "learning_rate": 9.379479971890372e-05,
      "loss": -6.7719,
      "step": 890
    },
    {
      "epoch": 0.6324666198172875,
      "grad_norm": 291.45452880859375,
      "learning_rate": 9.372452565003515e-05,
      "loss": -6.7775,
      "step": 900
    },
    {
      "epoch": 0.6394940267041461,
      "grad_norm": 158.0874481201172,
      "learning_rate": 9.365425158116655e-05,
      "loss": -6.8678,
      "step": 910
    },
    {
      "epoch": 0.6465214335910049,
      "grad_norm": 98.92765045166016,
      "learning_rate": 9.358397751229797e-05,
      "loss": -6.6101,
      "step": 920
    },
    {
      "epoch": 0.6535488404778637,
      "grad_norm": 177.81846618652344,
      "learning_rate": 9.351370344342939e-05,
      "loss": -6.7756,
      "step": 930
    },
    {
      "epoch": 0.6605762473647224,
      "grad_norm": 209.10328674316406,
      "learning_rate": 9.344342937456079e-05,
      "loss": -6.7317,
      "step": 940
    },
    {
      "epoch": 0.6676036542515812,
      "grad_norm": 79.2008285522461,
      "learning_rate": 9.33731553056922e-05,
      "loss": -6.856,
      "step": 950
    },
    {
      "epoch": 0.67463106113844,
      "grad_norm": 35.080535888671875,
      "learning_rate": 9.330288123682361e-05,
      "loss": -6.784,
      "step": 960
    },
    {
      "epoch": 0.6816584680252986,
      "grad_norm": 123.33367919921875,
      "learning_rate": 9.323260716795504e-05,
      "loss": -6.8228,
      "step": 970
    },
    {
      "epoch": 0.6886858749121574,
      "grad_norm": 110.54161071777344,
      "learning_rate": 9.316233309908644e-05,
      "loss": -6.8982,
      "step": 980
    },
    {
      "epoch": 0.6957132817990161,
      "grad_norm": 86.40049743652344,
      "learning_rate": 9.309205903021784e-05,
      "loss": -7.0877,
      "step": 990
    },
    {
      "epoch": 0.7027406886858749,
      "grad_norm": 49.513816833496094,
      "learning_rate": 9.302178496134927e-05,
      "loss": -7.0558,
      "step": 1000
    },
    {
      "epoch": 0.7097680955727337,
      "grad_norm": 55.642215728759766,
      "learning_rate": 9.295151089248068e-05,
      "loss": -6.8424,
      "step": 1010
    },
    {
      "epoch": 0.7167955024595924,
      "grad_norm": 65.7772445678711,
      "learning_rate": 9.288123682361209e-05,
      "loss": -6.943,
      "step": 1020
    },
    {
      "epoch": 0.7238229093464511,
      "grad_norm": 68.26837158203125,
      "learning_rate": 9.28109627547435e-05,
      "loss": -6.6938,
      "step": 1030
    },
    {
      "epoch": 0.7308503162333099,
      "grad_norm": 63.334434509277344,
      "learning_rate": 9.274068868587493e-05,
      "loss": -7.1175,
      "step": 1040
    },
    {
      "epoch": 0.7378777231201686,
      "grad_norm": 261.530029296875,
      "learning_rate": 9.267041461700633e-05,
      "loss": -6.9909,
      "step": 1050
    },
    {
      "epoch": 0.7449051300070274,
      "grad_norm": 159.00860595703125,
      "learning_rate": 9.260014054813773e-05,
      "loss": -6.7705,
      "step": 1060
    },
    {
      "epoch": 0.7519325368938862,
      "grad_norm": 91.65581512451172,
      "learning_rate": 9.252986647926916e-05,
      "loss": -6.9613,
      "step": 1070
    },
    {
      "epoch": 0.7589599437807449,
      "grad_norm": 102.63330078125,
      "learning_rate": 9.245959241040056e-05,
      "loss": -7.0755,
      "step": 1080
    },
    {
      "epoch": 0.7659873506676037,
      "grad_norm": 148.8389434814453,
      "learning_rate": 9.238931834153198e-05,
      "loss": -7.1456,
      "step": 1090
    },
    {
      "epoch": 0.7730147575544624,
      "grad_norm": 209.2189178466797,
      "learning_rate": 9.23190442726634e-05,
      "loss": -6.9934,
      "step": 1100
    },
    {
      "epoch": 0.7800421644413211,
      "grad_norm": 120.55187225341797,
      "learning_rate": 9.22487702037948e-05,
      "loss": -6.174,
      "step": 1110
    },
    {
      "epoch": 0.7870695713281799,
      "grad_norm": 129.6073760986328,
      "learning_rate": 9.217849613492622e-05,
      "loss": -6.8899,
      "step": 1120
    },
    {
      "epoch": 0.7940969782150387,
      "grad_norm": 104.16826629638672,
      "learning_rate": 9.210822206605762e-05,
      "loss": -6.9409,
      "step": 1130
    },
    {
      "epoch": 0.8011243851018974,
      "grad_norm": 66.002197265625,
      "learning_rate": 9.203794799718905e-05,
      "loss": -7.2038,
      "step": 1140
    },
    {
      "epoch": 0.8081517919887562,
      "grad_norm": 88.42974853515625,
      "learning_rate": 9.196767392832045e-05,
      "loss": -7.2089,
      "step": 1150
    },
    {
      "epoch": 0.8151791988756149,
      "grad_norm": 219.88674926757812,
      "learning_rate": 9.189739985945187e-05,
      "loss": -7.2009,
      "step": 1160
    },
    {
      "epoch": 0.8222066057624736,
      "grad_norm": 72.87736511230469,
      "learning_rate": 9.182712579058329e-05,
      "loss": -7.2629,
      "step": 1170
    },
    {
      "epoch": 0.8292340126493324,
      "grad_norm": 82.43689727783203,
      "learning_rate": 9.175685172171469e-05,
      "loss": -7.1921,
      "step": 1180
    },
    {
      "epoch": 0.8362614195361912,
      "grad_norm": 59.75246047973633,
      "learning_rate": 9.16865776528461e-05,
      "loss": -7.2145,
      "step": 1190
    },
    {
      "epoch": 0.8432888264230499,
      "grad_norm": 268.36041259765625,
      "learning_rate": 9.161630358397751e-05,
      "loss": -7.0839,
      "step": 1200
    },
    {
      "epoch": 0.8503162333099087,
      "grad_norm": 62.51845932006836,
      "learning_rate": 9.154602951510894e-05,
      "loss": -7.308,
      "step": 1210
    },
    {
      "epoch": 0.8573436401967673,
      "grad_norm": 96.8796157836914,
      "learning_rate": 9.147575544624034e-05,
      "loss": -7.2365,
      "step": 1220
    },
    {
      "epoch": 0.8643710470836261,
      "grad_norm": 63.921627044677734,
      "learning_rate": 9.140548137737174e-05,
      "loss": -7.1622,
      "step": 1230
    },
    {
      "epoch": 0.8713984539704849,
      "grad_norm": 97.95970153808594,
      "learning_rate": 9.133520730850317e-05,
      "loss": -7.3567,
      "step": 1240
    },
    {
      "epoch": 0.8784258608573436,
      "grad_norm": 85.88983154296875,
      "learning_rate": 9.126493323963458e-05,
      "loss": -6.9544,
      "step": 1250
    },
    {
      "epoch": 0.8854532677442024,
      "grad_norm": 231.1924591064453,
      "learning_rate": 9.1194659170766e-05,
      "loss": -7.2324,
      "step": 1260
    },
    {
      "epoch": 0.8924806746310612,
      "grad_norm": 39.58894348144531,
      "learning_rate": 9.11243851018974e-05,
      "loss": -7.4257,
      "step": 1270
    },
    {
      "epoch": 0.8995080815179198,
      "grad_norm": 196.9145965576172,
      "learning_rate": 9.105411103302883e-05,
      "loss": -7.4067,
      "step": 1280
    },
    {
      "epoch": 0.9065354884047786,
      "grad_norm": 72.6319580078125,
      "learning_rate": 9.098383696416023e-05,
      "loss": -7.3203,
      "step": 1290
    },
    {
      "epoch": 0.9135628952916374,
      "grad_norm": 152.64486694335938,
      "learning_rate": 9.091356289529163e-05,
      "loss": -7.3962,
      "step": 1300
    },
    {
      "epoch": 0.9205903021784961,
      "grad_norm": 25.05042266845703,
      "learning_rate": 9.084328882642306e-05,
      "loss": -7.3108,
      "step": 1310
    },
    {
      "epoch": 0.9276177090653549,
      "grad_norm": 151.17747497558594,
      "learning_rate": 9.077301475755447e-05,
      "loss": -7.3855,
      "step": 1320
    },
    {
      "epoch": 0.9346451159522137,
      "grad_norm": 107.43682861328125,
      "learning_rate": 9.070274068868588e-05,
      "loss": -7.4443,
      "step": 1330
    },
    {
      "epoch": 0.9416725228390724,
      "grad_norm": 350.61492919921875,
      "learning_rate": 9.063246661981728e-05,
      "loss": -7.3617,
      "step": 1340
    },
    {
      "epoch": 0.9486999297259311,
      "grad_norm": 83.1407241821289,
      "learning_rate": 9.05621925509487e-05,
      "loss": -7.5029,
      "step": 1350
    },
    {
      "epoch": 0.9557273366127899,
      "grad_norm": 88.5924301147461,
      "learning_rate": 9.049191848208012e-05,
      "loss": -7.1782,
      "step": 1360
    },
    {
      "epoch": 0.9627547434996486,
      "grad_norm": 104.07402038574219,
      "learning_rate": 9.042164441321152e-05,
      "loss": -7.4722,
      "step": 1370
    },
    {
      "epoch": 0.9697821503865074,
      "grad_norm": 79.9546890258789,
      "learning_rate": 9.035137034434295e-05,
      "loss": -7.3738,
      "step": 1380
    },
    {
      "epoch": 0.9768095572733662,
      "grad_norm": 157.92059326171875,
      "learning_rate": 9.028109627547435e-05,
      "loss": -7.5088,
      "step": 1390
    },
    {
      "epoch": 0.9838369641602249,
      "grad_norm": 197.66197204589844,
      "learning_rate": 9.021082220660577e-05,
      "loss": -7.5222,
      "step": 1400
    },
    {
      "epoch": 0.9908643710470836,
      "grad_norm": 66.03207397460938,
      "learning_rate": 9.014054813773719e-05,
      "loss": -7.4772,
      "step": 1410
    },
    {
      "epoch": 0.9978917779339423,
      "grad_norm": 52.583370208740234,
      "learning_rate": 9.007027406886859e-05,
      "loss": -7.5506,
      "step": 1420
    },
    {
      "epoch": 1.0,
      "eval_runtime": 10.4756,
      "eval_samples_per_second": 65428.547,
      "eval_steps_per_second": 16.037,
      "step": 1423
    },
    {
      "epoch": 1.0049191848208012,
      "grad_norm": 110.1646957397461,
      "learning_rate": 9e-05,
      "loss": -6.5827,
      "step": 1430
    },
    {
      "epoch": 1.0119465917076598,
      "grad_norm": 80.48126983642578,
      "learning_rate": 8.992972593113141e-05,
      "loss": -7.5213,
      "step": 1440
    },
    {
      "epoch": 1.0189739985945185,
      "grad_norm": 101.3356704711914,
      "learning_rate": 8.985945186226284e-05,
      "loss": -6.6798,
      "step": 1450
    },
    {
      "epoch": 1.0260014054813773,
      "grad_norm": 76.03248596191406,
      "learning_rate": 8.978917779339424e-05,
      "loss": -7.5439,
      "step": 1460
    },
    {
      "epoch": 1.033028812368236,
      "grad_norm": 44.563987731933594,
      "learning_rate": 8.971890372452564e-05,
      "loss": -7.5398,
      "step": 1470
    },
    {
      "epoch": 1.0400562192550948,
      "grad_norm": 46.822689056396484,
      "learning_rate": 8.964862965565707e-05,
      "loss": -7.6593,
      "step": 1480
    },
    {
      "epoch": 1.0470836261419536,
      "grad_norm": 111.41635131835938,
      "learning_rate": 8.957835558678848e-05,
      "loss": -7.4396,
      "step": 1490
    },
    {
      "epoch": 1.0541110330288124,
      "grad_norm": 74.81256103515625,
      "learning_rate": 8.95080815179199e-05,
      "loss": -7.4216,
      "step": 1500
    },
    {
      "epoch": 1.0611384399156711,
      "grad_norm": 253.42327880859375,
      "learning_rate": 8.94378074490513e-05,
      "loss": -7.4702,
      "step": 1510
    },
    {
      "epoch": 1.06816584680253,
      "grad_norm": 109.47811889648438,
      "learning_rate": 8.936753338018273e-05,
      "loss": -7.5352,
      "step": 1520
    },
    {
      "epoch": 1.0751932536893887,
      "grad_norm": 212.60894775390625,
      "learning_rate": 8.929725931131413e-05,
      "loss": -7.6768,
      "step": 1530
    },
    {
      "epoch": 1.0822206605762474,
      "grad_norm": 307.6495666503906,
      "learning_rate": 8.922698524244553e-05,
      "loss": -7.457,
      "step": 1540
    },
    {
      "epoch": 1.0892480674631062,
      "grad_norm": 94.30987548828125,
      "learning_rate": 8.915671117357696e-05,
      "loss": -7.6864,
      "step": 1550
    },
    {
      "epoch": 1.096275474349965,
      "grad_norm": 190.77357482910156,
      "learning_rate": 8.908643710470837e-05,
      "loss": -7.5173,
      "step": 1560
    },
    {
      "epoch": 1.1033028812368235,
      "grad_norm": 118.46442413330078,
      "learning_rate": 8.901616303583978e-05,
      "loss": -7.7033,
      "step": 1570
    },
    {
      "epoch": 1.1103302881236823,
      "grad_norm": 62.27548599243164,
      "learning_rate": 8.894588896697119e-05,
      "loss": -6.8829,
      "step": 1580
    },
    {
      "epoch": 1.117357695010541,
      "grad_norm": 306.39337158203125,
      "learning_rate": 8.88756148981026e-05,
      "loss": -7.4595,
      "step": 1590
    },
    {
      "epoch": 1.1243851018973998,
      "grad_norm": 169.19017028808594,
      "learning_rate": 8.880534082923402e-05,
      "loss": -7.7259,
      "step": 1600
    },
    {
      "epoch": 1.1314125087842586,
      "grad_norm": 127.43582916259766,
      "learning_rate": 8.873506676036542e-05,
      "loss": -7.6918,
      "step": 1610
    },
    {
      "epoch": 1.1384399156711174,
      "grad_norm": 34.56240463256836,
      "learning_rate": 8.866479269149685e-05,
      "loss": -7.7454,
      "step": 1620
    },
    {
      "epoch": 1.1454673225579761,
      "grad_norm": 67.09613800048828,
      "learning_rate": 8.859451862262825e-05,
      "loss": -7.6953,
      "step": 1630
    },
    {
      "epoch": 1.1524947294448349,
      "grad_norm": 415.13470458984375,
      "learning_rate": 8.852424455375967e-05,
      "loss": -6.8616,
      "step": 1640
    },
    {
      "epoch": 1.1595221363316937,
      "grad_norm": 242.69825744628906,
      "learning_rate": 8.845397048489107e-05,
      "loss": -7.7246,
      "step": 1650
    },
    {
      "epoch": 1.1665495432185524,
      "grad_norm": 437.3819274902344,
      "learning_rate": 8.838369641602249e-05,
      "loss": -7.6091,
      "step": 1660
    },
    {
      "epoch": 1.1735769501054112,
      "grad_norm": 114.82805633544922,
      "learning_rate": 8.83134223471539e-05,
      "loss": -7.7485,
      "step": 1670
    },
    {
      "epoch": 1.1806043569922697,
      "grad_norm": 200.8641815185547,
      "learning_rate": 8.824314827828531e-05,
      "loss": -7.7532,
      "step": 1680
    },
    {
      "epoch": 1.1876317638791285,
      "grad_norm": 210.88645935058594,
      "learning_rate": 8.817287420941674e-05,
      "loss": -7.6673,
      "step": 1690
    },
    {
      "epoch": 1.1946591707659873,
      "grad_norm": 147.242431640625,
      "learning_rate": 8.810260014054814e-05,
      "loss": -7.6826,
      "step": 1700
    },
    {
      "epoch": 1.201686577652846,
      "grad_norm": 45.30035400390625,
      "learning_rate": 8.803232607167955e-05,
      "loss": -7.6831,
      "step": 1710
    },
    {
      "epoch": 1.2087139845397048,
      "grad_norm": 375.5536193847656,
      "learning_rate": 8.796205200281098e-05,
      "loss": -7.6217,
      "step": 1720
    },
    {
      "epoch": 1.2157413914265636,
      "grad_norm": 149.2346649169922,
      "learning_rate": 8.789177793394238e-05,
      "loss": -7.488,
      "step": 1730
    },
    {
      "epoch": 1.2227687983134223,
      "grad_norm": 39.424407958984375,
      "learning_rate": 8.78215038650738e-05,
      "loss": -7.7215,
      "step": 1740
    },
    {
      "epoch": 1.229796205200281,
      "grad_norm": 138.93104553222656,
      "learning_rate": 8.77512297962052e-05,
      "loss": -7.6875,
      "step": 1750
    },
    {
      "epoch": 1.2368236120871399,
      "grad_norm": 201.68557739257812,
      "learning_rate": 8.768095572733663e-05,
      "loss": -7.8683,
      "step": 1760
    },
    {
      "epoch": 1.2438510189739986,
      "grad_norm": 91.5733642578125,
      "learning_rate": 8.761068165846803e-05,
      "loss": -7.8843,
      "step": 1770
    },
    {
      "epoch": 1.2508784258608574,
      "grad_norm": 128.1421356201172,
      "learning_rate": 8.754040758959943e-05,
      "loss": -7.0119,
      "step": 1780
    },
    {
      "epoch": 1.2579058327477162,
      "grad_norm": 216.029052734375,
      "learning_rate": 8.747013352073086e-05,
      "loss": -6.133,
      "step": 1790
    },
    {
      "epoch": 1.264933239634575,
      "grad_norm": 132.05491638183594,
      "learning_rate": 8.739985945186227e-05,
      "loss": -7.9004,
      "step": 1800
    },
    {
      "epoch": 1.2719606465214337,
      "grad_norm": 125.29669189453125,
      "learning_rate": 8.732958538299368e-05,
      "loss": -7.7738,
      "step": 1810
    },
    {
      "epoch": 1.2789880534082925,
      "grad_norm": 135.6445770263672,
      "learning_rate": 8.725931131412509e-05,
      "loss": -7.7767,
      "step": 1820
    },
    {
      "epoch": 1.286015460295151,
      "grad_norm": 87.72846221923828,
      "learning_rate": 8.71890372452565e-05,
      "loss": -7.8891,
      "step": 1830
    },
    {
      "epoch": 1.2930428671820098,
      "grad_norm": 12.66505241394043,
      "learning_rate": 8.711876317638792e-05,
      "loss": -7.764,
      "step": 1840
    },
    {
      "epoch": 1.3000702740688685,
      "grad_norm": 241.4489288330078,
      "learning_rate": 8.704848910751932e-05,
      "loss": -7.8857,
      "step": 1850
    },
    {
      "epoch": 1.3070976809557273,
      "grad_norm": 185.24317932128906,
      "learning_rate": 8.697821503865075e-05,
      "loss": -7.7664,
      "step": 1860
    },
    {
      "epoch": 1.314125087842586,
      "grad_norm": 218.0685272216797,
      "learning_rate": 8.690794096978215e-05,
      "loss": -7.9495,
      "step": 1870
    },
    {
      "epoch": 1.3211524947294448,
      "grad_norm": 24.654024124145508,
      "learning_rate": 8.683766690091357e-05,
      "loss": -7.825,
      "step": 1880
    },
    {
      "epoch": 1.3281799016163036,
      "grad_norm": 203.9728546142578,
      "learning_rate": 8.676739283204497e-05,
      "loss": -7.6977,
      "step": 1890
    },
    {
      "epoch": 1.3352073085031624,
      "grad_norm": 60.6337890625,
      "learning_rate": 8.669711876317639e-05,
      "loss": -7.8872,
      "step": 1900
    },
    {
      "epoch": 1.3422347153900211,
      "grad_norm": 311.719482421875,
      "learning_rate": 8.662684469430781e-05,
      "loss": -7.8994,
      "step": 1910
    },
    {
      "epoch": 1.3492621222768797,
      "grad_norm": 333.6035461425781,
      "learning_rate": 8.655657062543921e-05,
      "loss": -7.1058,
      "step": 1920
    },
    {
      "epoch": 1.3562895291637385,
      "grad_norm": 130.58074951171875,
      "learning_rate": 8.648629655657064e-05,
      "loss": -7.8661,
      "step": 1930
    },
    {
      "epoch": 1.3633169360505972,
      "grad_norm": 222.85052490234375,
      "learning_rate": 8.641602248770204e-05,
      "loss": -7.8809,
      "step": 1940
    },
    {
      "epoch": 1.370344342937456,
      "grad_norm": 129.05902099609375,
      "learning_rate": 8.634574841883345e-05,
      "loss": -7.9062,
      "step": 1950
    },
    {
      "epoch": 1.3773717498243148,
      "grad_norm": 105.25391387939453,
      "learning_rate": 8.627547434996486e-05,
      "loss": -7.8071,
      "step": 1960
    },
    {
      "epoch": 1.3843991567111735,
      "grad_norm": 307.2703552246094,
      "learning_rate": 8.620520028109628e-05,
      "loss": -7.9027,
      "step": 1970
    },
    {
      "epoch": 1.3914265635980323,
      "grad_norm": 62.729393005371094,
      "learning_rate": 8.61349262122277e-05,
      "loss": -7.9062,
      "step": 1980
    },
    {
      "epoch": 1.398453970484891,
      "grad_norm": 116.59447479248047,
      "learning_rate": 8.60646521433591e-05,
      "loss": -7.7431,
      "step": 1990
    },
    {
      "epoch": 1.4054813773717498,
      "grad_norm": 292.3125,
      "learning_rate": 8.599437807449053e-05,
      "loss": -7.9295,
      "step": 2000
    },
    {
      "epoch": 1.4125087842586086,
      "grad_norm": 250.04989624023438,
      "learning_rate": 8.592410400562193e-05,
      "loss": -8.0243,
      "step": 2010
    },
    {
      "epoch": 1.4195361911454674,
      "grad_norm": 105.00770568847656,
      "learning_rate": 8.585382993675333e-05,
      "loss": -7.8226,
      "step": 2020
    },
    {
      "epoch": 1.4265635980323261,
      "grad_norm": 21.951568603515625,
      "learning_rate": 8.578355586788476e-05,
      "loss": -7.9054,
      "step": 2030
    },
    {
      "epoch": 1.433591004919185,
      "grad_norm": 50.0550537109375,
      "learning_rate": 8.571328179901617e-05,
      "loss": -7.9653,
      "step": 2040
    },
    {
      "epoch": 1.4406184118060437,
      "grad_norm": 133.7836456298828,
      "learning_rate": 8.564300773014758e-05,
      "loss": -8.0536,
      "step": 2050
    },
    {
      "epoch": 1.4476458186929024,
      "grad_norm": Infinity,
      "learning_rate": 8.557976106816585e-05,
      "loss": -7.1716,
      "step": 2060
    },
    {
      "epoch": 1.4546732255797612,
      "grad_norm": 176.17286682128906,
      "learning_rate": 8.550948699929726e-05,
      "loss": -8.0077,
      "step": 2070
    },
    {
      "epoch": 1.4617006324666197,
      "grad_norm": 159.12123107910156,
      "learning_rate": 8.543921293042868e-05,
      "loss": -8.056,
      "step": 2080
    },
    {
      "epoch": 1.4687280393534785,
      "grad_norm": 191.763671875,
      "learning_rate": 8.536893886156008e-05,
      "loss": -8.0518,
      "step": 2090
    },
    {
      "epoch": 1.4757554462403373,
      "grad_norm": 108.61103057861328,
      "learning_rate": 8.52986647926915e-05,
      "loss": -8.0676,
      "step": 2100
    },
    {
      "epoch": 1.482782853127196,
      "grad_norm": 111.61298370361328,
      "learning_rate": 8.522839072382292e-05,
      "loss": -7.9058,
      "step": 2110
    },
    {
      "epoch": 1.4898102600140548,
      "grad_norm": 250.8838348388672,
      "learning_rate": 8.515811665495432e-05,
      "loss": -8.0135,
      "step": 2120
    },
    {
      "epoch": 1.4968376669009136,
      "grad_norm": 82.99273681640625,
      "learning_rate": 8.508784258608574e-05,
      "loss": -8.1104,
      "step": 2130
    },
    {
      "epoch": 1.5038650737877723,
      "grad_norm": 118.11593627929688,
      "learning_rate": 8.501756851721715e-05,
      "loss": -7.9839,
      "step": 2140
    },
    {
      "epoch": 1.510892480674631,
      "grad_norm": 357.0450744628906,
      "learning_rate": 8.494729444834857e-05,
      "loss": -7.9512,
      "step": 2150
    },
    {
      "epoch": 1.5179198875614897,
      "grad_norm": 152.9487762451172,
      "learning_rate": 8.487702037947997e-05,
      "loss": -7.9291,
      "step": 2160
    },
    {
      "epoch": 1.5249472944483484,
      "grad_norm": 103.45035552978516,
      "learning_rate": 8.480674631061139e-05,
      "loss": -8.0236,
      "step": 2170
    },
    {
      "epoch": 1.5319747013352072,
      "grad_norm": 271.2438659667969,
      "learning_rate": 8.47364722417428e-05,
      "loss": -8.0018,
      "step": 2180
    },
    {
      "epoch": 1.539002108222066,
      "grad_norm": 270.0941467285156,
      "learning_rate": 8.466619817287421e-05,
      "loss": -7.2476,
      "step": 2190
    },
    {
      "epoch": 1.5460295151089247,
      "grad_norm": 235.96742248535156,
      "learning_rate": 8.459592410400562e-05,
      "loss": -7.9856,
      "step": 2200
    },
    {
      "epoch": 1.5530569219957835,
      "grad_norm": 280.56396484375,
      "learning_rate": 8.452565003513704e-05,
      "loss": -8.0417,
      "step": 2210
    },
    {
      "epoch": 1.5600843288826423,
      "grad_norm": 100.65755462646484,
      "learning_rate": 8.445537596626844e-05,
      "loss": -7.9256,
      "step": 2220
    },
    {
      "epoch": 1.567111735769501,
      "grad_norm": 220.64414978027344,
      "learning_rate": 8.438510189739986e-05,
      "loss": -8.1307,
      "step": 2230
    },
    {
      "epoch": 1.5741391426563598,
      "grad_norm": 175.27894592285156,
      "learning_rate": 8.431482782853128e-05,
      "loss": -7.9889,
      "step": 2240
    },
    {
      "epoch": 1.5811665495432186,
      "grad_norm": 317.344970703125,
      "learning_rate": 8.424455375966269e-05,
      "loss": -8.0626,
      "step": 2250
    },
    {
      "epoch": 1.5881939564300773,
      "grad_norm": 207.9340362548828,
      "learning_rate": 8.41742796907941e-05,
      "loss": -7.9755,
      "step": 2260
    },
    {
      "epoch": 1.595221363316936,
      "grad_norm": 198.05422973632812,
      "learning_rate": 8.410400562192553e-05,
      "loss": -8.1633,
      "step": 2270
    },
    {
      "epoch": 1.6022487702037949,
      "grad_norm": 65.14700317382812,
      "learning_rate": 8.403373155305693e-05,
      "loss": -8.0752,
      "step": 2280
    },
    {
      "epoch": 1.6092761770906536,
      "grad_norm": 454.77374267578125,
      "learning_rate": 8.396345748418833e-05,
      "loss": -8.0676,
      "step": 2290
    },
    {
      "epoch": 1.6163035839775124,
      "grad_norm": 200.0966796875,
      "learning_rate": 8.389318341531975e-05,
      "loss": -8.0332,
      "step": 2300
    },
    {
      "epoch": 1.6233309908643712,
      "grad_norm": 103.19322204589844,
      "learning_rate": 8.382290934645116e-05,
      "loss": -8.122,
      "step": 2310
    },
    {
      "epoch": 1.63035839775123,
      "grad_norm": 149.4123992919922,
      "learning_rate": 8.375263527758258e-05,
      "loss": -8.1029,
      "step": 2320
    },
    {
      "epoch": 1.6373858046380887,
      "grad_norm": 103.67337036132812,
      "learning_rate": 8.368236120871398e-05,
      "loss": -8.0326,
      "step": 2330
    },
    {
      "epoch": 1.6444132115249475,
      "grad_norm": 185.35316467285156,
      "learning_rate": 8.36120871398454e-05,
      "loss": -8.0803,
      "step": 2340
    },
    {
      "epoch": 1.651440618411806,
      "grad_norm": 197.25575256347656,
      "learning_rate": 8.354181307097682e-05,
      "loss": -8.1533,
      "step": 2350
    },
    {
      "epoch": 1.6584680252986648,
      "grad_norm": 249.06040954589844,
      "learning_rate": 8.347153900210822e-05,
      "loss": -8.1724,
      "step": 2360
    },
    {
      "epoch": 1.6654954321855235,
      "grad_norm": 333.2807312011719,
      "learning_rate": 8.340126493323964e-05,
      "loss": -7.5979,
      "step": 2370
    },
    {
      "epoch": 1.6725228390723823,
      "grad_norm": 145.76206970214844,
      "learning_rate": 8.333099086437105e-05,
      "loss": -8.2499,
      "step": 2380
    },
    {
      "epoch": 1.679550245959241,
      "grad_norm": 187.86170959472656,
      "learning_rate": 8.326071679550246e-05,
      "loss": -8.1486,
      "step": 2390
    },
    {
      "epoch": 1.6865776528460998,
      "grad_norm": 167.01622009277344,
      "learning_rate": 8.319044272663387e-05,
      "loss": -8.0498,
      "step": 2400
    },
    {
      "epoch": 1.6936050597329584,
      "grad_norm": 177.0873260498047,
      "learning_rate": 8.312016865776529e-05,
      "loss": -8.1046,
      "step": 2410
    },
    {
      "epoch": 1.7006324666198172,
      "grad_norm": 198.8045654296875,
      "learning_rate": 8.30498945888967e-05,
      "loss": -8.1156,
      "step": 2420
    },
    {
      "epoch": 1.707659873506676,
      "grad_norm": 151.84422302246094,
      "learning_rate": 8.297962052002811e-05,
      "loss": -8.1786,
      "step": 2430
    },
    {
      "epoch": 1.7146872803935347,
      "grad_norm": 159.3452606201172,
      "learning_rate": 8.290934645115952e-05,
      "loss": -8.1629,
      "step": 2440
    },
    {
      "epoch": 1.7217146872803935,
      "grad_norm": 142.29541015625,
      "learning_rate": 8.283907238229094e-05,
      "loss": -8.1284,
      "step": 2450
    },
    {
      "epoch": 1.7287420941672522,
      "grad_norm": 160.29884338378906,
      "learning_rate": 8.276879831342234e-05,
      "loss": -8.2265,
      "step": 2460
    },
    {
      "epoch": 1.735769501054111,
      "grad_norm": 27.566295623779297,
      "learning_rate": 8.269852424455376e-05,
      "loss": -8.177,
      "step": 2470
    },
    {
      "epoch": 1.7427969079409698,
      "grad_norm": 148.38868713378906,
      "learning_rate": 8.262825017568518e-05,
      "loss": -8.1977,
      "step": 2480
    },
    {
      "epoch": 1.7498243148278285,
      "grad_norm": 146.3419647216797,
      "learning_rate": 8.255797610681659e-05,
      "loss": -8.1051,
      "step": 2490
    },
    {
      "epoch": 1.7568517217146873,
      "grad_norm": 127.4921646118164,
      "learning_rate": 8.2487702037948e-05,
      "loss": -7.8588,
      "step": 2500
    },
    {
      "epoch": 1.763879128601546,
      "grad_norm": 270.7547607421875,
      "learning_rate": 8.241742796907941e-05,
      "loss": -8.2274,
      "step": 2510
    },
    {
      "epoch": 1.7709065354884048,
      "grad_norm": 92.5652084350586,
      "learning_rate": 8.234715390021083e-05,
      "loss": -8.1875,
      "step": 2520
    },
    {
      "epoch": 1.7779339423752636,
      "grad_norm": 108.85939025878906,
      "learning_rate": 8.227687983134223e-05,
      "loss": -8.2365,
      "step": 2530
    },
    {
      "epoch": 1.7849613492621224,
      "grad_norm": 136.16384887695312,
      "learning_rate": 8.220660576247365e-05,
      "loss": -8.1736,
      "step": 2540
    },
    {
      "epoch": 1.7919887561489811,
      "grad_norm": 174.91600036621094,
      "learning_rate": 8.213633169360506e-05,
      "loss": -8.273,
      "step": 2550
    },
    {
      "epoch": 1.7990161630358399,
      "grad_norm": 190.63568115234375,
      "learning_rate": 8.206605762473648e-05,
      "loss": -8.2195,
      "step": 2560
    },
    {
      "epoch": 1.8060435699226987,
      "grad_norm": 118.7540054321289,
      "learning_rate": 8.199578355586788e-05,
      "loss": -8.3201,
      "step": 2570
    },
    {
      "epoch": 1.8130709768095574,
      "grad_norm": 325.76190185546875,
      "learning_rate": 8.19255094869993e-05,
      "loss": -8.2348,
      "step": 2580
    },
    {
      "epoch": 1.8200983836964162,
      "grad_norm": 212.48995971679688,
      "learning_rate": 8.185523541813072e-05,
      "loss": -8.3127,
      "step": 2590
    },
    {
      "epoch": 1.8271257905832747,
      "grad_norm": 73.72135162353516,
      "learning_rate": 8.178496134926212e-05,
      "loss": -7.4182,
      "step": 2600
    },
    {
      "epoch": 1.8341531974701335,
      "grad_norm": 67.7585220336914,
      "learning_rate": 8.171468728039354e-05,
      "loss": -8.1626,
      "step": 2610
    },
    {
      "epoch": 1.8411806043569923,
      "grad_norm": 161.43724060058594,
      "learning_rate": 8.164441321152495e-05,
      "loss": -8.2256,
      "step": 2620
    },
    {
      "epoch": 1.848208011243851,
      "grad_norm": 324.5807800292969,
      "learning_rate": 8.157413914265636e-05,
      "loss": -8.2551,
      "step": 2630
    },
    {
      "epoch": 1.8552354181307098,
      "grad_norm": 104.27599334716797,
      "learning_rate": 8.150386507378777e-05,
      "loss": -8.3348,
      "step": 2640
    },
    {
      "epoch": 1.8622628250175686,
      "grad_norm": 75.72303771972656,
      "learning_rate": 8.143359100491919e-05,
      "loss": -7.4876,
      "step": 2650
    },
    {
      "epoch": 1.8692902319044271,
      "grad_norm": 136.56875610351562,
      "learning_rate": 8.13633169360506e-05,
      "loss": -8.4072,
      "step": 2660
    },
    {
      "epoch": 1.8763176387912859,
      "grad_norm": 86.91016387939453,
      "learning_rate": 8.129304286718201e-05,
      "loss": -7.5135,
      "step": 2670
    },
    {
      "epoch": 1.8833450456781446,
      "grad_norm": 79.09539794921875,
      "learning_rate": 8.122276879831342e-05,
      "loss": -8.4511,
      "step": 2680
    },
    {
      "epoch": 1.8903724525650034,
      "grad_norm": 219.55218505859375,
      "learning_rate": 8.115249472944484e-05,
      "loss": -8.3619,
      "step": 2690
    },
    {
      "epoch": 1.8973998594518622,
      "grad_norm": 65.51472473144531,
      "learning_rate": 8.108222066057624e-05,
      "loss": -8.3804,
      "step": 2700
    },
    {
      "epoch": 1.904427266338721,
      "grad_norm": 125.93338775634766,
      "learning_rate": 8.101194659170766e-05,
      "loss": -8.3732,
      "step": 2710
    },
    {
      "epoch": 1.9114546732255797,
      "grad_norm": 330.94012451171875,
      "learning_rate": 8.094167252283908e-05,
      "loss": -8.3931,
      "step": 2720
    },
    {
      "epoch": 1.9184820801124385,
      "grad_norm": 290.2658996582031,
      "learning_rate": 8.08713984539705e-05,
      "loss": -8.3797,
      "step": 2730
    },
    {
      "epoch": 1.9255094869992972,
      "grad_norm": 207.47683715820312,
      "learning_rate": 8.08011243851019e-05,
      "loss": -8.2656,
      "step": 2740
    },
    {
      "epoch": 1.932536893886156,
      "grad_norm": 216.8863067626953,
      "learning_rate": 8.073085031623331e-05,
      "loss": -7.5076,
      "step": 2750
    },
    {
      "epoch": 1.9395643007730148,
      "grad_norm": 91.2572250366211,
      "learning_rate": 8.066057624736473e-05,
      "loss": -7.6686,
      "step": 2760
    },
    {
      "epoch": 1.9465917076598735,
      "grad_norm": 205.54518127441406,
      "learning_rate": 8.059030217849613e-05,
      "loss": -8.3054,
      "step": 2770
    },
    {
      "epoch": 1.9536191145467323,
      "grad_norm": 84.35154724121094,
      "learning_rate": 8.052002810962755e-05,
      "loss": -8.365,
      "step": 2780
    },
    {
      "epoch": 1.960646521433591,
      "grad_norm": 114.12862396240234,
      "learning_rate": 8.044975404075897e-05,
      "loss": -8.4789,
      "step": 2790
    },
    {
      "epoch": 1.9676739283204498,
      "grad_norm": 294.4913635253906,
      "learning_rate": 8.037947997189038e-05,
      "loss": -8.3305,
      "step": 2800
    },
    {
      "epoch": 1.9747013352073086,
      "grad_norm": 160.31134033203125,
      "learning_rate": 8.030920590302178e-05,
      "loss": -7.5235,
      "step": 2810
    },
    {
      "epoch": 1.9817287420941674,
      "grad_norm": 198.88284301757812,
      "learning_rate": 8.02389318341532e-05,
      "loss": -8.4697,
      "step": 2820
    },
    {
      "epoch": 1.9887561489810262,
      "grad_norm": 263.5756530761719,
      "learning_rate": 8.016865776528462e-05,
      "loss": -8.3798,
      "step": 2830
    },
    {
      "epoch": 1.9957835558678847,
      "grad_norm": 387.53594970703125,
      "learning_rate": 8.009838369641602e-05,
      "loss": -7.7596,
      "step": 2840
    },
    {
      "epoch": 2.0,
      "eval_runtime": 10.7188,
      "eval_samples_per_second": 63944.128,
      "eval_steps_per_second": 15.673,
      "step": 2846
    }
  ],
  "logging_steps": 10,
  "max_steps": 14230,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1099061274345472.0,
  "train_batch_size": 1024,
  "trial_name": null,
  "trial_params": null
}
