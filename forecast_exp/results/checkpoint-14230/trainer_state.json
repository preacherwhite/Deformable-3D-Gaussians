{
  "best_metric": 10.7188,
  "best_model_checkpoint": "./results/checkpoint-2846",
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 14230,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007027406886858749,
      "grad_norm": 1.6860004663467407,
      "learning_rate": 9.993675333801827e-05,
      "loss": -1.2921,
      "step": 10
    },
    {
      "epoch": 0.014054813773717497,
      "grad_norm": 1.5223720073699951,
      "learning_rate": 9.98664792691497e-05,
      "loss": -1.3807,
      "step": 20
    },
    {
      "epoch": 0.02108222066057625,
      "grad_norm": 0.8838928937911987,
      "learning_rate": 9.97962052002811e-05,
      "loss": -1.4051,
      "step": 30
    },
    {
      "epoch": 0.028109627547434995,
      "grad_norm": 0.9422695636749268,
      "learning_rate": 9.97259311314125e-05,
      "loss": -1.4302,
      "step": 40
    },
    {
      "epoch": 0.035137034434293744,
      "grad_norm": 2.1020593643188477,
      "learning_rate": 9.965565706254392e-05,
      "loss": -1.4926,
      "step": 50
    },
    {
      "epoch": 0.0421644413211525,
      "grad_norm": 1.212904453277588,
      "learning_rate": 9.958538299367534e-05,
      "loss": -1.5129,
      "step": 60
    },
    {
      "epoch": 0.049191848208011243,
      "grad_norm": 1.2417880296707153,
      "learning_rate": 9.951510892480675e-05,
      "loss": -1.5735,
      "step": 70
    },
    {
      "epoch": 0.05621925509486999,
      "grad_norm": 1.4859538078308105,
      "learning_rate": 9.944483485593816e-05,
      "loss": -1.657,
      "step": 80
    },
    {
      "epoch": 0.06324666198172874,
      "grad_norm": 2.792626142501831,
      "learning_rate": 9.937456078706959e-05,
      "loss": -1.772,
      "step": 90
    },
    {
      "epoch": 0.07027406886858749,
      "grad_norm": 4.062569618225098,
      "learning_rate": 9.930428671820099e-05,
      "loss": -2.0051,
      "step": 100
    },
    {
      "epoch": 0.07730147575544624,
      "grad_norm": 10.54062557220459,
      "learning_rate": 9.924104005621925e-05,
      "loss": -2.3975,
      "step": 110
    },
    {
      "epoch": 0.084328882642305,
      "grad_norm": Infinity,
      "learning_rate": 9.918482080112439e-05,
      "loss": -2.6963,
      "step": 120
    },
    {
      "epoch": 0.09135628952916373,
      "grad_norm": 11.13524341583252,
      "learning_rate": 9.91145467322558e-05,
      "loss": -2.8551,
      "step": 130
    },
    {
      "epoch": 0.09838369641602249,
      "grad_norm": 27.765460968017578,
      "learning_rate": 9.904427266338721e-05,
      "loss": -3.1458,
      "step": 140
    },
    {
      "epoch": 0.10541110330288124,
      "grad_norm": 50.406158447265625,
      "learning_rate": 9.897399859451863e-05,
      "loss": -3.0714,
      "step": 150
    },
    {
      "epoch": 0.11243851018973998,
      "grad_norm": 7.153792858123779,
      "learning_rate": 9.890372452565004e-05,
      "loss": -3.0842,
      "step": 160
    },
    {
      "epoch": 0.11946591707659873,
      "grad_norm": 19.641448974609375,
      "learning_rate": 9.883345045678145e-05,
      "loss": -3.6879,
      "step": 170
    },
    {
      "epoch": 0.12649332396345747,
      "grad_norm": Infinity,
      "learning_rate": 9.877020379479973e-05,
      "loss": -3.393,
      "step": 180
    },
    {
      "epoch": 0.13352073085031624,
      "grad_norm": 50.81373596191406,
      "learning_rate": 9.869992972593114e-05,
      "loss": -3.653,
      "step": 190
    },
    {
      "epoch": 0.14054813773717498,
      "grad_norm": 45.54872512817383,
      "learning_rate": 9.862965565706254e-05,
      "loss": -3.8452,
      "step": 200
    },
    {
      "epoch": 0.14757554462403374,
      "grad_norm": 20.347394943237305,
      "learning_rate": 9.855938158819396e-05,
      "loss": -4.0533,
      "step": 210
    },
    {
      "epoch": 0.15460295151089248,
      "grad_norm": 25.921110153198242,
      "learning_rate": 9.848910751932538e-05,
      "loss": -4.2427,
      "step": 220
    },
    {
      "epoch": 0.16163035839775122,
      "grad_norm": 13.012264251708984,
      "learning_rate": 9.841883345045678e-05,
      "loss": -4.3694,
      "step": 230
    },
    {
      "epoch": 0.16865776528461,
      "grad_norm": 60.69106674194336,
      "learning_rate": 9.83485593815882e-05,
      "loss": -4.5216,
      "step": 240
    },
    {
      "epoch": 0.17568517217146873,
      "grad_norm": 18.223241806030273,
      "learning_rate": 9.827828531271961e-05,
      "loss": -4.592,
      "step": 250
    },
    {
      "epoch": 0.18271257905832747,
      "grad_norm": 16.20248794555664,
      "learning_rate": 9.820801124385103e-05,
      "loss": -4.5475,
      "step": 260
    },
    {
      "epoch": 0.18973998594518623,
      "grad_norm": 54.024654388427734,
      "learning_rate": 9.813773717498243e-05,
      "loss": -4.8331,
      "step": 270
    },
    {
      "epoch": 0.19676739283204497,
      "grad_norm": 21.992366790771484,
      "learning_rate": 9.806746310611385e-05,
      "loss": -4.6675,
      "step": 280
    },
    {
      "epoch": 0.2037947997189037,
      "grad_norm": Infinity,
      "learning_rate": 9.800421644413213e-05,
      "loss": -4.5446,
      "step": 290
    },
    {
      "epoch": 0.21082220660576248,
      "grad_norm": 58.36254119873047,
      "learning_rate": 9.793394237526353e-05,
      "loss": -4.593,
      "step": 300
    },
    {
      "epoch": 0.21784961349262122,
      "grad_norm": 33.2662467956543,
      "learning_rate": 9.786366830639495e-05,
      "loss": -4.8962,
      "step": 310
    },
    {
      "epoch": 0.22487702037947996,
      "grad_norm": 43.39387512207031,
      "learning_rate": 9.779339423752636e-05,
      "loss": -4.9067,
      "step": 320
    },
    {
      "epoch": 0.23190442726633873,
      "grad_norm": 31.868099212646484,
      "learning_rate": 9.772312016865777e-05,
      "loss": -4.8554,
      "step": 330
    },
    {
      "epoch": 0.23893183415319746,
      "grad_norm": 32.99828338623047,
      "learning_rate": 9.765284609978918e-05,
      "loss": -4.7624,
      "step": 340
    },
    {
      "epoch": 0.24595924104005623,
      "grad_norm": 22.345361709594727,
      "learning_rate": 9.75825720309206e-05,
      "loss": -4.3312,
      "step": 350
    },
    {
      "epoch": 0.25298664792691494,
      "grad_norm": 19.150009155273438,
      "learning_rate": 9.751229796205202e-05,
      "loss": -5.3253,
      "step": 360
    },
    {
      "epoch": 0.2600140548137737,
      "grad_norm": 73.74205017089844,
      "learning_rate": 9.744202389318342e-05,
      "loss": -4.8292,
      "step": 370
    },
    {
      "epoch": 0.2670414617006325,
      "grad_norm": 66.38163757324219,
      "learning_rate": 9.737174982431483e-05,
      "loss": -5.1897,
      "step": 380
    },
    {
      "epoch": 0.27406886858749124,
      "grad_norm": 49.51173782348633,
      "learning_rate": 9.730147575544625e-05,
      "loss": -5.6284,
      "step": 390
    },
    {
      "epoch": 0.28109627547434995,
      "grad_norm": 35.72504806518555,
      "learning_rate": 9.723120168657765e-05,
      "loss": -5.4332,
      "step": 400
    },
    {
      "epoch": 0.2881236823612087,
      "grad_norm": 35.065677642822266,
      "learning_rate": 9.716092761770907e-05,
      "loss": -5.3912,
      "step": 410
    },
    {
      "epoch": 0.2951510892480675,
      "grad_norm": 64.38773345947266,
      "learning_rate": 9.709065354884049e-05,
      "loss": -5.3189,
      "step": 420
    },
    {
      "epoch": 0.3021784961349262,
      "grad_norm": 48.378482818603516,
      "learning_rate": 9.702037947997189e-05,
      "loss": -5.4603,
      "step": 430
    },
    {
      "epoch": 0.30920590302178497,
      "grad_norm": 23.19247055053711,
      "learning_rate": 9.69501054111033e-05,
      "loss": -5.4158,
      "step": 440
    },
    {
      "epoch": 0.31623330990864373,
      "grad_norm": 64.5969467163086,
      "learning_rate": 9.687983134223472e-05,
      "loss": -5.1215,
      "step": 450
    },
    {
      "epoch": 0.32326071679550245,
      "grad_norm": 44.53466033935547,
      "learning_rate": 9.680955727336614e-05,
      "loss": -5.9027,
      "step": 460
    },
    {
      "epoch": 0.3302881236823612,
      "grad_norm": 30.518274307250977,
      "learning_rate": 9.673928320449754e-05,
      "loss": -5.459,
      "step": 470
    },
    {
      "epoch": 0.33731553056922,
      "grad_norm": 101.69618225097656,
      "learning_rate": 9.666900913562896e-05,
      "loss": -5.2915,
      "step": 480
    },
    {
      "epoch": 0.3443429374560787,
      "grad_norm": 71.34317016601562,
      "learning_rate": 9.659873506676038e-05,
      "loss": -4.9095,
      "step": 490
    },
    {
      "epoch": 0.35137034434293746,
      "grad_norm": 84.34651184082031,
      "learning_rate": 9.652846099789178e-05,
      "loss": -5.6687,
      "step": 500
    },
    {
      "epoch": 0.3583977512297962,
      "grad_norm": 42.30650329589844,
      "learning_rate": 9.64581869290232e-05,
      "loss": -5.7226,
      "step": 510
    },
    {
      "epoch": 0.36542515811665494,
      "grad_norm": 79.91998291015625,
      "learning_rate": 9.638791286015461e-05,
      "loss": -5.7669,
      "step": 520
    },
    {
      "epoch": 0.3724525650035137,
      "grad_norm": 63.55778503417969,
      "learning_rate": 9.631763879128603e-05,
      "loss": -5.9739,
      "step": 530
    },
    {
      "epoch": 0.37947997189037247,
      "grad_norm": 23.731168746948242,
      "learning_rate": 9.624736472241743e-05,
      "loss": -5.8008,
      "step": 540
    },
    {
      "epoch": 0.3865073787772312,
      "grad_norm": 58.560874938964844,
      "learning_rate": 9.617709065354885e-05,
      "loss": -5.2713,
      "step": 550
    },
    {
      "epoch": 0.39353478566408995,
      "grad_norm": 48.000518798828125,
      "learning_rate": 9.610681658468026e-05,
      "loss": -5.8234,
      "step": 560
    },
    {
      "epoch": 0.4005621925509487,
      "grad_norm": 64.36412048339844,
      "learning_rate": 9.603654251581167e-05,
      "loss": -5.5284,
      "step": 570
    },
    {
      "epoch": 0.4075895994378074,
      "grad_norm": 53.271366119384766,
      "learning_rate": 9.596626844694308e-05,
      "loss": -5.5427,
      "step": 580
    },
    {
      "epoch": 0.4146170063246662,
      "grad_norm": 79.79280090332031,
      "learning_rate": 9.58959943780745e-05,
      "loss": -5.9536,
      "step": 590
    },
    {
      "epoch": 0.42164441321152496,
      "grad_norm": 150.04550170898438,
      "learning_rate": 9.582572030920592e-05,
      "loss": -5.8571,
      "step": 600
    },
    {
      "epoch": 0.42867182009838367,
      "grad_norm": 93.87496948242188,
      "learning_rate": 9.575544624033732e-05,
      "loss": -6.0092,
      "step": 610
    },
    {
      "epoch": 0.43569922698524244,
      "grad_norm": 117.1666259765625,
      "learning_rate": 9.568517217146874e-05,
      "loss": -6.087,
      "step": 620
    },
    {
      "epoch": 0.4427266338721012,
      "grad_norm": 50.84917449951172,
      "learning_rate": 9.561489810260015e-05,
      "loss": -6.1588,
      "step": 630
    },
    {
      "epoch": 0.4497540407589599,
      "grad_norm": 51.93496322631836,
      "learning_rate": 9.554462403373155e-05,
      "loss": -6.3485,
      "step": 640
    },
    {
      "epoch": 0.4567814476458187,
      "grad_norm": 32.84553527832031,
      "learning_rate": 9.547434996486297e-05,
      "loss": -6.2964,
      "step": 650
    },
    {
      "epoch": 0.46380885453267745,
      "grad_norm": 60.058799743652344,
      "learning_rate": 9.540407589599439e-05,
      "loss": -6.2155,
      "step": 660
    },
    {
      "epoch": 0.4708362614195362,
      "grad_norm": 32.49545669555664,
      "learning_rate": 9.533380182712579e-05,
      "loss": -6.1921,
      "step": 670
    },
    {
      "epoch": 0.47786366830639493,
      "grad_norm": 27.035213470458984,
      "learning_rate": 9.526352775825721e-05,
      "loss": -6.3931,
      "step": 680
    },
    {
      "epoch": 0.4848910751932537,
      "grad_norm": 81.44160461425781,
      "learning_rate": 9.519325368938862e-05,
      "loss": -6.3278,
      "step": 690
    },
    {
      "epoch": 0.49191848208011246,
      "grad_norm": 158.89794921875,
      "learning_rate": 9.512297962052004e-05,
      "loss": -6.2272,
      "step": 700
    },
    {
      "epoch": 0.4989458889669712,
      "grad_norm": 130.32418823242188,
      "learning_rate": 9.505270555165144e-05,
      "loss": -6.2297,
      "step": 710
    },
    {
      "epoch": 0.5059732958538299,
      "grad_norm": 74.09990692138672,
      "learning_rate": 9.498243148278286e-05,
      "loss": -6.4421,
      "step": 720
    },
    {
      "epoch": 0.5130007027406887,
      "grad_norm": 49.37168884277344,
      "learning_rate": 9.491215741391428e-05,
      "loss": -6.165,
      "step": 730
    },
    {
      "epoch": 0.5200281096275474,
      "grad_norm": 75.5392074584961,
      "learning_rate": 9.484188334504568e-05,
      "loss": -6.5732,
      "step": 740
    },
    {
      "epoch": 0.5270555165144062,
      "grad_norm": 45.11758041381836,
      "learning_rate": 9.47716092761771e-05,
      "loss": -6.3273,
      "step": 750
    },
    {
      "epoch": 0.534082923401265,
      "grad_norm": 95.66844940185547,
      "learning_rate": 9.470133520730851e-05,
      "loss": -6.5303,
      "step": 760
    },
    {
      "epoch": 0.5411103302881237,
      "grad_norm": 59.14422607421875,
      "learning_rate": 9.463106113843993e-05,
      "loss": -6.4698,
      "step": 770
    },
    {
      "epoch": 0.5481377371749825,
      "grad_norm": 68.3644790649414,
      "learning_rate": 9.456078706957133e-05,
      "loss": -6.5819,
      "step": 780
    },
    {
      "epoch": 0.5551651440618411,
      "grad_norm": 72.96343994140625,
      "learning_rate": 9.449754040758961e-05,
      "loss": -6.4701,
      "step": 790
    },
    {
      "epoch": 0.5621925509486999,
      "grad_norm": 47.393516540527344,
      "learning_rate": 9.442726633872103e-05,
      "loss": -6.5356,
      "step": 800
    },
    {
      "epoch": 0.5692199578355587,
      "grad_norm": 99.93370819091797,
      "learning_rate": 9.435699226985243e-05,
      "loss": -6.5243,
      "step": 810
    },
    {
      "epoch": 0.5762473647224174,
      "grad_norm": 168.73931884765625,
      "learning_rate": 9.428671820098383e-05,
      "loss": -6.6569,
      "step": 820
    },
    {
      "epoch": 0.5832747716092762,
      "grad_norm": 50.17271041870117,
      "learning_rate": 9.421644413211526e-05,
      "loss": -6.8741,
      "step": 830
    },
    {
      "epoch": 0.590302178496135,
      "grad_norm": 78.69847869873047,
      "learning_rate": 9.414617006324666e-05,
      "loss": -6.6368,
      "step": 840
    },
    {
      "epoch": 0.5973295853829936,
      "grad_norm": 89.92803192138672,
      "learning_rate": 9.407589599437808e-05,
      "loss": -6.7147,
      "step": 850
    },
    {
      "epoch": 0.6043569922698524,
      "grad_norm": 277.0621337890625,
      "learning_rate": 9.40056219255095e-05,
      "loss": -6.0667,
      "step": 860
    },
    {
      "epoch": 0.6113843991567112,
      "grad_norm": 86.7811508178711,
      "learning_rate": 9.393534785664091e-05,
      "loss": -6.646,
      "step": 870
    },
    {
      "epoch": 0.6184118060435699,
      "grad_norm": 64.3265151977539,
      "learning_rate": 9.386507378777232e-05,
      "loss": -6.1169,
      "step": 880
    },
    {
      "epoch": 0.6254392129304287,
      "grad_norm": 246.79415893554688,
      "learning_rate": 9.379479971890372e-05,
      "loss": -6.7719,
      "step": 890
    },
    {
      "epoch": 0.6324666198172875,
      "grad_norm": 291.45452880859375,
      "learning_rate": 9.372452565003515e-05,
      "loss": -6.7775,
      "step": 900
    },
    {
      "epoch": 0.6394940267041461,
      "grad_norm": 158.0874481201172,
      "learning_rate": 9.365425158116655e-05,
      "loss": -6.8678,
      "step": 910
    },
    {
      "epoch": 0.6465214335910049,
      "grad_norm": 98.92765045166016,
      "learning_rate": 9.358397751229797e-05,
      "loss": -6.6101,
      "step": 920
    },
    {
      "epoch": 0.6535488404778637,
      "grad_norm": 177.81846618652344,
      "learning_rate": 9.351370344342939e-05,
      "loss": -6.7756,
      "step": 930
    },
    {
      "epoch": 0.6605762473647224,
      "grad_norm": 209.10328674316406,
      "learning_rate": 9.344342937456079e-05,
      "loss": -6.7317,
      "step": 940
    },
    {
      "epoch": 0.6676036542515812,
      "grad_norm": 79.2008285522461,
      "learning_rate": 9.33731553056922e-05,
      "loss": -6.856,
      "step": 950
    },
    {
      "epoch": 0.67463106113844,
      "grad_norm": 35.080535888671875,
      "learning_rate": 9.330288123682361e-05,
      "loss": -6.784,
      "step": 960
    },
    {
      "epoch": 0.6816584680252986,
      "grad_norm": 123.33367919921875,
      "learning_rate": 9.323260716795504e-05,
      "loss": -6.8228,
      "step": 970
    },
    {
      "epoch": 0.6886858749121574,
      "grad_norm": 110.54161071777344,
      "learning_rate": 9.316233309908644e-05,
      "loss": -6.8982,
      "step": 980
    },
    {
      "epoch": 0.6957132817990161,
      "grad_norm": 86.40049743652344,
      "learning_rate": 9.309205903021784e-05,
      "loss": -7.0877,
      "step": 990
    },
    {
      "epoch": 0.7027406886858749,
      "grad_norm": 49.513816833496094,
      "learning_rate": 9.302178496134927e-05,
      "loss": -7.0558,
      "step": 1000
    },
    {
      "epoch": 0.7097680955727337,
      "grad_norm": 55.642215728759766,
      "learning_rate": 9.295151089248068e-05,
      "loss": -6.8424,
      "step": 1010
    },
    {
      "epoch": 0.7167955024595924,
      "grad_norm": 65.7772445678711,
      "learning_rate": 9.288123682361209e-05,
      "loss": -6.943,
      "step": 1020
    },
    {
      "epoch": 0.7238229093464511,
      "grad_norm": 68.26837158203125,
      "learning_rate": 9.28109627547435e-05,
      "loss": -6.6938,
      "step": 1030
    },
    {
      "epoch": 0.7308503162333099,
      "grad_norm": 63.334434509277344,
      "learning_rate": 9.274068868587493e-05,
      "loss": -7.1175,
      "step": 1040
    },
    {
      "epoch": 0.7378777231201686,
      "grad_norm": 261.530029296875,
      "learning_rate": 9.267041461700633e-05,
      "loss": -6.9909,
      "step": 1050
    },
    {
      "epoch": 0.7449051300070274,
      "grad_norm": 159.00860595703125,
      "learning_rate": 9.260014054813773e-05,
      "loss": -6.7705,
      "step": 1060
    },
    {
      "epoch": 0.7519325368938862,
      "grad_norm": 91.65581512451172,
      "learning_rate": 9.252986647926916e-05,
      "loss": -6.9613,
      "step": 1070
    },
    {
      "epoch": 0.7589599437807449,
      "grad_norm": 102.63330078125,
      "learning_rate": 9.245959241040056e-05,
      "loss": -7.0755,
      "step": 1080
    },
    {
      "epoch": 0.7659873506676037,
      "grad_norm": 148.8389434814453,
      "learning_rate": 9.238931834153198e-05,
      "loss": -7.1456,
      "step": 1090
    },
    {
      "epoch": 0.7730147575544624,
      "grad_norm": 209.2189178466797,
      "learning_rate": 9.23190442726634e-05,
      "loss": -6.9934,
      "step": 1100
    },
    {
      "epoch": 0.7800421644413211,
      "grad_norm": 120.55187225341797,
      "learning_rate": 9.22487702037948e-05,
      "loss": -6.174,
      "step": 1110
    },
    {
      "epoch": 0.7870695713281799,
      "grad_norm": 129.6073760986328,
      "learning_rate": 9.217849613492622e-05,
      "loss": -6.8899,
      "step": 1120
    },
    {
      "epoch": 0.7940969782150387,
      "grad_norm": 104.16826629638672,
      "learning_rate": 9.210822206605762e-05,
      "loss": -6.9409,
      "step": 1130
    },
    {
      "epoch": 0.8011243851018974,
      "grad_norm": 66.002197265625,
      "learning_rate": 9.203794799718905e-05,
      "loss": -7.2038,
      "step": 1140
    },
    {
      "epoch": 0.8081517919887562,
      "grad_norm": 88.42974853515625,
      "learning_rate": 9.196767392832045e-05,
      "loss": -7.2089,
      "step": 1150
    },
    {
      "epoch": 0.8151791988756149,
      "grad_norm": 219.88674926757812,
      "learning_rate": 9.189739985945187e-05,
      "loss": -7.2009,
      "step": 1160
    },
    {
      "epoch": 0.8222066057624736,
      "grad_norm": 72.87736511230469,
      "learning_rate": 9.182712579058329e-05,
      "loss": -7.2629,
      "step": 1170
    },
    {
      "epoch": 0.8292340126493324,
      "grad_norm": 82.43689727783203,
      "learning_rate": 9.175685172171469e-05,
      "loss": -7.1921,
      "step": 1180
    },
    {
      "epoch": 0.8362614195361912,
      "grad_norm": 59.75246047973633,
      "learning_rate": 9.16865776528461e-05,
      "loss": -7.2145,
      "step": 1190
    },
    {
      "epoch": 0.8432888264230499,
      "grad_norm": 268.36041259765625,
      "learning_rate": 9.161630358397751e-05,
      "loss": -7.0839,
      "step": 1200
    },
    {
      "epoch": 0.8503162333099087,
      "grad_norm": 62.51845932006836,
      "learning_rate": 9.154602951510894e-05,
      "loss": -7.308,
      "step": 1210
    },
    {
      "epoch": 0.8573436401967673,
      "grad_norm": 96.8796157836914,
      "learning_rate": 9.147575544624034e-05,
      "loss": -7.2365,
      "step": 1220
    },
    {
      "epoch": 0.8643710470836261,
      "grad_norm": 63.921627044677734,
      "learning_rate": 9.140548137737174e-05,
      "loss": -7.1622,
      "step": 1230
    },
    {
      "epoch": 0.8713984539704849,
      "grad_norm": 97.95970153808594,
      "learning_rate": 9.133520730850317e-05,
      "loss": -7.3567,
      "step": 1240
    },
    {
      "epoch": 0.8784258608573436,
      "grad_norm": 85.88983154296875,
      "learning_rate": 9.126493323963458e-05,
      "loss": -6.9544,
      "step": 1250
    },
    {
      "epoch": 0.8854532677442024,
      "grad_norm": 231.1924591064453,
      "learning_rate": 9.1194659170766e-05,
      "loss": -7.2324,
      "step": 1260
    },
    {
      "epoch": 0.8924806746310612,
      "grad_norm": 39.58894348144531,
      "learning_rate": 9.11243851018974e-05,
      "loss": -7.4257,
      "step": 1270
    },
    {
      "epoch": 0.8995080815179198,
      "grad_norm": 196.9145965576172,
      "learning_rate": 9.105411103302883e-05,
      "loss": -7.4067,
      "step": 1280
    },
    {
      "epoch": 0.9065354884047786,
      "grad_norm": 72.6319580078125,
      "learning_rate": 9.098383696416023e-05,
      "loss": -7.3203,
      "step": 1290
    },
    {
      "epoch": 0.9135628952916374,
      "grad_norm": 152.64486694335938,
      "learning_rate": 9.091356289529163e-05,
      "loss": -7.3962,
      "step": 1300
    },
    {
      "epoch": 0.9205903021784961,
      "grad_norm": 25.05042266845703,
      "learning_rate": 9.084328882642306e-05,
      "loss": -7.3108,
      "step": 1310
    },
    {
      "epoch": 0.9276177090653549,
      "grad_norm": 151.17747497558594,
      "learning_rate": 9.077301475755447e-05,
      "loss": -7.3855,
      "step": 1320
    },
    {
      "epoch": 0.9346451159522137,
      "grad_norm": 107.43682861328125,
      "learning_rate": 9.070274068868588e-05,
      "loss": -7.4443,
      "step": 1330
    },
    {
      "epoch": 0.9416725228390724,
      "grad_norm": 350.61492919921875,
      "learning_rate": 9.063246661981728e-05,
      "loss": -7.3617,
      "step": 1340
    },
    {
      "epoch": 0.9486999297259311,
      "grad_norm": 83.1407241821289,
      "learning_rate": 9.05621925509487e-05,
      "loss": -7.5029,
      "step": 1350
    },
    {
      "epoch": 0.9557273366127899,
      "grad_norm": 88.5924301147461,
      "learning_rate": 9.049191848208012e-05,
      "loss": -7.1782,
      "step": 1360
    },
    {
      "epoch": 0.9627547434996486,
      "grad_norm": 104.07402038574219,
      "learning_rate": 9.042164441321152e-05,
      "loss": -7.4722,
      "step": 1370
    },
    {
      "epoch": 0.9697821503865074,
      "grad_norm": 79.9546890258789,
      "learning_rate": 9.035137034434295e-05,
      "loss": -7.3738,
      "step": 1380
    },
    {
      "epoch": 0.9768095572733662,
      "grad_norm": 157.92059326171875,
      "learning_rate": 9.028109627547435e-05,
      "loss": -7.5088,
      "step": 1390
    },
    {
      "epoch": 0.9838369641602249,
      "grad_norm": 197.66197204589844,
      "learning_rate": 9.021082220660577e-05,
      "loss": -7.5222,
      "step": 1400
    },
    {
      "epoch": 0.9908643710470836,
      "grad_norm": 66.03207397460938,
      "learning_rate": 9.014054813773719e-05,
      "loss": -7.4772,
      "step": 1410
    },
    {
      "epoch": 0.9978917779339423,
      "grad_norm": 52.583370208740234,
      "learning_rate": 9.007027406886859e-05,
      "loss": -7.5506,
      "step": 1420
    },
    {
      "epoch": 1.0,
      "eval_runtime": 10.4756,
      "eval_samples_per_second": 65428.547,
      "eval_steps_per_second": 16.037,
      "step": 1423
    },
    {
      "epoch": 1.0049191848208012,
      "grad_norm": 110.1646957397461,
      "learning_rate": 9e-05,
      "loss": -6.5827,
      "step": 1430
    },
    {
      "epoch": 1.0119465917076598,
      "grad_norm": 80.48126983642578,
      "learning_rate": 8.992972593113141e-05,
      "loss": -7.5213,
      "step": 1440
    },
    {
      "epoch": 1.0189739985945185,
      "grad_norm": 101.3356704711914,
      "learning_rate": 8.985945186226284e-05,
      "loss": -6.6798,
      "step": 1450
    },
    {
      "epoch": 1.0260014054813773,
      "grad_norm": 76.03248596191406,
      "learning_rate": 8.978917779339424e-05,
      "loss": -7.5439,
      "step": 1460
    },
    {
      "epoch": 1.033028812368236,
      "grad_norm": 44.563987731933594,
      "learning_rate": 8.971890372452564e-05,
      "loss": -7.5398,
      "step": 1470
    },
    {
      "epoch": 1.0400562192550948,
      "grad_norm": 46.822689056396484,
      "learning_rate": 8.964862965565707e-05,
      "loss": -7.6593,
      "step": 1480
    },
    {
      "epoch": 1.0470836261419536,
      "grad_norm": 111.41635131835938,
      "learning_rate": 8.957835558678848e-05,
      "loss": -7.4396,
      "step": 1490
    },
    {
      "epoch": 1.0541110330288124,
      "grad_norm": 74.81256103515625,
      "learning_rate": 8.95080815179199e-05,
      "loss": -7.4216,
      "step": 1500
    },
    {
      "epoch": 1.0611384399156711,
      "grad_norm": 253.42327880859375,
      "learning_rate": 8.94378074490513e-05,
      "loss": -7.4702,
      "step": 1510
    },
    {
      "epoch": 1.06816584680253,
      "grad_norm": 109.47811889648438,
      "learning_rate": 8.936753338018273e-05,
      "loss": -7.5352,
      "step": 1520
    },
    {
      "epoch": 1.0751932536893887,
      "grad_norm": 212.60894775390625,
      "learning_rate": 8.929725931131413e-05,
      "loss": -7.6768,
      "step": 1530
    },
    {
      "epoch": 1.0822206605762474,
      "grad_norm": 307.6495666503906,
      "learning_rate": 8.922698524244553e-05,
      "loss": -7.457,
      "step": 1540
    },
    {
      "epoch": 1.0892480674631062,
      "grad_norm": 94.30987548828125,
      "learning_rate": 8.915671117357696e-05,
      "loss": -7.6864,
      "step": 1550
    },
    {
      "epoch": 1.096275474349965,
      "grad_norm": 190.77357482910156,
      "learning_rate": 8.908643710470837e-05,
      "loss": -7.5173,
      "step": 1560
    },
    {
      "epoch": 1.1033028812368235,
      "grad_norm": 118.46442413330078,
      "learning_rate": 8.901616303583978e-05,
      "loss": -7.7033,
      "step": 1570
    },
    {
      "epoch": 1.1103302881236823,
      "grad_norm": 62.27548599243164,
      "learning_rate": 8.894588896697119e-05,
      "loss": -6.8829,
      "step": 1580
    },
    {
      "epoch": 1.117357695010541,
      "grad_norm": 306.39337158203125,
      "learning_rate": 8.88756148981026e-05,
      "loss": -7.4595,
      "step": 1590
    },
    {
      "epoch": 1.1243851018973998,
      "grad_norm": 169.19017028808594,
      "learning_rate": 8.880534082923402e-05,
      "loss": -7.7259,
      "step": 1600
    },
    {
      "epoch": 1.1314125087842586,
      "grad_norm": 127.43582916259766,
      "learning_rate": 8.873506676036542e-05,
      "loss": -7.6918,
      "step": 1610
    },
    {
      "epoch": 1.1384399156711174,
      "grad_norm": 34.56240463256836,
      "learning_rate": 8.866479269149685e-05,
      "loss": -7.7454,
      "step": 1620
    },
    {
      "epoch": 1.1454673225579761,
      "grad_norm": 67.09613800048828,
      "learning_rate": 8.859451862262825e-05,
      "loss": -7.6953,
      "step": 1630
    },
    {
      "epoch": 1.1524947294448349,
      "grad_norm": 415.13470458984375,
      "learning_rate": 8.852424455375967e-05,
      "loss": -6.8616,
      "step": 1640
    },
    {
      "epoch": 1.1595221363316937,
      "grad_norm": 242.69825744628906,
      "learning_rate": 8.845397048489107e-05,
      "loss": -7.7246,
      "step": 1650
    },
    {
      "epoch": 1.1665495432185524,
      "grad_norm": 437.3819274902344,
      "learning_rate": 8.838369641602249e-05,
      "loss": -7.6091,
      "step": 1660
    },
    {
      "epoch": 1.1735769501054112,
      "grad_norm": 114.82805633544922,
      "learning_rate": 8.83134223471539e-05,
      "loss": -7.7485,
      "step": 1670
    },
    {
      "epoch": 1.1806043569922697,
      "grad_norm": 200.8641815185547,
      "learning_rate": 8.824314827828531e-05,
      "loss": -7.7532,
      "step": 1680
    },
    {
      "epoch": 1.1876317638791285,
      "grad_norm": 210.88645935058594,
      "learning_rate": 8.817287420941674e-05,
      "loss": -7.6673,
      "step": 1690
    },
    {
      "epoch": 1.1946591707659873,
      "grad_norm": 147.242431640625,
      "learning_rate": 8.810260014054814e-05,
      "loss": -7.6826,
      "step": 1700
    },
    {
      "epoch": 1.201686577652846,
      "grad_norm": 45.30035400390625,
      "learning_rate": 8.803232607167955e-05,
      "loss": -7.6831,
      "step": 1710
    },
    {
      "epoch": 1.2087139845397048,
      "grad_norm": 375.5536193847656,
      "learning_rate": 8.796205200281098e-05,
      "loss": -7.6217,
      "step": 1720
    },
    {
      "epoch": 1.2157413914265636,
      "grad_norm": 149.2346649169922,
      "learning_rate": 8.789177793394238e-05,
      "loss": -7.488,
      "step": 1730
    },
    {
      "epoch": 1.2227687983134223,
      "grad_norm": 39.424407958984375,
      "learning_rate": 8.78215038650738e-05,
      "loss": -7.7215,
      "step": 1740
    },
    {
      "epoch": 1.229796205200281,
      "grad_norm": 138.93104553222656,
      "learning_rate": 8.77512297962052e-05,
      "loss": -7.6875,
      "step": 1750
    },
    {
      "epoch": 1.2368236120871399,
      "grad_norm": 201.68557739257812,
      "learning_rate": 8.768095572733663e-05,
      "loss": -7.8683,
      "step": 1760
    },
    {
      "epoch": 1.2438510189739986,
      "grad_norm": 91.5733642578125,
      "learning_rate": 8.761068165846803e-05,
      "loss": -7.8843,
      "step": 1770
    },
    {
      "epoch": 1.2508784258608574,
      "grad_norm": 128.1421356201172,
      "learning_rate": 8.754040758959943e-05,
      "loss": -7.0119,
      "step": 1780
    },
    {
      "epoch": 1.2579058327477162,
      "grad_norm": 216.029052734375,
      "learning_rate": 8.747013352073086e-05,
      "loss": -6.133,
      "step": 1790
    },
    {
      "epoch": 1.264933239634575,
      "grad_norm": 132.05491638183594,
      "learning_rate": 8.739985945186227e-05,
      "loss": -7.9004,
      "step": 1800
    },
    {
      "epoch": 1.2719606465214337,
      "grad_norm": 125.29669189453125,
      "learning_rate": 8.732958538299368e-05,
      "loss": -7.7738,
      "step": 1810
    },
    {
      "epoch": 1.2789880534082925,
      "grad_norm": 135.6445770263672,
      "learning_rate": 8.725931131412509e-05,
      "loss": -7.7767,
      "step": 1820
    },
    {
      "epoch": 1.286015460295151,
      "grad_norm": 87.72846221923828,
      "learning_rate": 8.71890372452565e-05,
      "loss": -7.8891,
      "step": 1830
    },
    {
      "epoch": 1.2930428671820098,
      "grad_norm": 12.66505241394043,
      "learning_rate": 8.711876317638792e-05,
      "loss": -7.764,
      "step": 1840
    },
    {
      "epoch": 1.3000702740688685,
      "grad_norm": 241.4489288330078,
      "learning_rate": 8.704848910751932e-05,
      "loss": -7.8857,
      "step": 1850
    },
    {
      "epoch": 1.3070976809557273,
      "grad_norm": 185.24317932128906,
      "learning_rate": 8.697821503865075e-05,
      "loss": -7.7664,
      "step": 1860
    },
    {
      "epoch": 1.314125087842586,
      "grad_norm": 218.0685272216797,
      "learning_rate": 8.690794096978215e-05,
      "loss": -7.9495,
      "step": 1870
    },
    {
      "epoch": 1.3211524947294448,
      "grad_norm": 24.654024124145508,
      "learning_rate": 8.683766690091357e-05,
      "loss": -7.825,
      "step": 1880
    },
    {
      "epoch": 1.3281799016163036,
      "grad_norm": 203.9728546142578,
      "learning_rate": 8.676739283204497e-05,
      "loss": -7.6977,
      "step": 1890
    },
    {
      "epoch": 1.3352073085031624,
      "grad_norm": 60.6337890625,
      "learning_rate": 8.669711876317639e-05,
      "loss": -7.8872,
      "step": 1900
    },
    {
      "epoch": 1.3422347153900211,
      "grad_norm": 311.719482421875,
      "learning_rate": 8.662684469430781e-05,
      "loss": -7.8994,
      "step": 1910
    },
    {
      "epoch": 1.3492621222768797,
      "grad_norm": 333.6035461425781,
      "learning_rate": 8.655657062543921e-05,
      "loss": -7.1058,
      "step": 1920
    },
    {
      "epoch": 1.3562895291637385,
      "grad_norm": 130.58074951171875,
      "learning_rate": 8.648629655657064e-05,
      "loss": -7.8661,
      "step": 1930
    },
    {
      "epoch": 1.3633169360505972,
      "grad_norm": 222.85052490234375,
      "learning_rate": 8.641602248770204e-05,
      "loss": -7.8809,
      "step": 1940
    },
    {
      "epoch": 1.370344342937456,
      "grad_norm": 129.05902099609375,
      "learning_rate": 8.634574841883345e-05,
      "loss": -7.9062,
      "step": 1950
    },
    {
      "epoch": 1.3773717498243148,
      "grad_norm": 105.25391387939453,
      "learning_rate": 8.627547434996486e-05,
      "loss": -7.8071,
      "step": 1960
    },
    {
      "epoch": 1.3843991567111735,
      "grad_norm": 307.2703552246094,
      "learning_rate": 8.620520028109628e-05,
      "loss": -7.9027,
      "step": 1970
    },
    {
      "epoch": 1.3914265635980323,
      "grad_norm": 62.729393005371094,
      "learning_rate": 8.61349262122277e-05,
      "loss": -7.9062,
      "step": 1980
    },
    {
      "epoch": 1.398453970484891,
      "grad_norm": 116.59447479248047,
      "learning_rate": 8.60646521433591e-05,
      "loss": -7.7431,
      "step": 1990
    },
    {
      "epoch": 1.4054813773717498,
      "grad_norm": 292.3125,
      "learning_rate": 8.599437807449053e-05,
      "loss": -7.9295,
      "step": 2000
    },
    {
      "epoch": 1.4125087842586086,
      "grad_norm": 250.04989624023438,
      "learning_rate": 8.592410400562193e-05,
      "loss": -8.0243,
      "step": 2010
    },
    {
      "epoch": 1.4195361911454674,
      "grad_norm": 105.00770568847656,
      "learning_rate": 8.585382993675333e-05,
      "loss": -7.8226,
      "step": 2020
    },
    {
      "epoch": 1.4265635980323261,
      "grad_norm": 21.951568603515625,
      "learning_rate": 8.578355586788476e-05,
      "loss": -7.9054,
      "step": 2030
    },
    {
      "epoch": 1.433591004919185,
      "grad_norm": 50.0550537109375,
      "learning_rate": 8.571328179901617e-05,
      "loss": -7.9653,
      "step": 2040
    },
    {
      "epoch": 1.4406184118060437,
      "grad_norm": 133.7836456298828,
      "learning_rate": 8.564300773014758e-05,
      "loss": -8.0536,
      "step": 2050
    },
    {
      "epoch": 1.4476458186929024,
      "grad_norm": Infinity,
      "learning_rate": 8.557976106816585e-05,
      "loss": -7.1716,
      "step": 2060
    },
    {
      "epoch": 1.4546732255797612,
      "grad_norm": 176.17286682128906,
      "learning_rate": 8.550948699929726e-05,
      "loss": -8.0077,
      "step": 2070
    },
    {
      "epoch": 1.4617006324666197,
      "grad_norm": 159.12123107910156,
      "learning_rate": 8.543921293042868e-05,
      "loss": -8.056,
      "step": 2080
    },
    {
      "epoch": 1.4687280393534785,
      "grad_norm": 191.763671875,
      "learning_rate": 8.536893886156008e-05,
      "loss": -8.0518,
      "step": 2090
    },
    {
      "epoch": 1.4757554462403373,
      "grad_norm": 108.61103057861328,
      "learning_rate": 8.52986647926915e-05,
      "loss": -8.0676,
      "step": 2100
    },
    {
      "epoch": 1.482782853127196,
      "grad_norm": 111.61298370361328,
      "learning_rate": 8.522839072382292e-05,
      "loss": -7.9058,
      "step": 2110
    },
    {
      "epoch": 1.4898102600140548,
      "grad_norm": 250.8838348388672,
      "learning_rate": 8.515811665495432e-05,
      "loss": -8.0135,
      "step": 2120
    },
    {
      "epoch": 1.4968376669009136,
      "grad_norm": 82.99273681640625,
      "learning_rate": 8.508784258608574e-05,
      "loss": -8.1104,
      "step": 2130
    },
    {
      "epoch": 1.5038650737877723,
      "grad_norm": 118.11593627929688,
      "learning_rate": 8.501756851721715e-05,
      "loss": -7.9839,
      "step": 2140
    },
    {
      "epoch": 1.510892480674631,
      "grad_norm": 357.0450744628906,
      "learning_rate": 8.494729444834857e-05,
      "loss": -7.9512,
      "step": 2150
    },
    {
      "epoch": 1.5179198875614897,
      "grad_norm": 152.9487762451172,
      "learning_rate": 8.487702037947997e-05,
      "loss": -7.9291,
      "step": 2160
    },
    {
      "epoch": 1.5249472944483484,
      "grad_norm": 103.45035552978516,
      "learning_rate": 8.480674631061139e-05,
      "loss": -8.0236,
      "step": 2170
    },
    {
      "epoch": 1.5319747013352072,
      "grad_norm": 271.2438659667969,
      "learning_rate": 8.47364722417428e-05,
      "loss": -8.0018,
      "step": 2180
    },
    {
      "epoch": 1.539002108222066,
      "grad_norm": 270.0941467285156,
      "learning_rate": 8.466619817287421e-05,
      "loss": -7.2476,
      "step": 2190
    },
    {
      "epoch": 1.5460295151089247,
      "grad_norm": 235.96742248535156,
      "learning_rate": 8.459592410400562e-05,
      "loss": -7.9856,
      "step": 2200
    },
    {
      "epoch": 1.5530569219957835,
      "grad_norm": 280.56396484375,
      "learning_rate": 8.452565003513704e-05,
      "loss": -8.0417,
      "step": 2210
    },
    {
      "epoch": 1.5600843288826423,
      "grad_norm": 100.65755462646484,
      "learning_rate": 8.445537596626844e-05,
      "loss": -7.9256,
      "step": 2220
    },
    {
      "epoch": 1.567111735769501,
      "grad_norm": 220.64414978027344,
      "learning_rate": 8.438510189739986e-05,
      "loss": -8.1307,
      "step": 2230
    },
    {
      "epoch": 1.5741391426563598,
      "grad_norm": 175.27894592285156,
      "learning_rate": 8.431482782853128e-05,
      "loss": -7.9889,
      "step": 2240
    },
    {
      "epoch": 1.5811665495432186,
      "grad_norm": 317.344970703125,
      "learning_rate": 8.424455375966269e-05,
      "loss": -8.0626,
      "step": 2250
    },
    {
      "epoch": 1.5881939564300773,
      "grad_norm": 207.9340362548828,
      "learning_rate": 8.41742796907941e-05,
      "loss": -7.9755,
      "step": 2260
    },
    {
      "epoch": 1.595221363316936,
      "grad_norm": 198.05422973632812,
      "learning_rate": 8.410400562192553e-05,
      "loss": -8.1633,
      "step": 2270
    },
    {
      "epoch": 1.6022487702037949,
      "grad_norm": 65.14700317382812,
      "learning_rate": 8.403373155305693e-05,
      "loss": -8.0752,
      "step": 2280
    },
    {
      "epoch": 1.6092761770906536,
      "grad_norm": 454.77374267578125,
      "learning_rate": 8.396345748418833e-05,
      "loss": -8.0676,
      "step": 2290
    },
    {
      "epoch": 1.6163035839775124,
      "grad_norm": 200.0966796875,
      "learning_rate": 8.389318341531975e-05,
      "loss": -8.0332,
      "step": 2300
    },
    {
      "epoch": 1.6233309908643712,
      "grad_norm": 103.19322204589844,
      "learning_rate": 8.382290934645116e-05,
      "loss": -8.122,
      "step": 2310
    },
    {
      "epoch": 1.63035839775123,
      "grad_norm": 149.4123992919922,
      "learning_rate": 8.375263527758258e-05,
      "loss": -8.1029,
      "step": 2320
    },
    {
      "epoch": 1.6373858046380887,
      "grad_norm": 103.67337036132812,
      "learning_rate": 8.368236120871398e-05,
      "loss": -8.0326,
      "step": 2330
    },
    {
      "epoch": 1.6444132115249475,
      "grad_norm": 185.35316467285156,
      "learning_rate": 8.36120871398454e-05,
      "loss": -8.0803,
      "step": 2340
    },
    {
      "epoch": 1.651440618411806,
      "grad_norm": 197.25575256347656,
      "learning_rate": 8.354181307097682e-05,
      "loss": -8.1533,
      "step": 2350
    },
    {
      "epoch": 1.6584680252986648,
      "grad_norm": 249.06040954589844,
      "learning_rate": 8.347153900210822e-05,
      "loss": -8.1724,
      "step": 2360
    },
    {
      "epoch": 1.6654954321855235,
      "grad_norm": 333.2807312011719,
      "learning_rate": 8.340126493323964e-05,
      "loss": -7.5979,
      "step": 2370
    },
    {
      "epoch": 1.6725228390723823,
      "grad_norm": 145.76206970214844,
      "learning_rate": 8.333099086437105e-05,
      "loss": -8.2499,
      "step": 2380
    },
    {
      "epoch": 1.679550245959241,
      "grad_norm": 187.86170959472656,
      "learning_rate": 8.326071679550246e-05,
      "loss": -8.1486,
      "step": 2390
    },
    {
      "epoch": 1.6865776528460998,
      "grad_norm": 167.01622009277344,
      "learning_rate": 8.319044272663387e-05,
      "loss": -8.0498,
      "step": 2400
    },
    {
      "epoch": 1.6936050597329584,
      "grad_norm": 177.0873260498047,
      "learning_rate": 8.312016865776529e-05,
      "loss": -8.1046,
      "step": 2410
    },
    {
      "epoch": 1.7006324666198172,
      "grad_norm": 198.8045654296875,
      "learning_rate": 8.30498945888967e-05,
      "loss": -8.1156,
      "step": 2420
    },
    {
      "epoch": 1.707659873506676,
      "grad_norm": 151.84422302246094,
      "learning_rate": 8.297962052002811e-05,
      "loss": -8.1786,
      "step": 2430
    },
    {
      "epoch": 1.7146872803935347,
      "grad_norm": 159.3452606201172,
      "learning_rate": 8.290934645115952e-05,
      "loss": -8.1629,
      "step": 2440
    },
    {
      "epoch": 1.7217146872803935,
      "grad_norm": 142.29541015625,
      "learning_rate": 8.283907238229094e-05,
      "loss": -8.1284,
      "step": 2450
    },
    {
      "epoch": 1.7287420941672522,
      "grad_norm": 160.29884338378906,
      "learning_rate": 8.276879831342234e-05,
      "loss": -8.2265,
      "step": 2460
    },
    {
      "epoch": 1.735769501054111,
      "grad_norm": 27.566295623779297,
      "learning_rate": 8.269852424455376e-05,
      "loss": -8.177,
      "step": 2470
    },
    {
      "epoch": 1.7427969079409698,
      "grad_norm": 148.38868713378906,
      "learning_rate": 8.262825017568518e-05,
      "loss": -8.1977,
      "step": 2480
    },
    {
      "epoch": 1.7498243148278285,
      "grad_norm": 146.3419647216797,
      "learning_rate": 8.255797610681659e-05,
      "loss": -8.1051,
      "step": 2490
    },
    {
      "epoch": 1.7568517217146873,
      "grad_norm": 127.4921646118164,
      "learning_rate": 8.2487702037948e-05,
      "loss": -7.8588,
      "step": 2500
    },
    {
      "epoch": 1.763879128601546,
      "grad_norm": 270.7547607421875,
      "learning_rate": 8.241742796907941e-05,
      "loss": -8.2274,
      "step": 2510
    },
    {
      "epoch": 1.7709065354884048,
      "grad_norm": 92.5652084350586,
      "learning_rate": 8.234715390021083e-05,
      "loss": -8.1875,
      "step": 2520
    },
    {
      "epoch": 1.7779339423752636,
      "grad_norm": 108.85939025878906,
      "learning_rate": 8.227687983134223e-05,
      "loss": -8.2365,
      "step": 2530
    },
    {
      "epoch": 1.7849613492621224,
      "grad_norm": 136.16384887695312,
      "learning_rate": 8.220660576247365e-05,
      "loss": -8.1736,
      "step": 2540
    },
    {
      "epoch": 1.7919887561489811,
      "grad_norm": 174.91600036621094,
      "learning_rate": 8.213633169360506e-05,
      "loss": -8.273,
      "step": 2550
    },
    {
      "epoch": 1.7990161630358399,
      "grad_norm": 190.63568115234375,
      "learning_rate": 8.206605762473648e-05,
      "loss": -8.2195,
      "step": 2560
    },
    {
      "epoch": 1.8060435699226987,
      "grad_norm": 118.7540054321289,
      "learning_rate": 8.199578355586788e-05,
      "loss": -8.3201,
      "step": 2570
    },
    {
      "epoch": 1.8130709768095574,
      "grad_norm": 325.76190185546875,
      "learning_rate": 8.19255094869993e-05,
      "loss": -8.2348,
      "step": 2580
    },
    {
      "epoch": 1.8200983836964162,
      "grad_norm": 212.48995971679688,
      "learning_rate": 8.185523541813072e-05,
      "loss": -8.3127,
      "step": 2590
    },
    {
      "epoch": 1.8271257905832747,
      "grad_norm": 73.72135162353516,
      "learning_rate": 8.178496134926212e-05,
      "loss": -7.4182,
      "step": 2600
    },
    {
      "epoch": 1.8341531974701335,
      "grad_norm": 67.7585220336914,
      "learning_rate": 8.171468728039354e-05,
      "loss": -8.1626,
      "step": 2610
    },
    {
      "epoch": 1.8411806043569923,
      "grad_norm": 161.43724060058594,
      "learning_rate": 8.164441321152495e-05,
      "loss": -8.2256,
      "step": 2620
    },
    {
      "epoch": 1.848208011243851,
      "grad_norm": 324.5807800292969,
      "learning_rate": 8.157413914265636e-05,
      "loss": -8.2551,
      "step": 2630
    },
    {
      "epoch": 1.8552354181307098,
      "grad_norm": 104.27599334716797,
      "learning_rate": 8.150386507378777e-05,
      "loss": -8.3348,
      "step": 2640
    },
    {
      "epoch": 1.8622628250175686,
      "grad_norm": 75.72303771972656,
      "learning_rate": 8.143359100491919e-05,
      "loss": -7.4876,
      "step": 2650
    },
    {
      "epoch": 1.8692902319044271,
      "grad_norm": 136.56875610351562,
      "learning_rate": 8.13633169360506e-05,
      "loss": -8.4072,
      "step": 2660
    },
    {
      "epoch": 1.8763176387912859,
      "grad_norm": 86.91016387939453,
      "learning_rate": 8.129304286718201e-05,
      "loss": -7.5135,
      "step": 2670
    },
    {
      "epoch": 1.8833450456781446,
      "grad_norm": 79.09539794921875,
      "learning_rate": 8.122276879831342e-05,
      "loss": -8.4511,
      "step": 2680
    },
    {
      "epoch": 1.8903724525650034,
      "grad_norm": 219.55218505859375,
      "learning_rate": 8.115249472944484e-05,
      "loss": -8.3619,
      "step": 2690
    },
    {
      "epoch": 1.8973998594518622,
      "grad_norm": 65.51472473144531,
      "learning_rate": 8.108222066057624e-05,
      "loss": -8.3804,
      "step": 2700
    },
    {
      "epoch": 1.904427266338721,
      "grad_norm": 125.93338775634766,
      "learning_rate": 8.101194659170766e-05,
      "loss": -8.3732,
      "step": 2710
    },
    {
      "epoch": 1.9114546732255797,
      "grad_norm": 330.94012451171875,
      "learning_rate": 8.094167252283908e-05,
      "loss": -8.3931,
      "step": 2720
    },
    {
      "epoch": 1.9184820801124385,
      "grad_norm": 290.2658996582031,
      "learning_rate": 8.08713984539705e-05,
      "loss": -8.3797,
      "step": 2730
    },
    {
      "epoch": 1.9255094869992972,
      "grad_norm": 207.47683715820312,
      "learning_rate": 8.08011243851019e-05,
      "loss": -8.2656,
      "step": 2740
    },
    {
      "epoch": 1.932536893886156,
      "grad_norm": 216.8863067626953,
      "learning_rate": 8.073085031623331e-05,
      "loss": -7.5076,
      "step": 2750
    },
    {
      "epoch": 1.9395643007730148,
      "grad_norm": 91.2572250366211,
      "learning_rate": 8.066057624736473e-05,
      "loss": -7.6686,
      "step": 2760
    },
    {
      "epoch": 1.9465917076598735,
      "grad_norm": 205.54518127441406,
      "learning_rate": 8.059030217849613e-05,
      "loss": -8.3054,
      "step": 2770
    },
    {
      "epoch": 1.9536191145467323,
      "grad_norm": 84.35154724121094,
      "learning_rate": 8.052002810962755e-05,
      "loss": -8.365,
      "step": 2780
    },
    {
      "epoch": 1.960646521433591,
      "grad_norm": 114.12862396240234,
      "learning_rate": 8.044975404075897e-05,
      "loss": -8.4789,
      "step": 2790
    },
    {
      "epoch": 1.9676739283204498,
      "grad_norm": 294.4913635253906,
      "learning_rate": 8.037947997189038e-05,
      "loss": -8.3305,
      "step": 2800
    },
    {
      "epoch": 1.9747013352073086,
      "grad_norm": 160.31134033203125,
      "learning_rate": 8.030920590302178e-05,
      "loss": -7.5235,
      "step": 2810
    },
    {
      "epoch": 1.9817287420941674,
      "grad_norm": 198.88284301757812,
      "learning_rate": 8.02389318341532e-05,
      "loss": -8.4697,
      "step": 2820
    },
    {
      "epoch": 1.9887561489810262,
      "grad_norm": 263.5756530761719,
      "learning_rate": 8.016865776528462e-05,
      "loss": -8.3798,
      "step": 2830
    },
    {
      "epoch": 1.9957835558678847,
      "grad_norm": 387.53594970703125,
      "learning_rate": 8.009838369641602e-05,
      "loss": -7.7596,
      "step": 2840
    },
    {
      "epoch": 2.0,
      "eval_runtime": 10.7188,
      "eval_samples_per_second": 63944.128,
      "eval_steps_per_second": 15.673,
      "step": 2846
    },
    {
      "epoch": 2.0028109627547437,
      "grad_norm": 225.02691650390625,
      "learning_rate": 8.002810962754744e-05,
      "loss": -8.4811,
      "step": 2850
    },
    {
      "epoch": 2.0098383696416025,
      "grad_norm": 110.37256622314453,
      "learning_rate": 7.995783555867885e-05,
      "loss": -8.3994,
      "step": 2860
    },
    {
      "epoch": 2.016865776528461,
      "grad_norm": 125.79314422607422,
      "learning_rate": 7.988756148981026e-05,
      "loss": -8.2953,
      "step": 2870
    },
    {
      "epoch": 2.0238931834153195,
      "grad_norm": 169.89019775390625,
      "learning_rate": 7.981728742094167e-05,
      "loss": -7.7113,
      "step": 2880
    },
    {
      "epoch": 2.0309205903021783,
      "grad_norm": 166.33570861816406,
      "learning_rate": 7.974701335207309e-05,
      "loss": -8.4608,
      "step": 2890
    },
    {
      "epoch": 2.037947997189037,
      "grad_norm": 164.51760864257812,
      "learning_rate": 7.96767392832045e-05,
      "loss": -8.3889,
      "step": 2900
    },
    {
      "epoch": 2.044975404075896,
      "grad_norm": 162.68081665039062,
      "learning_rate": 7.960646521433591e-05,
      "loss": -8.5235,
      "step": 2910
    },
    {
      "epoch": 2.0520028109627546,
      "grad_norm": 173.57479858398438,
      "learning_rate": 7.953619114546733e-05,
      "loss": -8.3284,
      "step": 2920
    },
    {
      "epoch": 2.0590302178496134,
      "grad_norm": 103.26721954345703,
      "learning_rate": 7.946591707659874e-05,
      "loss": -8.5108,
      "step": 2930
    },
    {
      "epoch": 2.066057624736472,
      "grad_norm": 94.17555236816406,
      "learning_rate": 7.939564300773014e-05,
      "loss": -8.4024,
      "step": 2940
    },
    {
      "epoch": 2.073085031623331,
      "grad_norm": 157.7161102294922,
      "learning_rate": 7.932536893886156e-05,
      "loss": -8.2318,
      "step": 2950
    },
    {
      "epoch": 2.0801124385101897,
      "grad_norm": 291.6960754394531,
      "learning_rate": 7.925509486999298e-05,
      "loss": -7.595,
      "step": 2960
    },
    {
      "epoch": 2.0871398453970484,
      "grad_norm": 309.52581787109375,
      "learning_rate": 7.91848208011244e-05,
      "loss": -8.386,
      "step": 2970
    },
    {
      "epoch": 2.094167252283907,
      "grad_norm": 203.21226501464844,
      "learning_rate": 7.91145467322558e-05,
      "loss": -8.3198,
      "step": 2980
    },
    {
      "epoch": 2.101194659170766,
      "grad_norm": 420.1835632324219,
      "learning_rate": 7.904427266338721e-05,
      "loss": -8.4286,
      "step": 2990
    },
    {
      "epoch": 2.1082220660576247,
      "grad_norm": 115.77474975585938,
      "learning_rate": 7.897399859451863e-05,
      "loss": -8.505,
      "step": 3000
    },
    {
      "epoch": 2.1152494729444835,
      "grad_norm": 99.93772888183594,
      "learning_rate": 7.890372452565003e-05,
      "loss": -7.8865,
      "step": 3010
    },
    {
      "epoch": 2.1222768798313423,
      "grad_norm": 200.03050231933594,
      "learning_rate": 7.883345045678145e-05,
      "loss": -8.4979,
      "step": 3020
    },
    {
      "epoch": 2.129304286718201,
      "grad_norm": 287.7288818359375,
      "learning_rate": 7.876317638791287e-05,
      "loss": -8.3979,
      "step": 3030
    },
    {
      "epoch": 2.13633169360506,
      "grad_norm": 143.4256591796875,
      "learning_rate": 7.869290231904428e-05,
      "loss": -8.5013,
      "step": 3040
    },
    {
      "epoch": 2.1433591004919186,
      "grad_norm": 135.6341094970703,
      "learning_rate": 7.862262825017569e-05,
      "loss": -8.5607,
      "step": 3050
    },
    {
      "epoch": 2.1503865073787773,
      "grad_norm": 54.66648864746094,
      "learning_rate": 7.85523541813071e-05,
      "loss": -8.4154,
      "step": 3060
    },
    {
      "epoch": 2.157413914265636,
      "grad_norm": 51.61063003540039,
      "learning_rate": 7.848208011243852e-05,
      "loss": -7.9824,
      "step": 3070
    },
    {
      "epoch": 2.164441321152495,
      "grad_norm": 126.29732513427734,
      "learning_rate": 7.841180604356992e-05,
      "loss": -8.4601,
      "step": 3080
    },
    {
      "epoch": 2.1714687280393536,
      "grad_norm": 149.10711669921875,
      "learning_rate": 7.834153197470134e-05,
      "loss": -8.5592,
      "step": 3090
    },
    {
      "epoch": 2.1784961349262124,
      "grad_norm": 124.78160858154297,
      "learning_rate": 7.827125790583275e-05,
      "loss": -8.5193,
      "step": 3100
    },
    {
      "epoch": 2.185523541813071,
      "grad_norm": 14.085470199584961,
      "learning_rate": 7.820098383696416e-05,
      "loss": -7.9478,
      "step": 3110
    },
    {
      "epoch": 2.19255094869993,
      "grad_norm": 115.70513916015625,
      "learning_rate": 7.813070976809557e-05,
      "loss": -8.5576,
      "step": 3120
    },
    {
      "epoch": 2.1995783555867883,
      "grad_norm": 116.06289672851562,
      "learning_rate": 7.806043569922699e-05,
      "loss": -8.3945,
      "step": 3130
    },
    {
      "epoch": 2.206605762473647,
      "grad_norm": 109.96200561523438,
      "learning_rate": 7.79901616303584e-05,
      "loss": -8.5986,
      "step": 3140
    },
    {
      "epoch": 2.213633169360506,
      "grad_norm": 235.173828125,
      "learning_rate": 7.791988756148981e-05,
      "loss": -8.5781,
      "step": 3150
    },
    {
      "epoch": 2.2206605762473646,
      "grad_norm": 257.8872375488281,
      "learning_rate": 7.784961349262123e-05,
      "loss": -8.5678,
      "step": 3160
    },
    {
      "epoch": 2.2276879831342233,
      "grad_norm": 341.38323974609375,
      "learning_rate": 7.777933942375264e-05,
      "loss": -8.0313,
      "step": 3170
    },
    {
      "epoch": 2.234715390021082,
      "grad_norm": 244.72068786621094,
      "learning_rate": 7.770906535488405e-05,
      "loss": -7.9912,
      "step": 3180
    },
    {
      "epoch": 2.241742796907941,
      "grad_norm": 82.59126281738281,
      "learning_rate": 7.763879128601546e-05,
      "loss": -8.5919,
      "step": 3190
    },
    {
      "epoch": 2.2487702037947996,
      "grad_norm": 120.18185424804688,
      "learning_rate": 7.756851721714688e-05,
      "loss": -8.474,
      "step": 3200
    },
    {
      "epoch": 2.2557976106816584,
      "grad_norm": 490.7081604003906,
      "learning_rate": 7.74982431482783e-05,
      "loss": -8.4963,
      "step": 3210
    },
    {
      "epoch": 2.262825017568517,
      "grad_norm": 315.64166259765625,
      "learning_rate": 7.74279690794097e-05,
      "loss": -8.5965,
      "step": 3220
    },
    {
      "epoch": 2.269852424455376,
      "grad_norm": 86.918701171875,
      "learning_rate": 7.735769501054111e-05,
      "loss": -8.6432,
      "step": 3230
    },
    {
      "epoch": 2.2768798313422347,
      "grad_norm": 231.6716766357422,
      "learning_rate": 7.728742094167253e-05,
      "loss": -8.685,
      "step": 3240
    },
    {
      "epoch": 2.2839072382290935,
      "grad_norm": 46.87607192993164,
      "learning_rate": 7.721714687280393e-05,
      "loss": -8.6976,
      "step": 3250
    },
    {
      "epoch": 2.2909346451159522,
      "grad_norm": 123.5180892944336,
      "learning_rate": 7.714687280393535e-05,
      "loss": -8.611,
      "step": 3260
    },
    {
      "epoch": 2.297962052002811,
      "grad_norm": 228.0556640625,
      "learning_rate": 7.707659873506677e-05,
      "loss": -8.6457,
      "step": 3270
    },
    {
      "epoch": 2.3049894588896698,
      "grad_norm": 184.5399627685547,
      "learning_rate": 7.700632466619818e-05,
      "loss": -8.6362,
      "step": 3280
    },
    {
      "epoch": 2.3120168657765285,
      "grad_norm": 252.17877197265625,
      "learning_rate": 7.693605059732959e-05,
      "loss": -8.6744,
      "step": 3290
    },
    {
      "epoch": 2.3190442726633873,
      "grad_norm": 164.254150390625,
      "learning_rate": 7.6865776528461e-05,
      "loss": -8.6135,
      "step": 3300
    },
    {
      "epoch": 2.326071679550246,
      "grad_norm": 134.58421325683594,
      "learning_rate": 7.679550245959242e-05,
      "loss": -8.5522,
      "step": 3310
    },
    {
      "epoch": 2.333099086437105,
      "grad_norm": 165.8588104248047,
      "learning_rate": 7.672522839072382e-05,
      "loss": -8.6254,
      "step": 3320
    },
    {
      "epoch": 2.3401264933239636,
      "grad_norm": 215.5231475830078,
      "learning_rate": 7.665495432185524e-05,
      "loss": -8.648,
      "step": 3330
    },
    {
      "epoch": 2.3471539002108224,
      "grad_norm": 87.52716064453125,
      "learning_rate": 7.658468025298665e-05,
      "loss": -8.7261,
      "step": 3340
    },
    {
      "epoch": 2.354181307097681,
      "grad_norm": 224.54930114746094,
      "learning_rate": 7.651440618411806e-05,
      "loss": -8.6498,
      "step": 3350
    },
    {
      "epoch": 2.3612087139845395,
      "grad_norm": 261.2884521484375,
      "learning_rate": 7.644413211524947e-05,
      "loss": -8.5963,
      "step": 3360
    },
    {
      "epoch": 2.3682361208713987,
      "grad_norm": 151.74978637695312,
      "learning_rate": 7.637385804638089e-05,
      "loss": -8.689,
      "step": 3370
    },
    {
      "epoch": 2.375263527758257,
      "grad_norm": 204.74972534179688,
      "learning_rate": 7.630358397751231e-05,
      "loss": -8.5878,
      "step": 3380
    },
    {
      "epoch": 2.3822909346451158,
      "grad_norm": 258.35174560546875,
      "learning_rate": 7.623330990864371e-05,
      "loss": -8.6177,
      "step": 3390
    },
    {
      "epoch": 2.3893183415319745,
      "grad_norm": 113.83328247070312,
      "learning_rate": 7.616303583977513e-05,
      "loss": -8.6938,
      "step": 3400
    },
    {
      "epoch": 2.3963457484188333,
      "grad_norm": 61.120479583740234,
      "learning_rate": 7.609276177090654e-05,
      "loss": -8.7091,
      "step": 3410
    },
    {
      "epoch": 2.403373155305692,
      "grad_norm": 87.94649505615234,
      "learning_rate": 7.602248770203795e-05,
      "loss": -8.6946,
      "step": 3420
    },
    {
      "epoch": 2.410400562192551,
      "grad_norm": 689.940673828125,
      "learning_rate": 7.595221363316936e-05,
      "loss": -8.681,
      "step": 3430
    },
    {
      "epoch": 2.4174279690794096,
      "grad_norm": 91.55699920654297,
      "learning_rate": 7.588193956430078e-05,
      "loss": -8.6879,
      "step": 3440
    },
    {
      "epoch": 2.4244553759662684,
      "grad_norm": 217.81845092773438,
      "learning_rate": 7.58116654954322e-05,
      "loss": -8.7641,
      "step": 3450
    },
    {
      "epoch": 2.431482782853127,
      "grad_norm": 260.87103271484375,
      "learning_rate": 7.57413914265636e-05,
      "loss": -8.8486,
      "step": 3460
    },
    {
      "epoch": 2.438510189739986,
      "grad_norm": 148.4253387451172,
      "learning_rate": 7.567111735769501e-05,
      "loss": -8.7479,
      "step": 3470
    },
    {
      "epoch": 2.4455375966268447,
      "grad_norm": 169.07968139648438,
      "learning_rate": 7.560084328882643e-05,
      "loss": -8.7749,
      "step": 3480
    },
    {
      "epoch": 2.4525650035137034,
      "grad_norm": 103.07903289794922,
      "learning_rate": 7.553056921995783e-05,
      "loss": -8.1678,
      "step": 3490
    },
    {
      "epoch": 2.459592410400562,
      "grad_norm": 42.290470123291016,
      "learning_rate": 7.546029515108925e-05,
      "loss": -8.8987,
      "step": 3500
    },
    {
      "epoch": 2.466619817287421,
      "grad_norm": 165.82789611816406,
      "learning_rate": 7.539002108222067e-05,
      "loss": -8.6965,
      "step": 3510
    },
    {
      "epoch": 2.4736472241742797,
      "grad_norm": 277.54132080078125,
      "learning_rate": 7.531974701335208e-05,
      "loss": -8.7299,
      "step": 3520
    },
    {
      "epoch": 2.4806746310611385,
      "grad_norm": 125.41350555419922,
      "learning_rate": 7.524947294448349e-05,
      "loss": -8.8253,
      "step": 3530
    },
    {
      "epoch": 2.4877020379479973,
      "grad_norm": 170.8343505859375,
      "learning_rate": 7.51791988756149e-05,
      "loss": -8.8679,
      "step": 3540
    },
    {
      "epoch": 2.494729444834856,
      "grad_norm": 58.62925720214844,
      "learning_rate": 7.510892480674632e-05,
      "loss": -8.8178,
      "step": 3550
    },
    {
      "epoch": 2.501756851721715,
      "grad_norm": 125.32413482666016,
      "learning_rate": 7.503865073787772e-05,
      "loss": -8.8724,
      "step": 3560
    },
    {
      "epoch": 2.5087842586085736,
      "grad_norm": 154.8121337890625,
      "learning_rate": 7.496837666900914e-05,
      "loss": -8.7037,
      "step": 3570
    },
    {
      "epoch": 2.5158116654954323,
      "grad_norm": 168.73098754882812,
      "learning_rate": 7.489810260014056e-05,
      "loss": -8.7687,
      "step": 3580
    },
    {
      "epoch": 2.5228390723822907,
      "grad_norm": 484.3903503417969,
      "learning_rate": 7.482782853127196e-05,
      "loss": -8.7116,
      "step": 3590
    },
    {
      "epoch": 2.52986647926915,
      "grad_norm": 130.0102996826172,
      "learning_rate": 7.475755446240337e-05,
      "loss": -8.729,
      "step": 3600
    },
    {
      "epoch": 2.536893886156008,
      "grad_norm": 301.7633056640625,
      "learning_rate": 7.468728039353479e-05,
      "loss": -8.6386,
      "step": 3610
    },
    {
      "epoch": 2.5439212930428674,
      "grad_norm": 70.4582290649414,
      "learning_rate": 7.461700632466621e-05,
      "loss": -8.9356,
      "step": 3620
    },
    {
      "epoch": 2.5509486999297257,
      "grad_norm": 224.5386199951172,
      "learning_rate": 7.454673225579761e-05,
      "loss": -8.8566,
      "step": 3630
    },
    {
      "epoch": 2.557976106816585,
      "grad_norm": 175.39588928222656,
      "learning_rate": 7.447645818692903e-05,
      "loss": -8.7781,
      "step": 3640
    },
    {
      "epoch": 2.5650035137034433,
      "grad_norm": 44.10871505737305,
      "learning_rate": 7.440618411806044e-05,
      "loss": -8.8705,
      "step": 3650
    },
    {
      "epoch": 2.572030920590302,
      "grad_norm": 364.081298828125,
      "learning_rate": 7.433591004919185e-05,
      "loss": -8.1116,
      "step": 3660
    },
    {
      "epoch": 2.579058327477161,
      "grad_norm": 193.93408203125,
      "learning_rate": 7.426563598032326e-05,
      "loss": -8.8242,
      "step": 3670
    },
    {
      "epoch": 2.5860857343640196,
      "grad_norm": 145.48922729492188,
      "learning_rate": 7.419536191145468e-05,
      "loss": -8.8194,
      "step": 3680
    },
    {
      "epoch": 2.5931131412508783,
      "grad_norm": 151.64952087402344,
      "learning_rate": 7.41250878425861e-05,
      "loss": -8.8997,
      "step": 3690
    },
    {
      "epoch": 2.600140548137737,
      "grad_norm": 73.07112884521484,
      "learning_rate": 7.40548137737175e-05,
      "loss": -8.8681,
      "step": 3700
    },
    {
      "epoch": 2.607167955024596,
      "grad_norm": 201.4570770263672,
      "learning_rate": 7.398453970484892e-05,
      "loss": -8.7974,
      "step": 3710
    },
    {
      "epoch": 2.6141953619114546,
      "grad_norm": 71.04912567138672,
      "learning_rate": 7.391426563598033e-05,
      "loss": -8.9505,
      "step": 3720
    },
    {
      "epoch": 2.6212227687983134,
      "grad_norm": 150.9524688720703,
      "learning_rate": 7.384399156711173e-05,
      "loss": -8.3037,
      "step": 3730
    },
    {
      "epoch": 2.628250175685172,
      "grad_norm": 132.1707000732422,
      "learning_rate": 7.377371749824315e-05,
      "loss": -8.9171,
      "step": 3740
    },
    {
      "epoch": 2.635277582572031,
      "grad_norm": 71.94610595703125,
      "learning_rate": 7.370344342937457e-05,
      "loss": -8.9627,
      "step": 3750
    },
    {
      "epoch": 2.6423049894588897,
      "grad_norm": 164.2379608154297,
      "learning_rate": 7.363316936050597e-05,
      "loss": -8.2449,
      "step": 3760
    },
    {
      "epoch": 2.6493323963457485,
      "grad_norm": 111.04104614257812,
      "learning_rate": 7.356289529163739e-05,
      "loss": -8.9378,
      "step": 3770
    },
    {
      "epoch": 2.6563598032326072,
      "grad_norm": 211.9732208251953,
      "learning_rate": 7.34926212227688e-05,
      "loss": -8.7267,
      "step": 3780
    },
    {
      "epoch": 2.663387210119466,
      "grad_norm": 95.69869232177734,
      "learning_rate": 7.342234715390022e-05,
      "loss": -8.8451,
      "step": 3790
    },
    {
      "epoch": 2.6704146170063248,
      "grad_norm": 212.389892578125,
      "learning_rate": 7.335207308503162e-05,
      "loss": -7.5867,
      "step": 3800
    },
    {
      "epoch": 2.6774420238931835,
      "grad_norm": 213.12350463867188,
      "learning_rate": 7.328179901616304e-05,
      "loss": -8.8605,
      "step": 3810
    },
    {
      "epoch": 2.6844694307800423,
      "grad_norm": 216.517333984375,
      "learning_rate": 7.321152494729446e-05,
      "loss": -8.9308,
      "step": 3820
    },
    {
      "epoch": 2.691496837666901,
      "grad_norm": 402.3065185546875,
      "learning_rate": 7.314125087842586e-05,
      "loss": -8.9093,
      "step": 3830
    },
    {
      "epoch": 2.6985242445537594,
      "grad_norm": 171.08599853515625,
      "learning_rate": 7.307097680955728e-05,
      "loss": -8.7313,
      "step": 3840
    },
    {
      "epoch": 2.7055516514406186,
      "grad_norm": 289.0184326171875,
      "learning_rate": 7.300070274068869e-05,
      "loss": -8.7891,
      "step": 3850
    },
    {
      "epoch": 2.712579058327477,
      "grad_norm": 374.5928955078125,
      "learning_rate": 7.293042867182011e-05,
      "loss": -8.8576,
      "step": 3860
    },
    {
      "epoch": 2.719606465214336,
      "grad_norm": 57.69321823120117,
      "learning_rate": 7.286015460295151e-05,
      "loss": -8.9739,
      "step": 3870
    },
    {
      "epoch": 2.7266338721011945,
      "grad_norm": 106.99384307861328,
      "learning_rate": 7.278988053408293e-05,
      "loss": -9.0261,
      "step": 3880
    },
    {
      "epoch": 2.7336612789880537,
      "grad_norm": 193.44566345214844,
      "learning_rate": 7.271960646521434e-05,
      "loss": -9.0087,
      "step": 3890
    },
    {
      "epoch": 2.740688685874912,
      "grad_norm": 107.8094482421875,
      "learning_rate": 7.264933239634575e-05,
      "loss": -8.9441,
      "step": 3900
    },
    {
      "epoch": 2.7477160927617708,
      "grad_norm": 341.1994934082031,
      "learning_rate": 7.257905832747716e-05,
      "loss": -8.9598,
      "step": 3910
    },
    {
      "epoch": 2.7547434996486295,
      "grad_norm": 89.58358764648438,
      "learning_rate": 7.250878425860858e-05,
      "loss": -8.9766,
      "step": 3920
    },
    {
      "epoch": 2.7617709065354883,
      "grad_norm": 100.24834442138672,
      "learning_rate": 7.243851018974e-05,
      "loss": -8.99,
      "step": 3930
    },
    {
      "epoch": 2.768798313422347,
      "grad_norm": 259.99322509765625,
      "learning_rate": 7.23682361208714e-05,
      "loss": -8.2665,
      "step": 3940
    },
    {
      "epoch": 2.775825720309206,
      "grad_norm": 250.20652770996094,
      "learning_rate": 7.229796205200282e-05,
      "loss": -8.9787,
      "step": 3950
    },
    {
      "epoch": 2.7828531271960646,
      "grad_norm": 133.3666534423828,
      "learning_rate": 7.222768798313423e-05,
      "loss": -8.9677,
      "step": 3960
    },
    {
      "epoch": 2.7898805340829234,
      "grad_norm": 274.1477355957031,
      "learning_rate": 7.215741391426564e-05,
      "loss": -8.9262,
      "step": 3970
    },
    {
      "epoch": 2.796907940969782,
      "grad_norm": 173.66744995117188,
      "learning_rate": 7.208713984539705e-05,
      "loss": -8.9989,
      "step": 3980
    },
    {
      "epoch": 2.803935347856641,
      "grad_norm": 263.7831726074219,
      "learning_rate": 7.201686577652847e-05,
      "loss": -9.1023,
      "step": 3990
    },
    {
      "epoch": 2.8109627547434997,
      "grad_norm": 174.24913024902344,
      "learning_rate": 7.194659170765987e-05,
      "loss": -9.0321,
      "step": 4000
    },
    {
      "epoch": 2.8179901616303584,
      "grad_norm": 190.3966522216797,
      "learning_rate": 7.187631763879129e-05,
      "loss": -9.0417,
      "step": 4010
    },
    {
      "epoch": 2.825017568517217,
      "grad_norm": 64.34220123291016,
      "learning_rate": 7.18060435699227e-05,
      "loss": -9.1527,
      "step": 4020
    },
    {
      "epoch": 2.832044975404076,
      "grad_norm": 177.2653045654297,
      "learning_rate": 7.173576950105412e-05,
      "loss": -8.8953,
      "step": 4030
    },
    {
      "epoch": 2.8390723822909347,
      "grad_norm": 188.2138214111328,
      "learning_rate": 7.166549543218552e-05,
      "loss": -9.0743,
      "step": 4040
    },
    {
      "epoch": 2.8460997891777935,
      "grad_norm": 272.4510498046875,
      "learning_rate": 7.159522136331694e-05,
      "loss": -9.0598,
      "step": 4050
    },
    {
      "epoch": 2.8531271960646523,
      "grad_norm": 245.04669189453125,
      "learning_rate": 7.152494729444836e-05,
      "loss": -9.0274,
      "step": 4060
    },
    {
      "epoch": 2.860154602951511,
      "grad_norm": 213.5389404296875,
      "learning_rate": 7.145467322557976e-05,
      "loss": -9.0245,
      "step": 4070
    },
    {
      "epoch": 2.86718200983837,
      "grad_norm": 107.598876953125,
      "learning_rate": 7.138439915671118e-05,
      "loss": -9.1295,
      "step": 4080
    },
    {
      "epoch": 2.874209416725228,
      "grad_norm": 102.602294921875,
      "learning_rate": 7.131412508784259e-05,
      "loss": -9.2079,
      "step": 4090
    },
    {
      "epoch": 2.8812368236120873,
      "grad_norm": 291.1586608886719,
      "learning_rate": 7.124385101897401e-05,
      "loss": -8.9937,
      "step": 4100
    },
    {
      "epoch": 2.8882642304989457,
      "grad_norm": 119.49273681640625,
      "learning_rate": 7.117357695010541e-05,
      "loss": -8.9685,
      "step": 4110
    },
    {
      "epoch": 2.895291637385805,
      "grad_norm": 58.415645599365234,
      "learning_rate": 7.110330288123683e-05,
      "loss": -8.3872,
      "step": 4120
    },
    {
      "epoch": 2.902319044272663,
      "grad_norm": 97.31568908691406,
      "learning_rate": 7.103302881236824e-05,
      "loss": -9.1378,
      "step": 4130
    },
    {
      "epoch": 2.9093464511595224,
      "grad_norm": 54.13397216796875,
      "learning_rate": 7.096275474349965e-05,
      "loss": -9.1776,
      "step": 4140
    },
    {
      "epoch": 2.9163738580463807,
      "grad_norm": 165.9150390625,
      "learning_rate": 7.089248067463106e-05,
      "loss": -9.0784,
      "step": 4150
    },
    {
      "epoch": 2.9234012649332395,
      "grad_norm": 78.35604095458984,
      "learning_rate": 7.082220660576248e-05,
      "loss": -9.1522,
      "step": 4160
    },
    {
      "epoch": 2.9304286718200983,
      "grad_norm": 191.75674438476562,
      "learning_rate": 7.07519325368939e-05,
      "loss": -9.2022,
      "step": 4170
    },
    {
      "epoch": 2.937456078706957,
      "grad_norm": 265.3722229003906,
      "learning_rate": 7.06816584680253e-05,
      "loss": -9.1821,
      "step": 4180
    },
    {
      "epoch": 2.944483485593816,
      "grad_norm": 244.09060668945312,
      "learning_rate": 7.061841180604358e-05,
      "loss": -8.8756,
      "step": 4190
    },
    {
      "epoch": 2.9515108924806746,
      "grad_norm": 76.80900573730469,
      "learning_rate": 7.0548137737175e-05,
      "loss": -9.1553,
      "step": 4200
    },
    {
      "epoch": 2.9585382993675333,
      "grad_norm": 58.860477447509766,
      "learning_rate": 7.04778636683064e-05,
      "loss": -9.2119,
      "step": 4210
    },
    {
      "epoch": 2.965565706254392,
      "grad_norm": 167.87155151367188,
      "learning_rate": 7.04075895994378e-05,
      "loss": -9.1215,
      "step": 4220
    },
    {
      "epoch": 2.972593113141251,
      "grad_norm": 97.35565948486328,
      "learning_rate": 7.033731553056923e-05,
      "loss": -9.0707,
      "step": 4230
    },
    {
      "epoch": 2.9796205200281096,
      "grad_norm": 131.60255432128906,
      "learning_rate": 7.026704146170063e-05,
      "loss": -9.2528,
      "step": 4240
    },
    {
      "epoch": 2.9866479269149684,
      "grad_norm": 122.82623291015625,
      "learning_rate": 7.019676739283205e-05,
      "loss": -9.2308,
      "step": 4250
    },
    {
      "epoch": 2.993675333801827,
      "grad_norm": 73.46363067626953,
      "learning_rate": 7.012649332396347e-05,
      "loss": -9.1556,
      "step": 4260
    },
    {
      "epoch": 3.0,
      "eval_runtime": 10.6885,
      "eval_samples_per_second": 64125.392,
      "eval_steps_per_second": 15.718,
      "step": 4269
    },
    {
      "epoch": 3.000702740688686,
      "grad_norm": 317.7818298339844,
      "learning_rate": 7.005621925509487e-05,
      "loss": -9.0161,
      "step": 4270
    },
    {
      "epoch": 3.0077301475755447,
      "grad_norm": 181.48472595214844,
      "learning_rate": 6.998594518622629e-05,
      "loss": -9.1736,
      "step": 4280
    },
    {
      "epoch": 3.0147575544624035,
      "grad_norm": 75.70367431640625,
      "learning_rate": 6.99156711173577e-05,
      "loss": -9.2918,
      "step": 4290
    },
    {
      "epoch": 3.021784961349262,
      "grad_norm": 105.50285339355469,
      "learning_rate": 6.984539704848912e-05,
      "loss": -9.2493,
      "step": 4300
    },
    {
      "epoch": 3.028812368236121,
      "grad_norm": 91.78470611572266,
      "learning_rate": 6.977512297962052e-05,
      "loss": -9.2013,
      "step": 4310
    },
    {
      "epoch": 3.0358397751229798,
      "grad_norm": 90.83464813232422,
      "learning_rate": 6.970484891075194e-05,
      "loss": -9.149,
      "step": 4320
    },
    {
      "epoch": 3.0428671820098385,
      "grad_norm": 104.18504333496094,
      "learning_rate": 6.963457484188335e-05,
      "loss": -9.0372,
      "step": 4330
    },
    {
      "epoch": 3.0498945888966973,
      "grad_norm": 183.42742919921875,
      "learning_rate": 6.956430077301476e-05,
      "loss": -9.1632,
      "step": 4340
    },
    {
      "epoch": 3.056921995783556,
      "grad_norm": 116.27074432373047,
      "learning_rate": 6.949402670414617e-05,
      "loss": -9.1202,
      "step": 4350
    },
    {
      "epoch": 3.063949402670415,
      "grad_norm": 158.91453552246094,
      "learning_rate": 6.942375263527759e-05,
      "loss": -9.0964,
      "step": 4360
    },
    {
      "epoch": 3.0709768095572736,
      "grad_norm": 639.2203369140625,
      "learning_rate": 6.9353478566409e-05,
      "loss": -9.1161,
      "step": 4370
    },
    {
      "epoch": 3.078004216444132,
      "grad_norm": 370.3136291503906,
      "learning_rate": 6.928320449754041e-05,
      "loss": -8.4117,
      "step": 4380
    },
    {
      "epoch": 3.0850316233309907,
      "grad_norm": 409.3277587890625,
      "learning_rate": 6.921293042867181e-05,
      "loss": -9.1271,
      "step": 4390
    },
    {
      "epoch": 3.0920590302178494,
      "grad_norm": 147.8509521484375,
      "learning_rate": 6.914265635980324e-05,
      "loss": -8.4272,
      "step": 4400
    },
    {
      "epoch": 3.099086437104708,
      "grad_norm": 438.5651550292969,
      "learning_rate": 6.907238229093465e-05,
      "loss": -9.196,
      "step": 4410
    },
    {
      "epoch": 3.106113843991567,
      "grad_norm": 157.9298095703125,
      "learning_rate": 6.900210822206606e-05,
      "loss": -9.2044,
      "step": 4420
    },
    {
      "epoch": 3.1131412508784257,
      "grad_norm": 245.4875946044922,
      "learning_rate": 6.893183415319748e-05,
      "loss": -9.1433,
      "step": 4430
    },
    {
      "epoch": 3.1201686577652845,
      "grad_norm": 245.66226196289062,
      "learning_rate": 6.88615600843289e-05,
      "loss": -9.2921,
      "step": 4440
    },
    {
      "epoch": 3.1271960646521433,
      "grad_norm": 272.2756042480469,
      "learning_rate": 6.87912860154603e-05,
      "loss": -9.2115,
      "step": 4450
    },
    {
      "epoch": 3.134223471539002,
      "grad_norm": 82.20806884765625,
      "learning_rate": 6.87210119465917e-05,
      "loss": -9.323,
      "step": 4460
    },
    {
      "epoch": 3.141250878425861,
      "grad_norm": 83.48697662353516,
      "learning_rate": 6.865073787772313e-05,
      "loss": -9.2051,
      "step": 4470
    },
    {
      "epoch": 3.1482782853127196,
      "grad_norm": 299.7193603515625,
      "learning_rate": 6.858046380885453e-05,
      "loss": -9.0876,
      "step": 4480
    },
    {
      "epoch": 3.1553056921995783,
      "grad_norm": 381.7047424316406,
      "learning_rate": 6.851018973998595e-05,
      "loss": -9.2459,
      "step": 4490
    },
    {
      "epoch": 3.162333099086437,
      "grad_norm": 158.24171447753906,
      "learning_rate": 6.843991567111737e-05,
      "loss": -9.2011,
      "step": 4500
    },
    {
      "epoch": 3.169360505973296,
      "grad_norm": 195.14112854003906,
      "learning_rate": 6.836964160224877e-05,
      "loss": -9.2597,
      "step": 4510
    },
    {
      "epoch": 3.1763879128601546,
      "grad_norm": 198.4600372314453,
      "learning_rate": 6.829936753338019e-05,
      "loss": -9.2076,
      "step": 4520
    },
    {
      "epoch": 3.1834153197470134,
      "grad_norm": 143.38113403320312,
      "learning_rate": 6.822909346451159e-05,
      "loss": -9.3106,
      "step": 4530
    },
    {
      "epoch": 3.190442726633872,
      "grad_norm": 175.71200561523438,
      "learning_rate": 6.815881939564302e-05,
      "loss": -9.3381,
      "step": 4540
    },
    {
      "epoch": 3.197470133520731,
      "grad_norm": 96.3656005859375,
      "learning_rate": 6.808854532677442e-05,
      "loss": -9.2327,
      "step": 4550
    },
    {
      "epoch": 3.2044975404075897,
      "grad_norm": 252.98036193847656,
      "learning_rate": 6.801827125790584e-05,
      "loss": -9.317,
      "step": 4560
    },
    {
      "epoch": 3.2115249472944485,
      "grad_norm": 103.48003387451172,
      "learning_rate": 6.794799718903725e-05,
      "loss": -9.2569,
      "step": 4570
    },
    {
      "epoch": 3.2185523541813073,
      "grad_norm": 288.8014831542969,
      "learning_rate": 6.787772312016866e-05,
      "loss": -9.3468,
      "step": 4580
    },
    {
      "epoch": 3.225579761068166,
      "grad_norm": 172.71327209472656,
      "learning_rate": 6.780744905130007e-05,
      "loss": -9.2644,
      "step": 4590
    },
    {
      "epoch": 3.232607167955025,
      "grad_norm": 235.83187866210938,
      "learning_rate": 6.773717498243149e-05,
      "loss": -9.188,
      "step": 4600
    },
    {
      "epoch": 3.2396345748418836,
      "grad_norm": 167.00633239746094,
      "learning_rate": 6.766690091356291e-05,
      "loss": -9.1988,
      "step": 4610
    },
    {
      "epoch": 3.2466619817287423,
      "grad_norm": 136.1014862060547,
      "learning_rate": 6.759662684469431e-05,
      "loss": -9.2114,
      "step": 4620
    },
    {
      "epoch": 3.2536893886156006,
      "grad_norm": 49.25227737426758,
      "learning_rate": 6.752635277582571e-05,
      "loss": -9.2787,
      "step": 4630
    },
    {
      "epoch": 3.2607167955024594,
      "grad_norm": 187.51524353027344,
      "learning_rate": 6.745607870695714e-05,
      "loss": -9.2623,
      "step": 4640
    },
    {
      "epoch": 3.267744202389318,
      "grad_norm": 122.34105682373047,
      "learning_rate": 6.738580463808855e-05,
      "loss": -8.7257,
      "step": 4650
    },
    {
      "epoch": 3.274771609276177,
      "grad_norm": 152.95921325683594,
      "learning_rate": 6.731553056921996e-05,
      "loss": -8.61,
      "step": 4660
    },
    {
      "epoch": 3.2817990161630357,
      "grad_norm": 138.380126953125,
      "learning_rate": 6.724525650035138e-05,
      "loss": -9.2861,
      "step": 4670
    },
    {
      "epoch": 3.2888264230498945,
      "grad_norm": 49.7448616027832,
      "learning_rate": 6.71749824314828e-05,
      "loss": -9.2808,
      "step": 4680
    },
    {
      "epoch": 3.2958538299367532,
      "grad_norm": 92.5799560546875,
      "learning_rate": 6.71047083626142e-05,
      "loss": -9.2037,
      "step": 4690
    },
    {
      "epoch": 3.302881236823612,
      "grad_norm": 459.4388122558594,
      "learning_rate": 6.70344342937456e-05,
      "loss": -9.3043,
      "step": 4700
    },
    {
      "epoch": 3.3099086437104708,
      "grad_norm": 269.48260498046875,
      "learning_rate": 6.696416022487703e-05,
      "loss": -9.2566,
      "step": 4710
    },
    {
      "epoch": 3.3169360505973295,
      "grad_norm": 139.7371063232422,
      "learning_rate": 6.689388615600843e-05,
      "loss": -9.3661,
      "step": 4720
    },
    {
      "epoch": 3.3239634574841883,
      "grad_norm": 201.1990966796875,
      "learning_rate": 6.682361208713985e-05,
      "loss": -9.2945,
      "step": 4730
    },
    {
      "epoch": 3.330990864371047,
      "grad_norm": 175.8166046142578,
      "learning_rate": 6.675333801827127e-05,
      "loss": -8.7199,
      "step": 4740
    },
    {
      "epoch": 3.338018271257906,
      "grad_norm": 99.28744506835938,
      "learning_rate": 6.668306394940267e-05,
      "loss": -9.4768,
      "step": 4750
    },
    {
      "epoch": 3.3450456781447646,
      "grad_norm": 207.19703674316406,
      "learning_rate": 6.661278988053409e-05,
      "loss": -9.2613,
      "step": 4760
    },
    {
      "epoch": 3.3520730850316234,
      "grad_norm": 401.6954345703125,
      "learning_rate": 6.654251581166549e-05,
      "loss": -9.3727,
      "step": 4770
    },
    {
      "epoch": 3.359100491918482,
      "grad_norm": 271.2758483886719,
      "learning_rate": 6.647224174279692e-05,
      "loss": -9.4398,
      "step": 4780
    },
    {
      "epoch": 3.366127898805341,
      "grad_norm": 143.84329223632812,
      "learning_rate": 6.640196767392832e-05,
      "loss": -9.337,
      "step": 4790
    },
    {
      "epoch": 3.3731553056921997,
      "grad_norm": 275.751708984375,
      "learning_rate": 6.633169360505974e-05,
      "loss": -9.326,
      "step": 4800
    },
    {
      "epoch": 3.3801827125790584,
      "grad_norm": 216.58729553222656,
      "learning_rate": 6.626141953619116e-05,
      "loss": -9.2935,
      "step": 4810
    },
    {
      "epoch": 3.387210119465917,
      "grad_norm": 328.886962890625,
      "learning_rate": 6.619114546732256e-05,
      "loss": -9.2555,
      "step": 4820
    },
    {
      "epoch": 3.394237526352776,
      "grad_norm": 309.67059326171875,
      "learning_rate": 6.612087139845397e-05,
      "loss": -9.4679,
      "step": 4830
    },
    {
      "epoch": 3.4012649332396347,
      "grad_norm": 386.97467041015625,
      "learning_rate": 6.605059732958538e-05,
      "loss": -9.3766,
      "step": 4840
    },
    {
      "epoch": 3.4082923401264935,
      "grad_norm": 88.3870620727539,
      "learning_rate": 6.598032326071681e-05,
      "loss": -9.2959,
      "step": 4850
    },
    {
      "epoch": 3.415319747013352,
      "grad_norm": 80.30619812011719,
      "learning_rate": 6.591004919184821e-05,
      "loss": -9.2769,
      "step": 4860
    },
    {
      "epoch": 3.422347153900211,
      "grad_norm": 206.01023864746094,
      "learning_rate": 6.583977512297961e-05,
      "loss": -9.3775,
      "step": 4870
    },
    {
      "epoch": 3.4293745607870694,
      "grad_norm": 94.6819076538086,
      "learning_rate": 6.576950105411104e-05,
      "loss": -9.5197,
      "step": 4880
    },
    {
      "epoch": 3.436401967673928,
      "grad_norm": 220.16549682617188,
      "learning_rate": 6.569922698524245e-05,
      "loss": -9.405,
      "step": 4890
    },
    {
      "epoch": 3.443429374560787,
      "grad_norm": 323.34552001953125,
      "learning_rate": 6.562895291637386e-05,
      "loss": -9.5317,
      "step": 4900
    },
    {
      "epoch": 3.4504567814476457,
      "grad_norm": 49.12788391113281,
      "learning_rate": 6.555867884750528e-05,
      "loss": -9.3693,
      "step": 4910
    },
    {
      "epoch": 3.4574841883345044,
      "grad_norm": 136.66146850585938,
      "learning_rate": 6.54884047786367e-05,
      "loss": -9.4497,
      "step": 4920
    },
    {
      "epoch": 3.464511595221363,
      "grad_norm": 151.87098693847656,
      "learning_rate": 6.54181307097681e-05,
      "loss": -9.348,
      "step": 4930
    },
    {
      "epoch": 3.471539002108222,
      "grad_norm": 188.59600830078125,
      "learning_rate": 6.53478566408995e-05,
      "loss": -9.4498,
      "step": 4940
    },
    {
      "epoch": 3.4785664089950807,
      "grad_norm": 74.43659973144531,
      "learning_rate": 6.527758257203093e-05,
      "loss": -9.4019,
      "step": 4950
    },
    {
      "epoch": 3.4855938158819395,
      "grad_norm": 317.06658935546875,
      "learning_rate": 6.520730850316233e-05,
      "loss": -9.4068,
      "step": 4960
    },
    {
      "epoch": 3.4926212227687983,
      "grad_norm": 344.8978271484375,
      "learning_rate": 6.513703443429375e-05,
      "loss": -9.4245,
      "step": 4970
    },
    {
      "epoch": 3.499648629655657,
      "grad_norm": 366.8873596191406,
      "learning_rate": 6.506676036542517e-05,
      "loss": -9.5508,
      "step": 4980
    },
    {
      "epoch": 3.506676036542516,
      "grad_norm": 116.49455261230469,
      "learning_rate": 6.499648629655657e-05,
      "loss": -9.5394,
      "step": 4990
    },
    {
      "epoch": 3.5137034434293746,
      "grad_norm": 504.4307861328125,
      "learning_rate": 6.492621222768799e-05,
      "loss": -8.7171,
      "step": 5000
    },
    {
      "epoch": 3.5207308503162333,
      "grad_norm": 222.0172576904297,
      "learning_rate": 6.485593815881939e-05,
      "loss": -9.5121,
      "step": 5010
    },
    {
      "epoch": 3.527758257203092,
      "grad_norm": 273.6445007324219,
      "learning_rate": 6.478566408995082e-05,
      "loss": -9.4716,
      "step": 5020
    },
    {
      "epoch": 3.534785664089951,
      "grad_norm": 313.3985290527344,
      "learning_rate": 6.471539002108222e-05,
      "loss": -9.392,
      "step": 5030
    },
    {
      "epoch": 3.5418130709768096,
      "grad_norm": 88.73362731933594,
      "learning_rate": 6.464511595221364e-05,
      "loss": -9.4257,
      "step": 5040
    },
    {
      "epoch": 3.5488404778636684,
      "grad_norm": 129.03028869628906,
      "learning_rate": 6.457484188334506e-05,
      "loss": -9.6039,
      "step": 5050
    },
    {
      "epoch": 3.555867884750527,
      "grad_norm": 351.9305725097656,
      "learning_rate": 6.450456781447646e-05,
      "loss": -9.5509,
      "step": 5060
    },
    {
      "epoch": 3.562895291637386,
      "grad_norm": 196.45486450195312,
      "learning_rate": 6.443429374560788e-05,
      "loss": -9.5462,
      "step": 5070
    },
    {
      "epoch": 3.5699226985242447,
      "grad_norm": 97.27666473388672,
      "learning_rate": 6.436401967673928e-05,
      "loss": -9.6162,
      "step": 5080
    },
    {
      "epoch": 3.576950105411103,
      "grad_norm": 124.7396240234375,
      "learning_rate": 6.429374560787071e-05,
      "loss": -9.6372,
      "step": 5090
    },
    {
      "epoch": 3.5839775122979622,
      "grad_norm": 123.9557113647461,
      "learning_rate": 6.422347153900211e-05,
      "loss": -9.3242,
      "step": 5100
    },
    {
      "epoch": 3.5910049191848206,
      "grad_norm": 117.28260803222656,
      "learning_rate": 6.415319747013351e-05,
      "loss": -8.7168,
      "step": 5110
    },
    {
      "epoch": 3.5980323260716798,
      "grad_norm": 101.74256896972656,
      "learning_rate": 6.408292340126494e-05,
      "loss": -9.5337,
      "step": 5120
    },
    {
      "epoch": 3.605059732958538,
      "grad_norm": 77.04703521728516,
      "learning_rate": 6.401264933239635e-05,
      "loss": -9.5764,
      "step": 5130
    },
    {
      "epoch": 3.6120871398453973,
      "grad_norm": 108.3470458984375,
      "learning_rate": 6.394237526352776e-05,
      "loss": -9.5225,
      "step": 5140
    },
    {
      "epoch": 3.6191145467322556,
      "grad_norm": 219.52955627441406,
      "learning_rate": 6.387210119465917e-05,
      "loss": -9.5824,
      "step": 5150
    },
    {
      "epoch": 3.6261419536191144,
      "grad_norm": 152.79986572265625,
      "learning_rate": 6.380182712579058e-05,
      "loss": -9.4746,
      "step": 5160
    },
    {
      "epoch": 3.633169360505973,
      "grad_norm": 97.82615661621094,
      "learning_rate": 6.3731553056922e-05,
      "loss": -9.5639,
      "step": 5170
    },
    {
      "epoch": 3.640196767392832,
      "grad_norm": 178.31402587890625,
      "learning_rate": 6.36612789880534e-05,
      "loss": -9.5452,
      "step": 5180
    },
    {
      "epoch": 3.6472241742796907,
      "grad_norm": 121.54655456542969,
      "learning_rate": 6.359100491918483e-05,
      "loss": -9.6187,
      "step": 5190
    },
    {
      "epoch": 3.6542515811665495,
      "grad_norm": 220.9017791748047,
      "learning_rate": 6.352073085031624e-05,
      "loss": -9.6298,
      "step": 5200
    },
    {
      "epoch": 3.6612789880534082,
      "grad_norm": 144.2965545654297,
      "learning_rate": 6.345045678144765e-05,
      "loss": -9.6157,
      "step": 5210
    },
    {
      "epoch": 3.668306394940267,
      "grad_norm": 155.0948486328125,
      "learning_rate": 6.338018271257907e-05,
      "loss": -9.6214,
      "step": 5220
    },
    {
      "epoch": 3.6753338018271258,
      "grad_norm": 163.47679138183594,
      "learning_rate": 6.330990864371047e-05,
      "loss": -9.5478,
      "step": 5230
    },
    {
      "epoch": 3.6823612087139845,
      "grad_norm": 115.3165283203125,
      "learning_rate": 6.323963457484189e-05,
      "loss": -9.6866,
      "step": 5240
    },
    {
      "epoch": 3.6893886156008433,
      "grad_norm": 531.2682495117188,
      "learning_rate": 6.316936050597329e-05,
      "loss": -9.4788,
      "step": 5250
    },
    {
      "epoch": 3.696416022487702,
      "grad_norm": 161.53216552734375,
      "learning_rate": 6.309908643710472e-05,
      "loss": -9.5981,
      "step": 5260
    },
    {
      "epoch": 3.703443429374561,
      "grad_norm": 399.5056457519531,
      "learning_rate": 6.302881236823612e-05,
      "loss": -9.4757,
      "step": 5270
    },
    {
      "epoch": 3.7104708362614196,
      "grad_norm": 213.628173828125,
      "learning_rate": 6.295853829936753e-05,
      "loss": -9.6372,
      "step": 5280
    },
    {
      "epoch": 3.7174982431482784,
      "grad_norm": 174.0318603515625,
      "learning_rate": 6.288826423049896e-05,
      "loss": -9.7072,
      "step": 5290
    },
    {
      "epoch": 3.724525650035137,
      "grad_norm": 93.0772933959961,
      "learning_rate": 6.281799016163036e-05,
      "loss": -9.5237,
      "step": 5300
    },
    {
      "epoch": 3.731553056921996,
      "grad_norm": 177.47023010253906,
      "learning_rate": 6.274771609276178e-05,
      "loss": -9.5806,
      "step": 5310
    },
    {
      "epoch": 3.7385804638088547,
      "grad_norm": 144.05809020996094,
      "learning_rate": 6.267744202389318e-05,
      "loss": -9.6725,
      "step": 5320
    },
    {
      "epoch": 3.7456078706957134,
      "grad_norm": 205.9955596923828,
      "learning_rate": 6.260716795502461e-05,
      "loss": -9.6034,
      "step": 5330
    },
    {
      "epoch": 3.7526352775825718,
      "grad_norm": 265.2481689453125,
      "learning_rate": 6.253689388615601e-05,
      "loss": -9.6457,
      "step": 5340
    },
    {
      "epoch": 3.759662684469431,
      "grad_norm": 131.41285705566406,
      "learning_rate": 6.246661981728741e-05,
      "loss": -9.7314,
      "step": 5350
    },
    {
      "epoch": 3.7666900913562893,
      "grad_norm": 252.281982421875,
      "learning_rate": 6.239634574841884e-05,
      "loss": -8.8322,
      "step": 5360
    },
    {
      "epoch": 3.7737174982431485,
      "grad_norm": 185.1199188232422,
      "learning_rate": 6.232607167955025e-05,
      "loss": -9.673,
      "step": 5370
    },
    {
      "epoch": 3.780744905130007,
      "grad_norm": 161.00546264648438,
      "learning_rate": 6.225579761068166e-05,
      "loss": -9.7307,
      "step": 5380
    },
    {
      "epoch": 3.787772312016866,
      "grad_norm": 148.48870849609375,
      "learning_rate": 6.218552354181307e-05,
      "loss": -9.5964,
      "step": 5390
    },
    {
      "epoch": 3.7947997189037244,
      "grad_norm": 98.59886932373047,
      "learning_rate": 6.211524947294448e-05,
      "loss": -9.7042,
      "step": 5400
    },
    {
      "epoch": 3.801827125790583,
      "grad_norm": 44.2283821105957,
      "learning_rate": 6.20449754040759e-05,
      "loss": -9.7054,
      "step": 5410
    },
    {
      "epoch": 3.808854532677442,
      "grad_norm": 376.626708984375,
      "learning_rate": 6.19747013352073e-05,
      "loss": -9.5873,
      "step": 5420
    },
    {
      "epoch": 3.8158819395643007,
      "grad_norm": 392.6363830566406,
      "learning_rate": 6.190442726633873e-05,
      "loss": -9.5889,
      "step": 5430
    },
    {
      "epoch": 3.8229093464511594,
      "grad_norm": 147.71983337402344,
      "learning_rate": 6.183415319747014e-05,
      "loss": -9.6535,
      "step": 5440
    },
    {
      "epoch": 3.829936753338018,
      "grad_norm": 105.8662338256836,
      "learning_rate": 6.176387912860155e-05,
      "loss": -9.6288,
      "step": 5450
    },
    {
      "epoch": 3.836964160224877,
      "grad_norm": 98.40209197998047,
      "learning_rate": 6.169360505973296e-05,
      "loss": -9.0204,
      "step": 5460
    },
    {
      "epoch": 3.8439915671117357,
      "grad_norm": 76.27021789550781,
      "learning_rate": 6.162333099086437e-05,
      "loss": -9.6059,
      "step": 5470
    },
    {
      "epoch": 3.8510189739985945,
      "grad_norm": 225.92852783203125,
      "learning_rate": 6.155305692199579e-05,
      "loss": -9.7225,
      "step": 5480
    },
    {
      "epoch": 3.8580463808854533,
      "grad_norm": 88.95600891113281,
      "learning_rate": 6.148278285312719e-05,
      "loss": -9.6925,
      "step": 5490
    },
    {
      "epoch": 3.865073787772312,
      "grad_norm": 88.00982666015625,
      "learning_rate": 6.141250878425862e-05,
      "loss": -9.6889,
      "step": 5500
    },
    {
      "epoch": 3.872101194659171,
      "grad_norm": 113.57298278808594,
      "learning_rate": 6.134223471539002e-05,
      "loss": -9.7592,
      "step": 5510
    },
    {
      "epoch": 3.8791286015460296,
      "grad_norm": 324.0041809082031,
      "learning_rate": 6.127196064652143e-05,
      "loss": -9.6139,
      "step": 5520
    },
    {
      "epoch": 3.8861560084328883,
      "grad_norm": 98.5216064453125,
      "learning_rate": 6.120168657765286e-05,
      "loss": -9.7647,
      "step": 5530
    },
    {
      "epoch": 3.893183415319747,
      "grad_norm": 132.6011199951172,
      "learning_rate": 6.113141250878426e-05,
      "loss": -9.7655,
      "step": 5540
    },
    {
      "epoch": 3.900210822206606,
      "grad_norm": 266.7753601074219,
      "learning_rate": 6.106113843991568e-05,
      "loss": -9.7442,
      "step": 5550
    },
    {
      "epoch": 3.9072382290934646,
      "grad_norm": 168.3669891357422,
      "learning_rate": 6.0990864371047086e-05,
      "loss": -9.8447,
      "step": 5560
    },
    {
      "epoch": 3.9142656359803234,
      "grad_norm": 402.8240966796875,
      "learning_rate": 6.09205903021785e-05,
      "loss": -9.7814,
      "step": 5570
    },
    {
      "epoch": 3.921293042867182,
      "grad_norm": 417.2120361328125,
      "learning_rate": 6.085031623330991e-05,
      "loss": -9.0395,
      "step": 5580
    },
    {
      "epoch": 3.9283204497540405,
      "grad_norm": 257.4863586425781,
      "learning_rate": 6.078004216444132e-05,
      "loss": -9.726,
      "step": 5590
    },
    {
      "epoch": 3.9353478566408997,
      "grad_norm": 262.6522521972656,
      "learning_rate": 6.070976809557274e-05,
      "loss": -9.714,
      "step": 5600
    },
    {
      "epoch": 3.942375263527758,
      "grad_norm": 238.28512573242188,
      "learning_rate": 6.063949402670415e-05,
      "loss": -9.8476,
      "step": 5610
    },
    {
      "epoch": 3.9494026704146172,
      "grad_norm": 408.6724853515625,
      "learning_rate": 6.0569219957835565e-05,
      "loss": -9.7787,
      "step": 5620
    },
    {
      "epoch": 3.9564300773014756,
      "grad_norm": 239.36618041992188,
      "learning_rate": 6.0498945888966974e-05,
      "loss": -9.0823,
      "step": 5630
    },
    {
      "epoch": 3.9634574841883348,
      "grad_norm": 98.94427490234375,
      "learning_rate": 6.0428671820098384e-05,
      "loss": -9.7042,
      "step": 5640
    },
    {
      "epoch": 3.970484891075193,
      "grad_norm": 269.2579650878906,
      "learning_rate": 6.03583977512298e-05,
      "loss": -9.1015,
      "step": 5650
    },
    {
      "epoch": 3.977512297962052,
      "grad_norm": 115.36185455322266,
      "learning_rate": 6.028812368236121e-05,
      "loss": -9.7403,
      "step": 5660
    },
    {
      "epoch": 3.9845397048489106,
      "grad_norm": 142.47499084472656,
      "learning_rate": 6.021784961349263e-05,
      "loss": -9.0063,
      "step": 5670
    },
    {
      "epoch": 3.9915671117357694,
      "grad_norm": 179.82542419433594,
      "learning_rate": 6.0147575544624036e-05,
      "loss": -9.8412,
      "step": 5680
    },
    {
      "epoch": 3.998594518622628,
      "grad_norm": 155.80848693847656,
      "learning_rate": 6.007730147575545e-05,
      "loss": -9.8168,
      "step": 5690
    },
    {
      "epoch": 4.0,
      "eval_runtime": 10.6758,
      "eval_samples_per_second": 64201.45,
      "eval_steps_per_second": 15.736,
      "step": 5692
    },
    {
      "epoch": 4.005621925509487,
      "grad_norm": 135.21853637695312,
      "learning_rate": 6.000702740688686e-05,
      "loss": -9.8252,
      "step": 5700
    },
    {
      "epoch": 4.012649332396346,
      "grad_norm": 173.21966552734375,
      "learning_rate": 5.993675333801827e-05,
      "loss": -9.7796,
      "step": 5710
    },
    {
      "epoch": 4.019676739283205,
      "grad_norm": 82.20442199707031,
      "learning_rate": 5.986647926914969e-05,
      "loss": -9.9207,
      "step": 5720
    },
    {
      "epoch": 4.026704146170063,
      "grad_norm": 168.9945526123047,
      "learning_rate": 5.97962052002811e-05,
      "loss": -9.814,
      "step": 5730
    },
    {
      "epoch": 4.033731553056922,
      "grad_norm": 259.4225158691406,
      "learning_rate": 5.9725931131412515e-05,
      "loss": -9.8624,
      "step": 5740
    },
    {
      "epoch": 4.040758959943781,
      "grad_norm": 166.66305541992188,
      "learning_rate": 5.9655657062543925e-05,
      "loss": -9.8793,
      "step": 5750
    },
    {
      "epoch": 4.047786366830639,
      "grad_norm": 168.5748748779297,
      "learning_rate": 5.9585382993675334e-05,
      "loss": -9.8693,
      "step": 5760
    },
    {
      "epoch": 4.054813773717498,
      "grad_norm": 111.2942886352539,
      "learning_rate": 5.951510892480675e-05,
      "loss": -9.7471,
      "step": 5770
    },
    {
      "epoch": 4.061841180604357,
      "grad_norm": 214.3673095703125,
      "learning_rate": 5.944483485593816e-05,
      "loss": -9.1822,
      "step": 5780
    },
    {
      "epoch": 4.068868587491216,
      "grad_norm": 212.40684509277344,
      "learning_rate": 5.937456078706958e-05,
      "loss": -9.6677,
      "step": 5790
    },
    {
      "epoch": 4.075895994378074,
      "grad_norm": 76.41120147705078,
      "learning_rate": 5.930428671820099e-05,
      "loss": -9.5639,
      "step": 5800
    },
    {
      "epoch": 4.082923401264933,
      "grad_norm": 106.04603576660156,
      "learning_rate": 5.92340126493324e-05,
      "loss": -9.8235,
      "step": 5810
    },
    {
      "epoch": 4.089950808151792,
      "grad_norm": 171.58743286132812,
      "learning_rate": 5.916373858046381e-05,
      "loss": -9.7725,
      "step": 5820
    },
    {
      "epoch": 4.096978215038651,
      "grad_norm": 159.66656494140625,
      "learning_rate": 5.909346451159522e-05,
      "loss": -9.9109,
      "step": 5830
    },
    {
      "epoch": 4.104005621925509,
      "grad_norm": 309.5386047363281,
      "learning_rate": 5.902319044272664e-05,
      "loss": -9.1353,
      "step": 5840
    },
    {
      "epoch": 4.111033028812368,
      "grad_norm": 191.5426788330078,
      "learning_rate": 5.895291637385805e-05,
      "loss": -9.7746,
      "step": 5850
    },
    {
      "epoch": 4.118060435699227,
      "grad_norm": 102.30597686767578,
      "learning_rate": 5.8882642304989465e-05,
      "loss": -9.9091,
      "step": 5860
    },
    {
      "epoch": 4.125087842586086,
      "grad_norm": 156.37423706054688,
      "learning_rate": 5.8812368236120875e-05,
      "loss": -9.8079,
      "step": 5870
    },
    {
      "epoch": 4.132115249472944,
      "grad_norm": 142.1815643310547,
      "learning_rate": 5.8742094167252285e-05,
      "loss": -9.9396,
      "step": 5880
    },
    {
      "epoch": 4.1391426563598035,
      "grad_norm": 26.017972946166992,
      "learning_rate": 5.86718200983837e-05,
      "loss": -9.8812,
      "step": 5890
    },
    {
      "epoch": 4.146170063246662,
      "grad_norm": 203.13558959960938,
      "learning_rate": 5.860154602951511e-05,
      "loss": -9.8468,
      "step": 5900
    },
    {
      "epoch": 4.153197470133521,
      "grad_norm": 136.73846435546875,
      "learning_rate": 5.853127196064653e-05,
      "loss": -9.7641,
      "step": 5910
    },
    {
      "epoch": 4.160224877020379,
      "grad_norm": 41.82766342163086,
      "learning_rate": 5.846099789177794e-05,
      "loss": -9.9422,
      "step": 5920
    },
    {
      "epoch": 4.167252283907239,
      "grad_norm": 123.83346557617188,
      "learning_rate": 5.8390723822909353e-05,
      "loss": -9.9438,
      "step": 5930
    },
    {
      "epoch": 4.174279690794097,
      "grad_norm": 72.30961608886719,
      "learning_rate": 5.832044975404076e-05,
      "loss": -10.022,
      "step": 5940
    },
    {
      "epoch": 4.181307097680956,
      "grad_norm": 183.09422302246094,
      "learning_rate": 5.825017568517217e-05,
      "loss": -9.1463,
      "step": 5950
    },
    {
      "epoch": 4.188334504567814,
      "grad_norm": 199.80853271484375,
      "learning_rate": 5.817990161630359e-05,
      "loss": -9.9164,
      "step": 5960
    },
    {
      "epoch": 4.195361911454674,
      "grad_norm": 159.8081512451172,
      "learning_rate": 5.8109627547435e-05,
      "loss": -9.9364,
      "step": 5970
    },
    {
      "epoch": 4.202389318341532,
      "grad_norm": 232.04371643066406,
      "learning_rate": 5.8039353478566416e-05,
      "loss": -9.7226,
      "step": 5980
    },
    {
      "epoch": 4.20941672522839,
      "grad_norm": 117.4449462890625,
      "learning_rate": 5.7969079409697825e-05,
      "loss": -10.0425,
      "step": 5990
    },
    {
      "epoch": 4.2164441321152495,
      "grad_norm": 91.34504699707031,
      "learning_rate": 5.7898805340829235e-05,
      "loss": -9.91,
      "step": 6000
    },
    {
      "epoch": 4.223471539002108,
      "grad_norm": 31.61080551147461,
      "learning_rate": 5.782853127196065e-05,
      "loss": -10.0708,
      "step": 6010
    },
    {
      "epoch": 4.230498945888967,
      "grad_norm": 182.40374755859375,
      "learning_rate": 5.775825720309206e-05,
      "loss": -10.0494,
      "step": 6020
    },
    {
      "epoch": 4.237526352775825,
      "grad_norm": 169.2659454345703,
      "learning_rate": 5.768798313422348e-05,
      "loss": -9.2302,
      "step": 6030
    },
    {
      "epoch": 4.2445537596626846,
      "grad_norm": 39.662078857421875,
      "learning_rate": 5.761770906535489e-05,
      "loss": -9.9248,
      "step": 6040
    },
    {
      "epoch": 4.251581166549543,
      "grad_norm": 124.94937896728516,
      "learning_rate": 5.7547434996486304e-05,
      "loss": -9.9207,
      "step": 6050
    },
    {
      "epoch": 4.258608573436402,
      "grad_norm": 272.2246398925781,
      "learning_rate": 5.7477160927617713e-05,
      "loss": -9.9339,
      "step": 6060
    },
    {
      "epoch": 4.26563598032326,
      "grad_norm": 360.4135437011719,
      "learning_rate": 5.740688685874912e-05,
      "loss": -9.8839,
      "step": 6070
    },
    {
      "epoch": 4.27266338721012,
      "grad_norm": 526.4760131835938,
      "learning_rate": 5.733661278988054e-05,
      "loss": -9.8598,
      "step": 6080
    },
    {
      "epoch": 4.279690794096978,
      "grad_norm": 186.25759887695312,
      "learning_rate": 5.726633872101195e-05,
      "loss": -9.9033,
      "step": 6090
    },
    {
      "epoch": 4.286718200983837,
      "grad_norm": 232.8411102294922,
      "learning_rate": 5.7196064652143366e-05,
      "loss": -9.9274,
      "step": 6100
    },
    {
      "epoch": 4.2937456078706955,
      "grad_norm": 153.59902954101562,
      "learning_rate": 5.7125790583274776e-05,
      "loss": -9.8994,
      "step": 6110
    },
    {
      "epoch": 4.300773014757555,
      "grad_norm": 228.050048828125,
      "learning_rate": 5.7055516514406185e-05,
      "loss": -9.9421,
      "step": 6120
    },
    {
      "epoch": 4.307800421644413,
      "grad_norm": 277.1666564941406,
      "learning_rate": 5.69852424455376e-05,
      "loss": -9.9934,
      "step": 6130
    },
    {
      "epoch": 4.314827828531272,
      "grad_norm": 148.9129638671875,
      "learning_rate": 5.691496837666901e-05,
      "loss": -10.0019,
      "step": 6140
    },
    {
      "epoch": 4.3218552354181305,
      "grad_norm": 25.00386619567871,
      "learning_rate": 5.684469430780043e-05,
      "loss": -9.9232,
      "step": 6150
    },
    {
      "epoch": 4.32888264230499,
      "grad_norm": 158.44869995117188,
      "learning_rate": 5.677442023893184e-05,
      "loss": -9.9966,
      "step": 6160
    },
    {
      "epoch": 4.335910049191848,
      "grad_norm": 175.80003356933594,
      "learning_rate": 5.6704146170063254e-05,
      "loss": -9.9484,
      "step": 6170
    },
    {
      "epoch": 4.342937456078707,
      "grad_norm": 81.95857238769531,
      "learning_rate": 5.6633872101194664e-05,
      "loss": -9.9194,
      "step": 6180
    },
    {
      "epoch": 4.349964862965566,
      "grad_norm": 169.0899658203125,
      "learning_rate": 5.6563598032326073e-05,
      "loss": -10.126,
      "step": 6190
    },
    {
      "epoch": 4.356992269852425,
      "grad_norm": 108.8511734008789,
      "learning_rate": 5.649332396345749e-05,
      "loss": -10.0509,
      "step": 6200
    },
    {
      "epoch": 4.364019676739283,
      "grad_norm": 109.35164642333984,
      "learning_rate": 5.64230498945889e-05,
      "loss": -10.0693,
      "step": 6210
    },
    {
      "epoch": 4.371047083626142,
      "grad_norm": 94.73626708984375,
      "learning_rate": 5.6352775825720316e-05,
      "loss": -10.0577,
      "step": 6220
    },
    {
      "epoch": 4.378074490513001,
      "grad_norm": 42.44414138793945,
      "learning_rate": 5.6282501756851726e-05,
      "loss": -10.0327,
      "step": 6230
    },
    {
      "epoch": 4.38510189739986,
      "grad_norm": 124.15324401855469,
      "learning_rate": 5.6212227687983135e-05,
      "loss": -10.0921,
      "step": 6240
    },
    {
      "epoch": 4.392129304286718,
      "grad_norm": 70.67738342285156,
      "learning_rate": 5.614195361911455e-05,
      "loss": -10.0347,
      "step": 6250
    },
    {
      "epoch": 4.3991567111735765,
      "grad_norm": 137.38519287109375,
      "learning_rate": 5.607167955024596e-05,
      "loss": -10.0324,
      "step": 6260
    },
    {
      "epoch": 4.406184118060436,
      "grad_norm": 194.02764892578125,
      "learning_rate": 5.600140548137738e-05,
      "loss": -10.0314,
      "step": 6270
    },
    {
      "epoch": 4.413211524947294,
      "grad_norm": 115.04463958740234,
      "learning_rate": 5.593113141250879e-05,
      "loss": -10.0531,
      "step": 6280
    },
    {
      "epoch": 4.420238931834153,
      "grad_norm": 99.22288513183594,
      "learning_rate": 5.5860857343640204e-05,
      "loss": -10.1159,
      "step": 6290
    },
    {
      "epoch": 4.427266338721012,
      "grad_norm": 108.36936950683594,
      "learning_rate": 5.5790583274771614e-05,
      "loss": -9.9923,
      "step": 6300
    },
    {
      "epoch": 4.434293745607871,
      "grad_norm": 106.17735290527344,
      "learning_rate": 5.5720309205903024e-05,
      "loss": -10.1083,
      "step": 6310
    },
    {
      "epoch": 4.441321152494729,
      "grad_norm": 111.0859375,
      "learning_rate": 5.565003513703444e-05,
      "loss": -10.1279,
      "step": 6320
    },
    {
      "epoch": 4.448348559381588,
      "grad_norm": 152.1688995361328,
      "learning_rate": 5.557976106816585e-05,
      "loss": -9.9851,
      "step": 6330
    },
    {
      "epoch": 4.455375966268447,
      "grad_norm": 170.07911682128906,
      "learning_rate": 5.5509486999297266e-05,
      "loss": -10.177,
      "step": 6340
    },
    {
      "epoch": 4.462403373155306,
      "grad_norm": 207.82264709472656,
      "learning_rate": 5.5439212930428676e-05,
      "loss": -10.1388,
      "step": 6350
    },
    {
      "epoch": 4.469430780042164,
      "grad_norm": 160.8548583984375,
      "learning_rate": 5.536893886156008e-05,
      "loss": -10.1413,
      "step": 6360
    },
    {
      "epoch": 4.476458186929023,
      "grad_norm": 380.13592529296875,
      "learning_rate": 5.52986647926915e-05,
      "loss": -10.146,
      "step": 6370
    },
    {
      "epoch": 4.483485593815882,
      "grad_norm": 297.97784423828125,
      "learning_rate": 5.522839072382291e-05,
      "loss": -10.0327,
      "step": 6380
    },
    {
      "epoch": 4.490513000702741,
      "grad_norm": 102.40658569335938,
      "learning_rate": 5.515811665495433e-05,
      "loss": -10.02,
      "step": 6390
    },
    {
      "epoch": 4.497540407589599,
      "grad_norm": 90.4447021484375,
      "learning_rate": 5.508784258608574e-05,
      "loss": -10.0419,
      "step": 6400
    },
    {
      "epoch": 4.5045678144764585,
      "grad_norm": 141.5467529296875,
      "learning_rate": 5.501756851721714e-05,
      "loss": -10.1326,
      "step": 6410
    },
    {
      "epoch": 4.511595221363317,
      "grad_norm": 76.3770523071289,
      "learning_rate": 5.4947294448348564e-05,
      "loss": -10.1393,
      "step": 6420
    },
    {
      "epoch": 4.518622628250176,
      "grad_norm": 80.92656707763672,
      "learning_rate": 5.4877020379479974e-05,
      "loss": -10.1669,
      "step": 6430
    },
    {
      "epoch": 4.525650035137034,
      "grad_norm": 118.88487243652344,
      "learning_rate": 5.480674631061139e-05,
      "loss": -10.1384,
      "step": 6440
    },
    {
      "epoch": 4.5326774420238936,
      "grad_norm": 129.108154296875,
      "learning_rate": 5.47364722417428e-05,
      "loss": -10.0545,
      "step": 6450
    },
    {
      "epoch": 4.539704848910752,
      "grad_norm": 146.6282196044922,
      "learning_rate": 5.466619817287422e-05,
      "loss": -10.0096,
      "step": 6460
    },
    {
      "epoch": 4.546732255797611,
      "grad_norm": 191.60569763183594,
      "learning_rate": 5.4595924104005626e-05,
      "loss": -10.1691,
      "step": 6470
    },
    {
      "epoch": 4.553759662684469,
      "grad_norm": 197.75088500976562,
      "learning_rate": 5.452565003513703e-05,
      "loss": -10.1192,
      "step": 6480
    },
    {
      "epoch": 4.560787069571328,
      "grad_norm": 211.834228515625,
      "learning_rate": 5.445537596626845e-05,
      "loss": -9.8955,
      "step": 6490
    },
    {
      "epoch": 4.567814476458187,
      "grad_norm": 103.95446014404297,
      "learning_rate": 5.438510189739986e-05,
      "loss": -10.1271,
      "step": 6500
    },
    {
      "epoch": 4.574841883345046,
      "grad_norm": 318.3213806152344,
      "learning_rate": 5.431482782853128e-05,
      "loss": -10.042,
      "step": 6510
    },
    {
      "epoch": 4.5818692902319045,
      "grad_norm": 130.16734313964844,
      "learning_rate": 5.424455375966269e-05,
      "loss": -10.0279,
      "step": 6520
    },
    {
      "epoch": 4.588896697118763,
      "grad_norm": 211.87098693847656,
      "learning_rate": 5.417427969079409e-05,
      "loss": -10.1969,
      "step": 6530
    },
    {
      "epoch": 4.595924104005622,
      "grad_norm": 113.39210510253906,
      "learning_rate": 5.4104005621925515e-05,
      "loss": -10.2748,
      "step": 6540
    },
    {
      "epoch": 4.60295151089248,
      "grad_norm": 120.87442779541016,
      "learning_rate": 5.4033731553056924e-05,
      "loss": -10.1989,
      "step": 6550
    },
    {
      "epoch": 4.6099789177793395,
      "grad_norm": 125.16686248779297,
      "learning_rate": 5.396345748418834e-05,
      "loss": -10.1143,
      "step": 6560
    },
    {
      "epoch": 4.617006324666198,
      "grad_norm": 208.80833435058594,
      "learning_rate": 5.389318341531975e-05,
      "loss": -9.9921,
      "step": 6570
    },
    {
      "epoch": 4.624033731553057,
      "grad_norm": 138.33230590820312,
      "learning_rate": 5.382290934645117e-05,
      "loss": -10.1493,
      "step": 6580
    },
    {
      "epoch": 4.631061138439915,
      "grad_norm": 138.09194946289062,
      "learning_rate": 5.375263527758258e-05,
      "loss": -9.4189,
      "step": 6590
    },
    {
      "epoch": 4.638088545326775,
      "grad_norm": 103.81805419921875,
      "learning_rate": 5.368236120871398e-05,
      "loss": -10.2329,
      "step": 6600
    },
    {
      "epoch": 4.645115952213633,
      "grad_norm": 247.53970336914062,
      "learning_rate": 5.36120871398454e-05,
      "loss": -10.1963,
      "step": 6610
    },
    {
      "epoch": 4.652143359100492,
      "grad_norm": 134.25393676757812,
      "learning_rate": 5.354181307097681e-05,
      "loss": -10.2629,
      "step": 6620
    },
    {
      "epoch": 4.6591707659873505,
      "grad_norm": 201.0281524658203,
      "learning_rate": 5.347153900210823e-05,
      "loss": -10.0669,
      "step": 6630
    },
    {
      "epoch": 4.66619817287421,
      "grad_norm": 144.41592407226562,
      "learning_rate": 5.340126493323964e-05,
      "loss": -10.195,
      "step": 6640
    },
    {
      "epoch": 4.673225579761068,
      "grad_norm": 117.6099624633789,
      "learning_rate": 5.333099086437104e-05,
      "loss": -10.2671,
      "step": 6650
    },
    {
      "epoch": 4.680252986647927,
      "grad_norm": 89.22895812988281,
      "learning_rate": 5.3260716795502465e-05,
      "loss": -10.2104,
      "step": 6660
    },
    {
      "epoch": 4.6872803935347855,
      "grad_norm": 170.36019897460938,
      "learning_rate": 5.319044272663387e-05,
      "loss": -10.318,
      "step": 6670
    },
    {
      "epoch": 4.694307800421645,
      "grad_norm": 130.11990356445312,
      "learning_rate": 5.312016865776529e-05,
      "loss": -10.2074,
      "step": 6680
    },
    {
      "epoch": 4.701335207308503,
      "grad_norm": 166.97805786132812,
      "learning_rate": 5.30498945888967e-05,
      "loss": -10.366,
      "step": 6690
    },
    {
      "epoch": 4.708362614195362,
      "grad_norm": 80.56045532226562,
      "learning_rate": 5.297962052002812e-05,
      "loss": -10.2899,
      "step": 6700
    },
    {
      "epoch": 4.715390021082221,
      "grad_norm": 157.23289489746094,
      "learning_rate": 5.290934645115953e-05,
      "loss": -9.6214,
      "step": 6710
    },
    {
      "epoch": 4.722417427969079,
      "grad_norm": 111.59538269042969,
      "learning_rate": 5.283907238229093e-05,
      "loss": -10.3623,
      "step": 6720
    },
    {
      "epoch": 4.729444834855938,
      "grad_norm": 153.7710418701172,
      "learning_rate": 5.276879831342235e-05,
      "loss": -10.2318,
      "step": 6730
    },
    {
      "epoch": 4.736472241742797,
      "grad_norm": 131.94607543945312,
      "learning_rate": 5.269852424455376e-05,
      "loss": -10.3841,
      "step": 6740
    },
    {
      "epoch": 4.743499648629656,
      "grad_norm": 104.95012664794922,
      "learning_rate": 5.262825017568518e-05,
      "loss": -10.232,
      "step": 6750
    },
    {
      "epoch": 4.750527055516514,
      "grad_norm": 125.07740783691406,
      "learning_rate": 5.255797610681659e-05,
      "loss": -10.2973,
      "step": 6760
    },
    {
      "epoch": 4.757554462403373,
      "grad_norm": 103.0893325805664,
      "learning_rate": 5.248770203794799e-05,
      "loss": -10.3516,
      "step": 6770
    },
    {
      "epoch": 4.7645818692902315,
      "grad_norm": 107.58157348632812,
      "learning_rate": 5.2417427969079415e-05,
      "loss": -10.282,
      "step": 6780
    },
    {
      "epoch": 4.771609276177091,
      "grad_norm": 193.83030700683594,
      "learning_rate": 5.234715390021082e-05,
      "loss": -10.3788,
      "step": 6790
    },
    {
      "epoch": 4.778636683063949,
      "grad_norm": 155.2691650390625,
      "learning_rate": 5.227687983134224e-05,
      "loss": -10.3269,
      "step": 6800
    },
    {
      "epoch": 4.785664089950808,
      "grad_norm": 115.8348617553711,
      "learning_rate": 5.220660576247365e-05,
      "loss": -10.3148,
      "step": 6810
    },
    {
      "epoch": 4.792691496837667,
      "grad_norm": 183.41162109375,
      "learning_rate": 5.213633169360507e-05,
      "loss": -10.2864,
      "step": 6820
    },
    {
      "epoch": 4.799718903724526,
      "grad_norm": 156.7699432373047,
      "learning_rate": 5.206605762473648e-05,
      "loss": -10.3634,
      "step": 6830
    },
    {
      "epoch": 4.806746310611384,
      "grad_norm": 30.36475372314453,
      "learning_rate": 5.199578355586788e-05,
      "loss": -9.5394,
      "step": 6840
    },
    {
      "epoch": 4.813773717498243,
      "grad_norm": 81.45970153808594,
      "learning_rate": 5.1925509486999303e-05,
      "loss": -10.4215,
      "step": 6850
    },
    {
      "epoch": 4.820801124385102,
      "grad_norm": 148.43038940429688,
      "learning_rate": 5.185523541813071e-05,
      "loss": -10.3822,
      "step": 6860
    },
    {
      "epoch": 4.827828531271961,
      "grad_norm": 163.149169921875,
      "learning_rate": 5.178496134926213e-05,
      "loss": -9.9171,
      "step": 6870
    },
    {
      "epoch": 4.834855938158819,
      "grad_norm": 201.26385498046875,
      "learning_rate": 5.171468728039354e-05,
      "loss": -10.1396,
      "step": 6880
    },
    {
      "epoch": 4.841883345045678,
      "grad_norm": 136.51768493652344,
      "learning_rate": 5.164441321152494e-05,
      "loss": -10.2313,
      "step": 6890
    },
    {
      "epoch": 4.848910751932537,
      "grad_norm": 222.7268524169922,
      "learning_rate": 5.1574139142656365e-05,
      "loss": -10.259,
      "step": 6900
    },
    {
      "epoch": 4.855938158819396,
      "grad_norm": 236.92527770996094,
      "learning_rate": 5.150386507378777e-05,
      "loss": -10.273,
      "step": 6910
    },
    {
      "epoch": 4.862965565706254,
      "grad_norm": 277.9212341308594,
      "learning_rate": 5.144061841180604e-05,
      "loss": -10.1629,
      "step": 6920
    },
    {
      "epoch": 4.8699929725931135,
      "grad_norm": 78.30260467529297,
      "learning_rate": 5.137034434293746e-05,
      "loss": -10.3611,
      "step": 6930
    },
    {
      "epoch": 4.877020379479972,
      "grad_norm": 97.79134368896484,
      "learning_rate": 5.1300070274068865e-05,
      "loss": -10.3846,
      "step": 6940
    },
    {
      "epoch": 4.884047786366831,
      "grad_norm": 131.770751953125,
      "learning_rate": 5.122979620520029e-05,
      "loss": -10.3202,
      "step": 6950
    },
    {
      "epoch": 4.891075193253689,
      "grad_norm": 88.40991973876953,
      "learning_rate": 5.115952213633169e-05,
      "loss": -10.3331,
      "step": 6960
    },
    {
      "epoch": 4.8981026001405485,
      "grad_norm": 95.30252838134766,
      "learning_rate": 5.1089248067463115e-05,
      "loss": -10.4877,
      "step": 6970
    },
    {
      "epoch": 4.905130007027407,
      "grad_norm": 89.72247314453125,
      "learning_rate": 5.1018973998594524e-05,
      "loss": -10.4602,
      "step": 6980
    },
    {
      "epoch": 4.912157413914265,
      "grad_norm": 90.7152099609375,
      "learning_rate": 5.094869992972593e-05,
      "loss": -10.3783,
      "step": 6990
    },
    {
      "epoch": 4.919184820801124,
      "grad_norm": 71.48173522949219,
      "learning_rate": 5.087842586085735e-05,
      "loss": -10.4123,
      "step": 7000
    },
    {
      "epoch": 4.926212227687984,
      "grad_norm": 119.37000274658203,
      "learning_rate": 5.0808151791988754e-05,
      "loss": -10.4157,
      "step": 7010
    },
    {
      "epoch": 4.933239634574842,
      "grad_norm": 86.76856994628906,
      "learning_rate": 5.073787772312018e-05,
      "loss": -10.344,
      "step": 7020
    },
    {
      "epoch": 4.9402670414617,
      "grad_norm": 115.6219711303711,
      "learning_rate": 5.066760365425158e-05,
      "loss": -10.4344,
      "step": 7030
    },
    {
      "epoch": 4.9472944483485595,
      "grad_norm": 148.87686157226562,
      "learning_rate": 5.059732958538299e-05,
      "loss": -10.3677,
      "step": 7040
    },
    {
      "epoch": 4.954321855235418,
      "grad_norm": 134.14170837402344,
      "learning_rate": 5.052705551651441e-05,
      "loss": -9.46,
      "step": 7050
    },
    {
      "epoch": 4.961349262122277,
      "grad_norm": 140.9396514892578,
      "learning_rate": 5.0456781447645816e-05,
      "loss": -10.4702,
      "step": 7060
    },
    {
      "epoch": 4.968376669009135,
      "grad_norm": 282.8132629394531,
      "learning_rate": 5.038650737877724e-05,
      "loss": -10.3222,
      "step": 7070
    },
    {
      "epoch": 4.9754040758959945,
      "grad_norm": 251.8422393798828,
      "learning_rate": 5.031623330990864e-05,
      "loss": -10.3615,
      "step": 7080
    },
    {
      "epoch": 4.982431482782853,
      "grad_norm": 205.54647827148438,
      "learning_rate": 5.0245959241040065e-05,
      "loss": -10.3515,
      "step": 7090
    },
    {
      "epoch": 4.989458889669712,
      "grad_norm": 92.31782531738281,
      "learning_rate": 5.0175685172171475e-05,
      "loss": -10.3968,
      "step": 7100
    },
    {
      "epoch": 4.99648629655657,
      "grad_norm": 71.73724365234375,
      "learning_rate": 5.010541110330288e-05,
      "loss": -10.3624,
      "step": 7110
    },
    {
      "epoch": 5.0,
      "eval_runtime": 10.6798,
      "eval_samples_per_second": 64177.334,
      "eval_steps_per_second": 15.731,
      "step": 7115
    },
    {
      "epoch": 5.00351370344343,
      "grad_norm": 56.970863342285156,
      "learning_rate": 5.00351370344343e-05,
      "loss": -10.466,
      "step": 7120
    },
    {
      "epoch": 5.010541110330288,
      "grad_norm": 123.6391372680664,
      "learning_rate": 4.9964862965565704e-05,
      "loss": -10.4614,
      "step": 7130
    },
    {
      "epoch": 5.017568517217147,
      "grad_norm": 46.691375732421875,
      "learning_rate": 4.989458889669712e-05,
      "loss": -10.4643,
      "step": 7140
    },
    {
      "epoch": 5.0245959241040055,
      "grad_norm": 188.40733337402344,
      "learning_rate": 4.982431482782853e-05,
      "loss": -10.5429,
      "step": 7150
    },
    {
      "epoch": 5.031623330990865,
      "grad_norm": 90.807373046875,
      "learning_rate": 4.9754040758959947e-05,
      "loss": -10.4669,
      "step": 7160
    },
    {
      "epoch": 5.038650737877723,
      "grad_norm": 213.87493896484375,
      "learning_rate": 4.968376669009136e-05,
      "loss": -10.2434,
      "step": 7170
    },
    {
      "epoch": 5.045678144764582,
      "grad_norm": 273.5224609375,
      "learning_rate": 4.961349262122277e-05,
      "loss": -10.4525,
      "step": 7180
    },
    {
      "epoch": 5.0527055516514405,
      "grad_norm": 64.35575103759766,
      "learning_rate": 4.954321855235418e-05,
      "loss": -10.3405,
      "step": 7190
    },
    {
      "epoch": 5.0597329585383,
      "grad_norm": 159.87481689453125,
      "learning_rate": 4.947294448348559e-05,
      "loss": -10.2332,
      "step": 7200
    },
    {
      "epoch": 5.066760365425158,
      "grad_norm": 84.32037353515625,
      "learning_rate": 4.940267041461701e-05,
      "loss": -10.5005,
      "step": 7210
    },
    {
      "epoch": 5.073787772312017,
      "grad_norm": 159.53265380859375,
      "learning_rate": 4.9332396345748425e-05,
      "loss": -10.3766,
      "step": 7220
    },
    {
      "epoch": 5.080815179198876,
      "grad_norm": 124.05982971191406,
      "learning_rate": 4.9262122276879835e-05,
      "loss": -10.5033,
      "step": 7230
    },
    {
      "epoch": 5.087842586085735,
      "grad_norm": 99.57544708251953,
      "learning_rate": 4.919184820801125e-05,
      "loss": -10.5094,
      "step": 7240
    },
    {
      "epoch": 5.094869992972593,
      "grad_norm": 125.31161499023438,
      "learning_rate": 4.9121574139142654e-05,
      "loss": -10.4093,
      "step": 7250
    },
    {
      "epoch": 5.1018973998594515,
      "grad_norm": 167.5478057861328,
      "learning_rate": 4.905130007027407e-05,
      "loss": -10.4401,
      "step": 7260
    },
    {
      "epoch": 5.108924806746311,
      "grad_norm": 189.4644012451172,
      "learning_rate": 4.898102600140548e-05,
      "loss": -10.3008,
      "step": 7270
    },
    {
      "epoch": 5.115952213633169,
      "grad_norm": 111.65300750732422,
      "learning_rate": 4.89107519325369e-05,
      "loss": -10.3962,
      "step": 7280
    },
    {
      "epoch": 5.122979620520028,
      "grad_norm": 100.72711944580078,
      "learning_rate": 4.884047786366831e-05,
      "loss": -10.2762,
      "step": 7290
    },
    {
      "epoch": 5.1300070274068865,
      "grad_norm": 71.52151489257812,
      "learning_rate": 4.877020379479972e-05,
      "loss": -10.4834,
      "step": 7300
    },
    {
      "epoch": 5.137034434293746,
      "grad_norm": 111.95266723632812,
      "learning_rate": 4.869992972593113e-05,
      "loss": -10.3404,
      "step": 7310
    },
    {
      "epoch": 5.144061841180604,
      "grad_norm": 109.61820983886719,
      "learning_rate": 4.862965565706254e-05,
      "loss": -10.3445,
      "step": 7320
    },
    {
      "epoch": 5.151089248067463,
      "grad_norm": 292.87176513671875,
      "learning_rate": 4.855938158819396e-05,
      "loss": -10.4245,
      "step": 7330
    },
    {
      "epoch": 5.158116654954322,
      "grad_norm": 186.84901428222656,
      "learning_rate": 4.848910751932537e-05,
      "loss": -10.4534,
      "step": 7340
    },
    {
      "epoch": 5.165144061841181,
      "grad_norm": 103.13387298583984,
      "learning_rate": 4.8418833450456785e-05,
      "loss": -10.4747,
      "step": 7350
    },
    {
      "epoch": 5.172171468728039,
      "grad_norm": 178.1102752685547,
      "learning_rate": 4.83485593815882e-05,
      "loss": -10.4857,
      "step": 7360
    },
    {
      "epoch": 5.179198875614898,
      "grad_norm": 379.8809509277344,
      "learning_rate": 4.8278285312719604e-05,
      "loss": -10.4342,
      "step": 7370
    },
    {
      "epoch": 5.186226282501757,
      "grad_norm": 143.85743713378906,
      "learning_rate": 4.820801124385102e-05,
      "loss": -9.6685,
      "step": 7380
    },
    {
      "epoch": 5.193253689388616,
      "grad_norm": 197.82049560546875,
      "learning_rate": 4.813773717498243e-05,
      "loss": -10.4943,
      "step": 7390
    },
    {
      "epoch": 5.200281096275474,
      "grad_norm": 94.86923217773438,
      "learning_rate": 4.806746310611385e-05,
      "loss": -10.3358,
      "step": 7400
    },
    {
      "epoch": 5.207308503162333,
      "grad_norm": 71.87676239013672,
      "learning_rate": 4.7997189037245264e-05,
      "loss": -9.7524,
      "step": 7410
    },
    {
      "epoch": 5.214335910049192,
      "grad_norm": 121.42491912841797,
      "learning_rate": 4.792691496837667e-05,
      "loss": -10.4932,
      "step": 7420
    },
    {
      "epoch": 5.221363316936051,
      "grad_norm": 83.93009185791016,
      "learning_rate": 4.785664089950808e-05,
      "loss": -10.5261,
      "step": 7430
    },
    {
      "epoch": 5.228390723822909,
      "grad_norm": 99.71577453613281,
      "learning_rate": 4.778636683063949e-05,
      "loss": -10.5416,
      "step": 7440
    },
    {
      "epoch": 5.2354181307097685,
      "grad_norm": 131.35450744628906,
      "learning_rate": 4.771609276177091e-05,
      "loss": -10.5434,
      "step": 7450
    },
    {
      "epoch": 5.242445537596627,
      "grad_norm": 61.90058135986328,
      "learning_rate": 4.764581869290232e-05,
      "loss": -10.601,
      "step": 7460
    },
    {
      "epoch": 5.249472944483486,
      "grad_norm": 134.8461151123047,
      "learning_rate": 4.7575544624033735e-05,
      "loss": -10.6263,
      "step": 7470
    },
    {
      "epoch": 5.256500351370344,
      "grad_norm": 32.844120025634766,
      "learning_rate": 4.750527055516515e-05,
      "loss": -10.6646,
      "step": 7480
    },
    {
      "epoch": 5.263527758257203,
      "grad_norm": 74.04571533203125,
      "learning_rate": 4.7434996486296555e-05,
      "loss": -10.4617,
      "step": 7490
    },
    {
      "epoch": 5.270555165144062,
      "grad_norm": 195.50836181640625,
      "learning_rate": 4.736472241742797e-05,
      "loss": -10.4811,
      "step": 7500
    },
    {
      "epoch": 5.27758257203092,
      "grad_norm": 68.92803955078125,
      "learning_rate": 4.729444834855938e-05,
      "loss": -10.469,
      "step": 7510
    },
    {
      "epoch": 5.284609978917779,
      "grad_norm": 73.83759307861328,
      "learning_rate": 4.72241742796908e-05,
      "loss": -10.532,
      "step": 7520
    },
    {
      "epoch": 5.291637385804638,
      "grad_norm": 209.0337371826172,
      "learning_rate": 4.7153900210822214e-05,
      "loss": -10.632,
      "step": 7530
    },
    {
      "epoch": 5.298664792691497,
      "grad_norm": 45.108089447021484,
      "learning_rate": 4.7083626141953624e-05,
      "loss": -10.6235,
      "step": 7540
    },
    {
      "epoch": 5.305692199578355,
      "grad_norm": 62.500038146972656,
      "learning_rate": 4.701335207308503e-05,
      "loss": -10.5053,
      "step": 7550
    },
    {
      "epoch": 5.3127196064652145,
      "grad_norm": 104.6729736328125,
      "learning_rate": 4.694307800421644e-05,
      "loss": -10.6229,
      "step": 7560
    },
    {
      "epoch": 5.319747013352073,
      "grad_norm": 208.18380737304688,
      "learning_rate": 4.687280393534786e-05,
      "loss": -10.4053,
      "step": 7570
    },
    {
      "epoch": 5.326774420238932,
      "grad_norm": 179.9315185546875,
      "learning_rate": 4.680252986647927e-05,
      "loss": -10.4278,
      "step": 7580
    },
    {
      "epoch": 5.33380182712579,
      "grad_norm": 99.0923080444336,
      "learning_rate": 4.6732255797610686e-05,
      "loss": -9.7742,
      "step": 7590
    },
    {
      "epoch": 5.3408292340126495,
      "grad_norm": 156.08819580078125,
      "learning_rate": 4.66619817287421e-05,
      "loss": -10.621,
      "step": 7600
    },
    {
      "epoch": 5.347856640899508,
      "grad_norm": 92.06519317626953,
      "learning_rate": 4.6591707659873505e-05,
      "loss": -10.7354,
      "step": 7610
    },
    {
      "epoch": 5.354884047786367,
      "grad_norm": 39.38058090209961,
      "learning_rate": 4.652143359100492e-05,
      "loss": -10.6207,
      "step": 7620
    },
    {
      "epoch": 5.361911454673225,
      "grad_norm": 118.0190658569336,
      "learning_rate": 4.645115952213633e-05,
      "loss": -10.7301,
      "step": 7630
    },
    {
      "epoch": 5.368938861560085,
      "grad_norm": 71.18534851074219,
      "learning_rate": 4.638088545326775e-05,
      "loss": -10.6519,
      "step": 7640
    },
    {
      "epoch": 5.375966268446943,
      "grad_norm": 80.5863037109375,
      "learning_rate": 4.631061138439916e-05,
      "loss": -10.6797,
      "step": 7650
    },
    {
      "epoch": 5.382993675333802,
      "grad_norm": 56.5152473449707,
      "learning_rate": 4.6240337315530574e-05,
      "loss": -10.7565,
      "step": 7660
    },
    {
      "epoch": 5.3900210822206605,
      "grad_norm": 97.28855895996094,
      "learning_rate": 4.6170063246661984e-05,
      "loss": -10.5855,
      "step": 7670
    },
    {
      "epoch": 5.39704848910752,
      "grad_norm": 152.75631713867188,
      "learning_rate": 4.609978917779339e-05,
      "loss": -10.6175,
      "step": 7680
    },
    {
      "epoch": 5.404075895994378,
      "grad_norm": 114.5976791381836,
      "learning_rate": 4.602951510892481e-05,
      "loss": -10.6118,
      "step": 7690
    },
    {
      "epoch": 5.411103302881237,
      "grad_norm": 184.6878662109375,
      "learning_rate": 4.595924104005622e-05,
      "loss": -10.7541,
      "step": 7700
    },
    {
      "epoch": 5.4181307097680955,
      "grad_norm": 133.59927368164062,
      "learning_rate": 4.5888966971187636e-05,
      "loss": -10.6195,
      "step": 7710
    },
    {
      "epoch": 5.425158116654955,
      "grad_norm": 132.27923583984375,
      "learning_rate": 4.581869290231905e-05,
      "loss": -10.5972,
      "step": 7720
    },
    {
      "epoch": 5.432185523541813,
      "grad_norm": 31.710615158081055,
      "learning_rate": 4.5748418833450455e-05,
      "loss": -10.5894,
      "step": 7730
    },
    {
      "epoch": 5.439212930428672,
      "grad_norm": 52.086158752441406,
      "learning_rate": 4.567814476458187e-05,
      "loss": -10.6911,
      "step": 7740
    },
    {
      "epoch": 5.446240337315531,
      "grad_norm": 114.31318664550781,
      "learning_rate": 4.560787069571328e-05,
      "loss": -10.6446,
      "step": 7750
    },
    {
      "epoch": 5.453267744202389,
      "grad_norm": 69.57096862792969,
      "learning_rate": 4.55375966268447e-05,
      "loss": -9.947,
      "step": 7760
    },
    {
      "epoch": 5.460295151089248,
      "grad_norm": 70.24007415771484,
      "learning_rate": 4.546732255797611e-05,
      "loss": -10.5851,
      "step": 7770
    },
    {
      "epoch": 5.4673225579761064,
      "grad_norm": 97.18074798583984,
      "learning_rate": 4.539704848910752e-05,
      "loss": -10.7019,
      "step": 7780
    },
    {
      "epoch": 5.474349964862966,
      "grad_norm": 86.9713363647461,
      "learning_rate": 4.5326774420238934e-05,
      "loss": -10.7631,
      "step": 7790
    },
    {
      "epoch": 5.481377371749824,
      "grad_norm": 312.94097900390625,
      "learning_rate": 4.5256500351370344e-05,
      "loss": -10.7094,
      "step": 7800
    },
    {
      "epoch": 5.488404778636683,
      "grad_norm": 36.125,
      "learning_rate": 4.518622628250176e-05,
      "loss": -10.6208,
      "step": 7810
    },
    {
      "epoch": 5.4954321855235415,
      "grad_norm": 129.2900390625,
      "learning_rate": 4.511595221363317e-05,
      "loss": -10.8165,
      "step": 7820
    },
    {
      "epoch": 5.502459592410401,
      "grad_norm": 114.64822387695312,
      "learning_rate": 4.5045678144764586e-05,
      "loss": -10.725,
      "step": 7830
    },
    {
      "epoch": 5.509486999297259,
      "grad_norm": 112.56828308105469,
      "learning_rate": 4.4975404075895996e-05,
      "loss": -10.0734,
      "step": 7840
    },
    {
      "epoch": 5.516514406184118,
      "grad_norm": 160.81004333496094,
      "learning_rate": 4.4905130007027406e-05,
      "loss": -10.5869,
      "step": 7850
    },
    {
      "epoch": 5.523541813070977,
      "grad_norm": 149.2353973388672,
      "learning_rate": 4.483485593815882e-05,
      "loss": -10.3491,
      "step": 7860
    },
    {
      "epoch": 5.530569219957836,
      "grad_norm": 319.5376892089844,
      "learning_rate": 4.476458186929023e-05,
      "loss": -10.6686,
      "step": 7870
    },
    {
      "epoch": 5.537596626844694,
      "grad_norm": 51.48536682128906,
      "learning_rate": 4.469430780042165e-05,
      "loss": -10.7831,
      "step": 7880
    },
    {
      "epoch": 5.544624033731553,
      "grad_norm": 147.95326232910156,
      "learning_rate": 4.462403373155306e-05,
      "loss": -10.75,
      "step": 7890
    },
    {
      "epoch": 5.551651440618412,
      "grad_norm": 84.73546600341797,
      "learning_rate": 4.455375966268447e-05,
      "loss": -10.6669,
      "step": 7900
    },
    {
      "epoch": 5.558678847505271,
      "grad_norm": 35.28516387939453,
      "learning_rate": 4.4483485593815884e-05,
      "loss": -10.6326,
      "step": 7910
    },
    {
      "epoch": 5.565706254392129,
      "grad_norm": 132.44595336914062,
      "learning_rate": 4.4413211524947294e-05,
      "loss": -10.7699,
      "step": 7920
    },
    {
      "epoch": 5.572733661278988,
      "grad_norm": 249.78347778320312,
      "learning_rate": 4.434293745607871e-05,
      "loss": -10.6508,
      "step": 7930
    },
    {
      "epoch": 5.579761068165847,
      "grad_norm": 222.32620239257812,
      "learning_rate": 4.427266338721012e-05,
      "loss": -10.5397,
      "step": 7940
    },
    {
      "epoch": 5.586788475052706,
      "grad_norm": 276.738525390625,
      "learning_rate": 4.4202389318341536e-05,
      "loss": -10.7799,
      "step": 7950
    },
    {
      "epoch": 5.593815881939564,
      "grad_norm": 97.08280944824219,
      "learning_rate": 4.4132115249472946e-05,
      "loss": -10.7315,
      "step": 7960
    },
    {
      "epoch": 5.6008432888264235,
      "grad_norm": 73.47245025634766,
      "learning_rate": 4.4061841180604356e-05,
      "loss": -10.7706,
      "step": 7970
    },
    {
      "epoch": 5.607870695713282,
      "grad_norm": 333.94732666015625,
      "learning_rate": 4.399156711173577e-05,
      "loss": -10.8186,
      "step": 7980
    },
    {
      "epoch": 5.61489810260014,
      "grad_norm": 69.07772064208984,
      "learning_rate": 4.392129304286718e-05,
      "loss": -10.7739,
      "step": 7990
    },
    {
      "epoch": 5.621925509486999,
      "grad_norm": 218.96400451660156,
      "learning_rate": 4.38510189739986e-05,
      "loss": -10.6641,
      "step": 8000
    },
    {
      "epoch": 5.6289529163738585,
      "grad_norm": 311.2547302246094,
      "learning_rate": 4.378074490513001e-05,
      "loss": -10.733,
      "step": 8010
    },
    {
      "epoch": 5.635980323260717,
      "grad_norm": 65.48285675048828,
      "learning_rate": 4.371047083626142e-05,
      "loss": -9.8037,
      "step": 8020
    },
    {
      "epoch": 5.643007730147575,
      "grad_norm": 105.76399993896484,
      "learning_rate": 4.3640196767392834e-05,
      "loss": -10.7823,
      "step": 8030
    },
    {
      "epoch": 5.650035137034434,
      "grad_norm": 80.71343994140625,
      "learning_rate": 4.3569922698524244e-05,
      "loss": -10.6577,
      "step": 8040
    },
    {
      "epoch": 5.657062543921293,
      "grad_norm": 227.64173889160156,
      "learning_rate": 4.349964862965566e-05,
      "loss": -10.8576,
      "step": 8050
    },
    {
      "epoch": 5.664089950808152,
      "grad_norm": 202.87289428710938,
      "learning_rate": 4.342937456078707e-05,
      "loss": -9.9234,
      "step": 8060
    },
    {
      "epoch": 5.67111735769501,
      "grad_norm": 73.44686126708984,
      "learning_rate": 4.335910049191849e-05,
      "loss": -10.5479,
      "step": 8070
    },
    {
      "epoch": 5.6781447645818695,
      "grad_norm": 117.12659454345703,
      "learning_rate": 4.3288826423049896e-05,
      "loss": -10.8565,
      "step": 8080
    },
    {
      "epoch": 5.685172171468728,
      "grad_norm": 122.18799591064453,
      "learning_rate": 4.3218552354181306e-05,
      "loss": -10.8368,
      "step": 8090
    },
    {
      "epoch": 5.692199578355587,
      "grad_norm": 72.55453491210938,
      "learning_rate": 4.314827828531272e-05,
      "loss": -10.7819,
      "step": 8100
    },
    {
      "epoch": 5.699226985242445,
      "grad_norm": 28.977510452270508,
      "learning_rate": 4.307800421644413e-05,
      "loss": -10.6565,
      "step": 8110
    },
    {
      "epoch": 5.7062543921293045,
      "grad_norm": 62.16973876953125,
      "learning_rate": 4.300773014757555e-05,
      "loss": -10.8307,
      "step": 8120
    },
    {
      "epoch": 5.713281799016163,
      "grad_norm": 74.65331268310547,
      "learning_rate": 4.293745607870696e-05,
      "loss": -10.8997,
      "step": 8130
    },
    {
      "epoch": 5.720309205903022,
      "grad_norm": 106.90243530273438,
      "learning_rate": 4.286718200983837e-05,
      "loss": -10.6599,
      "step": 8140
    },
    {
      "epoch": 5.72733661278988,
      "grad_norm": 84.15872955322266,
      "learning_rate": 4.2796907940969785e-05,
      "loss": -10.8267,
      "step": 8150
    },
    {
      "epoch": 5.73436401967674,
      "grad_norm": 141.98915100097656,
      "learning_rate": 4.2726633872101194e-05,
      "loss": -10.8054,
      "step": 8160
    },
    {
      "epoch": 5.741391426563598,
      "grad_norm": 72.91476440429688,
      "learning_rate": 4.265635980323261e-05,
      "loss": -10.7742,
      "step": 8170
    },
    {
      "epoch": 5.748418833450457,
      "grad_norm": 72.85612487792969,
      "learning_rate": 4.258608573436402e-05,
      "loss": -10.8036,
      "step": 8180
    },
    {
      "epoch": 5.7554462403373154,
      "grad_norm": 141.35997009277344,
      "learning_rate": 4.251581166549544e-05,
      "loss": -10.7383,
      "step": 8190
    },
    {
      "epoch": 5.762473647224175,
      "grad_norm": 118.41313934326172,
      "learning_rate": 4.244553759662685e-05,
      "loss": -10.692,
      "step": 8200
    },
    {
      "epoch": 5.769501054111033,
      "grad_norm": 97.96253204345703,
      "learning_rate": 4.2375263527758256e-05,
      "loss": -10.8178,
      "step": 8210
    },
    {
      "epoch": 5.776528460997891,
      "grad_norm": 64.35005950927734,
      "learning_rate": 4.230498945888967e-05,
      "loss": -10.8315,
      "step": 8220
    },
    {
      "epoch": 5.7835558678847505,
      "grad_norm": 184.21107482910156,
      "learning_rate": 4.223471539002108e-05,
      "loss": -10.7008,
      "step": 8230
    },
    {
      "epoch": 5.79058327477161,
      "grad_norm": 415.4342956542969,
      "learning_rate": 4.21644413211525e-05,
      "loss": -9.1952,
      "step": 8240
    },
    {
      "epoch": 5.797610681658468,
      "grad_norm": 150.5004119873047,
      "learning_rate": 4.209416725228391e-05,
      "loss": -10.9074,
      "step": 8250
    },
    {
      "epoch": 5.804638088545326,
      "grad_norm": 254.24354553222656,
      "learning_rate": 4.202389318341532e-05,
      "loss": -10.0741,
      "step": 8260
    },
    {
      "epoch": 5.811665495432186,
      "grad_norm": 163.65538024902344,
      "learning_rate": 4.1953619114546735e-05,
      "loss": -10.7865,
      "step": 8270
    },
    {
      "epoch": 5.818692902319044,
      "grad_norm": 55.777976989746094,
      "learning_rate": 4.1883345045678145e-05,
      "loss": -10.9033,
      "step": 8280
    },
    {
      "epoch": 5.825720309205903,
      "grad_norm": 127.98516082763672,
      "learning_rate": 4.181307097680956e-05,
      "loss": -10.7411,
      "step": 8290
    },
    {
      "epoch": 5.832747716092761,
      "grad_norm": 177.4141845703125,
      "learning_rate": 4.174279690794097e-05,
      "loss": -9.428,
      "step": 8300
    },
    {
      "epoch": 5.839775122979621,
      "grad_norm": 132.98890686035156,
      "learning_rate": 4.167252283907239e-05,
      "loss": -10.1605,
      "step": 8310
    },
    {
      "epoch": 5.846802529866479,
      "grad_norm": 56.748268127441406,
      "learning_rate": 4.16022487702038e-05,
      "loss": -10.989,
      "step": 8320
    },
    {
      "epoch": 5.853829936753338,
      "grad_norm": 100.68257141113281,
      "learning_rate": 4.153197470133521e-05,
      "loss": -10.7401,
      "step": 8330
    },
    {
      "epoch": 5.8608573436401965,
      "grad_norm": 97.37796020507812,
      "learning_rate": 4.146170063246662e-05,
      "loss": -10.6845,
      "step": 8340
    },
    {
      "epoch": 5.867884750527056,
      "grad_norm": 17.748449325561523,
      "learning_rate": 4.139142656359803e-05,
      "loss": -10.8136,
      "step": 8350
    },
    {
      "epoch": 5.874912157413914,
      "grad_norm": 50.42338943481445,
      "learning_rate": 4.132115249472945e-05,
      "loss": -10.5833,
      "step": 8360
    },
    {
      "epoch": 5.881939564300773,
      "grad_norm": 139.1417694091797,
      "learning_rate": 4.125087842586086e-05,
      "loss": -10.8828,
      "step": 8370
    },
    {
      "epoch": 5.888966971187632,
      "grad_norm": 137.6279754638672,
      "learning_rate": 4.118060435699227e-05,
      "loss": -10.9303,
      "step": 8380
    },
    {
      "epoch": 5.895994378074491,
      "grad_norm": 43.825523376464844,
      "learning_rate": 4.1110330288123685e-05,
      "loss": -10.944,
      "step": 8390
    },
    {
      "epoch": 5.903021784961349,
      "grad_norm": 78.07955932617188,
      "learning_rate": 4.1040056219255095e-05,
      "loss": -10.9675,
      "step": 8400
    },
    {
      "epoch": 5.910049191848208,
      "grad_norm": 99.67745208740234,
      "learning_rate": 4.096978215038651e-05,
      "loss": -10.9529,
      "step": 8410
    },
    {
      "epoch": 5.917076598735067,
      "grad_norm": 102.05054473876953,
      "learning_rate": 4.089950808151792e-05,
      "loss": -10.8574,
      "step": 8420
    },
    {
      "epoch": 5.924104005621926,
      "grad_norm": 54.41380310058594,
      "learning_rate": 4.082923401264934e-05,
      "loss": -10.8658,
      "step": 8430
    },
    {
      "epoch": 5.931131412508784,
      "grad_norm": 81.5078125,
      "learning_rate": 4.075895994378075e-05,
      "loss": -10.7455,
      "step": 8440
    },
    {
      "epoch": 5.938158819395643,
      "grad_norm": 151.41990661621094,
      "learning_rate": 4.068868587491216e-05,
      "loss": -10.1615,
      "step": 8450
    },
    {
      "epoch": 5.945186226282502,
      "grad_norm": 177.70960998535156,
      "learning_rate": 4.0618411806043574e-05,
      "loss": -10.0815,
      "step": 8460
    },
    {
      "epoch": 5.952213633169361,
      "grad_norm": 106.48606872558594,
      "learning_rate": 4.054813773717498e-05,
      "loss": -10.847,
      "step": 8470
    },
    {
      "epoch": 5.959241040056219,
      "grad_norm": 54.23075485229492,
      "learning_rate": 4.04778636683064e-05,
      "loss": -10.9558,
      "step": 8480
    },
    {
      "epoch": 5.966268446943078,
      "grad_norm": 141.2365264892578,
      "learning_rate": 4.040758959943781e-05,
      "loss": -10.977,
      "step": 8490
    },
    {
      "epoch": 5.973295853829937,
      "grad_norm": 168.42735290527344,
      "learning_rate": 4.033731553056922e-05,
      "loss": -10.9787,
      "step": 8500
    },
    {
      "epoch": 5.980323260716796,
      "grad_norm": 353.8116455078125,
      "learning_rate": 4.0267041461700636e-05,
      "loss": -10.645,
      "step": 8510
    },
    {
      "epoch": 5.987350667603654,
      "grad_norm": 160.56712341308594,
      "learning_rate": 4.0196767392832045e-05,
      "loss": -10.9272,
      "step": 8520
    },
    {
      "epoch": 5.994378074490513,
      "grad_norm": 91.20626831054688,
      "learning_rate": 4.012649332396346e-05,
      "loss": -10.7572,
      "step": 8530
    },
    {
      "epoch": 6.0,
      "eval_runtime": 10.7181,
      "eval_samples_per_second": 63948.186,
      "eval_steps_per_second": 15.674,
      "step": 8538
    },
    {
      "epoch": 6.001405481377372,
      "grad_norm": 83.55549621582031,
      "learning_rate": 4.005621925509487e-05,
      "loss": -10.9579,
      "step": 8540
    },
    {
      "epoch": 6.00843288826423,
      "grad_norm": 65.09931945800781,
      "learning_rate": 3.998594518622629e-05,
      "loss": -10.9966,
      "step": 8550
    },
    {
      "epoch": 6.015460295151089,
      "grad_norm": 166.79222106933594,
      "learning_rate": 3.99156711173577e-05,
      "loss": -11.0448,
      "step": 8560
    },
    {
      "epoch": 6.022487702037948,
      "grad_norm": 127.12399291992188,
      "learning_rate": 3.984539704848911e-05,
      "loss": -10.8151,
      "step": 8570
    },
    {
      "epoch": 6.029515108924807,
      "grad_norm": 267.1951904296875,
      "learning_rate": 3.9775122979620524e-05,
      "loss": -10.029,
      "step": 8580
    },
    {
      "epoch": 6.036542515811665,
      "grad_norm": 85.27837371826172,
      "learning_rate": 3.9704848910751934e-05,
      "loss": -10.8621,
      "step": 8590
    },
    {
      "epoch": 6.043569922698524,
      "grad_norm": 119.15767669677734,
      "learning_rate": 3.963457484188335e-05,
      "loss": -10.9007,
      "step": 8600
    },
    {
      "epoch": 6.050597329585383,
      "grad_norm": 69.52227020263672,
      "learning_rate": 3.956430077301476e-05,
      "loss": -10.8835,
      "step": 8610
    },
    {
      "epoch": 6.057624736472242,
      "grad_norm": 164.41683959960938,
      "learning_rate": 3.949402670414617e-05,
      "loss": -11.0155,
      "step": 8620
    },
    {
      "epoch": 6.0646521433591,
      "grad_norm": 178.8751220703125,
      "learning_rate": 3.9423752635277586e-05,
      "loss": -10.7478,
      "step": 8630
    },
    {
      "epoch": 6.0716795502459595,
      "grad_norm": 141.08653259277344,
      "learning_rate": 3.9353478566408996e-05,
      "loss": -10.9002,
      "step": 8640
    },
    {
      "epoch": 6.078706957132818,
      "grad_norm": 62.78038024902344,
      "learning_rate": 3.928320449754041e-05,
      "loss": -11.0298,
      "step": 8650
    },
    {
      "epoch": 6.085734364019677,
      "grad_norm": 91.89778900146484,
      "learning_rate": 3.921293042867182e-05,
      "loss": -10.9879,
      "step": 8660
    },
    {
      "epoch": 6.092761770906535,
      "grad_norm": 48.40869140625,
      "learning_rate": 3.914265635980324e-05,
      "loss": -10.9985,
      "step": 8670
    },
    {
      "epoch": 6.099789177793395,
      "grad_norm": 85.7077865600586,
      "learning_rate": 3.907238229093465e-05,
      "loss": -11.0396,
      "step": 8680
    },
    {
      "epoch": 6.106816584680253,
      "grad_norm": 104.96341705322266,
      "learning_rate": 3.900210822206606e-05,
      "loss": -11.1215,
      "step": 8690
    },
    {
      "epoch": 6.113843991567112,
      "grad_norm": 45.80128479003906,
      "learning_rate": 3.8931834153197474e-05,
      "loss": -10.89,
      "step": 8700
    },
    {
      "epoch": 6.12087139845397,
      "grad_norm": 85.84935760498047,
      "learning_rate": 3.8861560084328884e-05,
      "loss": -10.8671,
      "step": 8710
    },
    {
      "epoch": 6.12789880534083,
      "grad_norm": 144.24099731445312,
      "learning_rate": 3.87912860154603e-05,
      "loss": -10.8784,
      "step": 8720
    },
    {
      "epoch": 6.134926212227688,
      "grad_norm": 65.66551971435547,
      "learning_rate": 3.872101194659171e-05,
      "loss": -11.0076,
      "step": 8730
    },
    {
      "epoch": 6.141953619114547,
      "grad_norm": 138.4227294921875,
      "learning_rate": 3.865073787772312e-05,
      "loss": -10.8335,
      "step": 8740
    },
    {
      "epoch": 6.1489810260014055,
      "grad_norm": 220.99449157714844,
      "learning_rate": 3.8580463808854536e-05,
      "loss": -10.2662,
      "step": 8750
    },
    {
      "epoch": 6.156008432888264,
      "grad_norm": 57.42300796508789,
      "learning_rate": 3.8510189739985946e-05,
      "loss": -11.1178,
      "step": 8760
    },
    {
      "epoch": 6.163035839775123,
      "grad_norm": 101.20222473144531,
      "learning_rate": 3.843991567111736e-05,
      "loss": -10.9128,
      "step": 8770
    },
    {
      "epoch": 6.170063246661981,
      "grad_norm": 209.81768798828125,
      "learning_rate": 3.836964160224877e-05,
      "loss": -10.9716,
      "step": 8780
    },
    {
      "epoch": 6.177090653548841,
      "grad_norm": 66.0051498413086,
      "learning_rate": 3.829936753338019e-05,
      "loss": -10.8111,
      "step": 8790
    },
    {
      "epoch": 6.184118060435699,
      "grad_norm": 99.8890609741211,
      "learning_rate": 3.82290934645116e-05,
      "loss": -11.1416,
      "step": 8800
    },
    {
      "epoch": 6.191145467322558,
      "grad_norm": 58.6901969909668,
      "learning_rate": 3.815881939564301e-05,
      "loss": -10.3411,
      "step": 8810
    },
    {
      "epoch": 6.198172874209416,
      "grad_norm": 61.576446533203125,
      "learning_rate": 3.8088545326774424e-05,
      "loss": -11.0278,
      "step": 8820
    },
    {
      "epoch": 6.205200281096276,
      "grad_norm": 144.91696166992188,
      "learning_rate": 3.8018271257905834e-05,
      "loss": -10.9,
      "step": 8830
    },
    {
      "epoch": 6.212227687983134,
      "grad_norm": 65.53587341308594,
      "learning_rate": 3.794799718903725e-05,
      "loss": -11.142,
      "step": 8840
    },
    {
      "epoch": 6.219255094869993,
      "grad_norm": 71.6830825805664,
      "learning_rate": 3.787772312016866e-05,
      "loss": -10.9401,
      "step": 8850
    },
    {
      "epoch": 6.2262825017568515,
      "grad_norm": 27.0252742767334,
      "learning_rate": 3.780744905130007e-05,
      "loss": -10.8397,
      "step": 8860
    },
    {
      "epoch": 6.233309908643711,
      "grad_norm": 74.68505859375,
      "learning_rate": 3.7737174982431486e-05,
      "loss": -10.3241,
      "step": 8870
    },
    {
      "epoch": 6.240337315530569,
      "grad_norm": 89.63845825195312,
      "learning_rate": 3.7666900913562896e-05,
      "loss": -10.992,
      "step": 8880
    },
    {
      "epoch": 6.247364722417428,
      "grad_norm": 140.85418701171875,
      "learning_rate": 3.759662684469431e-05,
      "loss": -11.016,
      "step": 8890
    },
    {
      "epoch": 6.254392129304287,
      "grad_norm": 138.5464630126953,
      "learning_rate": 3.752635277582572e-05,
      "loss": -10.9117,
      "step": 8900
    },
    {
      "epoch": 6.261419536191146,
      "grad_norm": 60.13703536987305,
      "learning_rate": 3.745607870695714e-05,
      "loss": -11.1758,
      "step": 8910
    },
    {
      "epoch": 6.268446943078004,
      "grad_norm": 138.2438507080078,
      "learning_rate": 3.738580463808855e-05,
      "loss": -10.9684,
      "step": 8920
    },
    {
      "epoch": 6.275474349964863,
      "grad_norm": 131.01622009277344,
      "learning_rate": 3.731553056921996e-05,
      "loss": -10.9886,
      "step": 8930
    },
    {
      "epoch": 6.282501756851722,
      "grad_norm": 60.88646697998047,
      "learning_rate": 3.7245256500351375e-05,
      "loss": -11.1907,
      "step": 8940
    },
    {
      "epoch": 6.289529163738581,
      "grad_norm": 42.740440368652344,
      "learning_rate": 3.7174982431482784e-05,
      "loss": -11.1168,
      "step": 8950
    },
    {
      "epoch": 6.296556570625439,
      "grad_norm": 95.82910919189453,
      "learning_rate": 3.71047083626142e-05,
      "loss": -10.7465,
      "step": 8960
    },
    {
      "epoch": 6.303583977512298,
      "grad_norm": 53.27954864501953,
      "learning_rate": 3.703443429374561e-05,
      "loss": -11.0205,
      "step": 8970
    },
    {
      "epoch": 6.310611384399157,
      "grad_norm": 96.53538513183594,
      "learning_rate": 3.696416022487702e-05,
      "loss": -10.7757,
      "step": 8980
    },
    {
      "epoch": 6.317638791286015,
      "grad_norm": 93.8287124633789,
      "learning_rate": 3.689388615600844e-05,
      "loss": -10.2161,
      "step": 8990
    },
    {
      "epoch": 6.324666198172874,
      "grad_norm": 68.57683563232422,
      "learning_rate": 3.6823612087139846e-05,
      "loss": -11.033,
      "step": 9000
    },
    {
      "epoch": 6.3316936050597326,
      "grad_norm": 103.99491882324219,
      "learning_rate": 3.675333801827126e-05,
      "loss": -11.1428,
      "step": 9010
    },
    {
      "epoch": 6.338721011946592,
      "grad_norm": 127.37325286865234,
      "learning_rate": 3.668306394940267e-05,
      "loss": -11.0178,
      "step": 9020
    },
    {
      "epoch": 6.34574841883345,
      "grad_norm": 52.87324905395508,
      "learning_rate": 3.661278988053409e-05,
      "loss": -11.1571,
      "step": 9030
    },
    {
      "epoch": 6.352775825720309,
      "grad_norm": 46.125186920166016,
      "learning_rate": 3.654251581166549e-05,
      "loss": -11.1679,
      "step": 9040
    },
    {
      "epoch": 6.359803232607168,
      "grad_norm": 77.6430892944336,
      "learning_rate": 3.647224174279691e-05,
      "loss": -11.097,
      "step": 9050
    },
    {
      "epoch": 6.366830639494027,
      "grad_norm": 147.73020935058594,
      "learning_rate": 3.6401967673928325e-05,
      "loss": -11.0785,
      "step": 9060
    },
    {
      "epoch": 6.373858046380885,
      "grad_norm": 83.517333984375,
      "learning_rate": 3.6331693605059735e-05,
      "loss": -10.6615,
      "step": 9070
    },
    {
      "epoch": 6.380885453267744,
      "grad_norm": 65.90301513671875,
      "learning_rate": 3.626141953619115e-05,
      "loss": -10.9228,
      "step": 9080
    },
    {
      "epoch": 6.387912860154603,
      "grad_norm": 69.22330474853516,
      "learning_rate": 3.6191145467322554e-05,
      "loss": -10.9591,
      "step": 9090
    },
    {
      "epoch": 6.394940267041462,
      "grad_norm": 83.91101837158203,
      "learning_rate": 3.612087139845397e-05,
      "loss": -11.1463,
      "step": 9100
    },
    {
      "epoch": 6.40196767392832,
      "grad_norm": 94.52617645263672,
      "learning_rate": 3.605059732958539e-05,
      "loss": -11.1131,
      "step": 9110
    },
    {
      "epoch": 6.408995080815179,
      "grad_norm": 180.4928741455078,
      "learning_rate": 3.59803232607168e-05,
      "loss": -11.0618,
      "step": 9120
    },
    {
      "epoch": 6.416022487702038,
      "grad_norm": 138.03671264648438,
      "learning_rate": 3.591004919184821e-05,
      "loss": -10.8285,
      "step": 9130
    },
    {
      "epoch": 6.423049894588897,
      "grad_norm": 66.9731674194336,
      "learning_rate": 3.583977512297962e-05,
      "loss": -11.0129,
      "step": 9140
    },
    {
      "epoch": 6.430077301475755,
      "grad_norm": 74.76258087158203,
      "learning_rate": 3.576950105411103e-05,
      "loss": -11.1371,
      "step": 9150
    },
    {
      "epoch": 6.4371047083626145,
      "grad_norm": 47.187461853027344,
      "learning_rate": 3.569922698524244e-05,
      "loss": -10.4727,
      "step": 9160
    },
    {
      "epoch": 6.444132115249473,
      "grad_norm": 68.04261779785156,
      "learning_rate": 3.562895291637386e-05,
      "loss": -11.2572,
      "step": 9170
    },
    {
      "epoch": 6.451159522136332,
      "grad_norm": 43.7583122253418,
      "learning_rate": 3.5558678847505275e-05,
      "loss": -11.1198,
      "step": 9180
    },
    {
      "epoch": 6.45818692902319,
      "grad_norm": 64.83019256591797,
      "learning_rate": 3.5488404778636685e-05,
      "loss": -11.2709,
      "step": 9190
    },
    {
      "epoch": 6.46521433591005,
      "grad_norm": 60.760982513427734,
      "learning_rate": 3.54181307097681e-05,
      "loss": -11.0819,
      "step": 9200
    },
    {
      "epoch": 6.472241742796908,
      "grad_norm": 81.359130859375,
      "learning_rate": 3.5347856640899504e-05,
      "loss": -11.295,
      "step": 9210
    },
    {
      "epoch": 6.479269149683767,
      "grad_norm": 87.49689483642578,
      "learning_rate": 3.527758257203092e-05,
      "loss": -11.2749,
      "step": 9220
    },
    {
      "epoch": 6.486296556570625,
      "grad_norm": 95.69856262207031,
      "learning_rate": 3.520730850316234e-05,
      "loss": -11.0298,
      "step": 9230
    },
    {
      "epoch": 6.493323963457485,
      "grad_norm": 69.12765502929688,
      "learning_rate": 3.513703443429375e-05,
      "loss": -11.0598,
      "step": 9240
    },
    {
      "epoch": 6.500351370344343,
      "grad_norm": 126.15137481689453,
      "learning_rate": 3.5066760365425163e-05,
      "loss": -11.0413,
      "step": 9250
    },
    {
      "epoch": 6.507378777231201,
      "grad_norm": 50.31019592285156,
      "learning_rate": 3.499648629655657e-05,
      "loss": -11.17,
      "step": 9260
    },
    {
      "epoch": 6.5144061841180605,
      "grad_norm": 167.79039001464844,
      "learning_rate": 3.492621222768798e-05,
      "loss": -11.1886,
      "step": 9270
    },
    {
      "epoch": 6.521433591004919,
      "grad_norm": 106.04161834716797,
      "learning_rate": 3.485593815881939e-05,
      "loss": -11.1993,
      "step": 9280
    },
    {
      "epoch": 6.528460997891778,
      "grad_norm": 81.0169906616211,
      "learning_rate": 3.478566408995081e-05,
      "loss": -11.2271,
      "step": 9290
    },
    {
      "epoch": 6.535488404778636,
      "grad_norm": 59.55110168457031,
      "learning_rate": 3.4715390021082226e-05,
      "loss": -11.1265,
      "step": 9300
    },
    {
      "epoch": 6.542515811665496,
      "grad_norm": 55.979408264160156,
      "learning_rate": 3.4645115952213635e-05,
      "loss": -10.9562,
      "step": 9310
    },
    {
      "epoch": 6.549543218552354,
      "grad_norm": 46.40483856201172,
      "learning_rate": 3.457484188334505e-05,
      "loss": -11.2325,
      "step": 9320
    },
    {
      "epoch": 6.556570625439213,
      "grad_norm": 122.97071838378906,
      "learning_rate": 3.4504567814476455e-05,
      "loss": -11.0164,
      "step": 9330
    },
    {
      "epoch": 6.563598032326071,
      "grad_norm": 51.55015563964844,
      "learning_rate": 3.443429374560787e-05,
      "loss": -10.5012,
      "step": 9340
    },
    {
      "epoch": 6.570625439212931,
      "grad_norm": 55.030399322509766,
      "learning_rate": 3.436401967673928e-05,
      "loss": -11.1312,
      "step": 9350
    },
    {
      "epoch": 6.577652846099789,
      "grad_norm": 46.06470489501953,
      "learning_rate": 3.42937456078707e-05,
      "loss": -10.426,
      "step": 9360
    },
    {
      "epoch": 6.584680252986648,
      "grad_norm": 110.46534729003906,
      "learning_rate": 3.4223471539002114e-05,
      "loss": -11.065,
      "step": 9370
    },
    {
      "epoch": 6.5917076598735065,
      "grad_norm": 81.48638916015625,
      "learning_rate": 3.4153197470133523e-05,
      "loss": -11.0866,
      "step": 9380
    },
    {
      "epoch": 6.598735066760366,
      "grad_norm": 54.0628547668457,
      "learning_rate": 3.408292340126493e-05,
      "loss": -11.1646,
      "step": 9390
    },
    {
      "epoch": 6.605762473647224,
      "grad_norm": 49.22671890258789,
      "learning_rate": 3.401264933239634e-05,
      "loss": -11.1445,
      "step": 9400
    },
    {
      "epoch": 6.612789880534083,
      "grad_norm": 49.79894256591797,
      "learning_rate": 3.394237526352776e-05,
      "loss": -11.1143,
      "step": 9410
    },
    {
      "epoch": 6.6198172874209416,
      "grad_norm": 23.255573272705078,
      "learning_rate": 3.3872101194659176e-05,
      "loss": -11.2609,
      "step": 9420
    },
    {
      "epoch": 6.626844694307801,
      "grad_norm": 91.32830810546875,
      "learning_rate": 3.3801827125790586e-05,
      "loss": -11.2853,
      "step": 9430
    },
    {
      "epoch": 6.633872101194659,
      "grad_norm": 56.585575103759766,
      "learning_rate": 3.3731553056922e-05,
      "loss": -11.3403,
      "step": 9440
    },
    {
      "epoch": 6.640899508081518,
      "grad_norm": 86.06101989746094,
      "learning_rate": 3.3661278988053405e-05,
      "loss": -11.1118,
      "step": 9450
    },
    {
      "epoch": 6.647926914968377,
      "grad_norm": 71.0372543334961,
      "learning_rate": 3.359100491918482e-05,
      "loss": -10.9247,
      "step": 9460
    },
    {
      "epoch": 6.654954321855236,
      "grad_norm": 24.784563064575195,
      "learning_rate": 3.352073085031623e-05,
      "loss": -11.07,
      "step": 9470
    },
    {
      "epoch": 6.661981728742094,
      "grad_norm": 64.4252700805664,
      "learning_rate": 3.345045678144765e-05,
      "loss": -11.1269,
      "step": 9480
    },
    {
      "epoch": 6.6690091356289525,
      "grad_norm": 37.9817008972168,
      "learning_rate": 3.3380182712579064e-05,
      "loss": -11.3265,
      "step": 9490
    },
    {
      "epoch": 6.676036542515812,
      "grad_norm": 34.55886459350586,
      "learning_rate": 3.3309908643710474e-05,
      "loss": -11.3667,
      "step": 9500
    },
    {
      "epoch": 6.683063949402671,
      "grad_norm": 153.5134735107422,
      "learning_rate": 3.3239634574841883e-05,
      "loss": -11.3187,
      "step": 9510
    },
    {
      "epoch": 6.690091356289529,
      "grad_norm": 42.41312789916992,
      "learning_rate": 3.316936050597329e-05,
      "loss": -11.0639,
      "step": 9520
    },
    {
      "epoch": 6.6971187631763875,
      "grad_norm": 81.09271240234375,
      "learning_rate": 3.309908643710471e-05,
      "loss": -10.9326,
      "step": 9530
    },
    {
      "epoch": 6.704146170063247,
      "grad_norm": 34.51554489135742,
      "learning_rate": 3.3028812368236126e-05,
      "loss": -11.4098,
      "step": 9540
    },
    {
      "epoch": 6.711173576950105,
      "grad_norm": 98.88934326171875,
      "learning_rate": 3.2958538299367536e-05,
      "loss": -10.8646,
      "step": 9550
    },
    {
      "epoch": 6.718200983836964,
      "grad_norm": 54.075599670410156,
      "learning_rate": 3.288826423049895e-05,
      "loss": -11.3771,
      "step": 9560
    },
    {
      "epoch": 6.725228390723823,
      "grad_norm": 71.7997055053711,
      "learning_rate": 3.2817990161630355e-05,
      "loss": -11.3227,
      "step": 9570
    },
    {
      "epoch": 6.732255797610682,
      "grad_norm": 63.06501007080078,
      "learning_rate": 3.274771609276177e-05,
      "loss": -11.2157,
      "step": 9580
    },
    {
      "epoch": 6.73928320449754,
      "grad_norm": 28.46270751953125,
      "learning_rate": 3.267744202389318e-05,
      "loss": -11.2117,
      "step": 9590
    },
    {
      "epoch": 6.746310611384399,
      "grad_norm": 121.53681945800781,
      "learning_rate": 3.26071679550246e-05,
      "loss": -10.3003,
      "step": 9600
    },
    {
      "epoch": 6.753338018271258,
      "grad_norm": 63.74211883544922,
      "learning_rate": 3.2536893886156014e-05,
      "loss": -11.2913,
      "step": 9610
    },
    {
      "epoch": 6.760365425158117,
      "grad_norm": 77.42420959472656,
      "learning_rate": 3.2466619817287424e-05,
      "loss": -10.4367,
      "step": 9620
    },
    {
      "epoch": 6.767392832044975,
      "grad_norm": 70.64044952392578,
      "learning_rate": 3.2396345748418834e-05,
      "loss": -11.3068,
      "step": 9630
    },
    {
      "epoch": 6.774420238931834,
      "grad_norm": 76.35689544677734,
      "learning_rate": 3.2326071679550243e-05,
      "loss": -11.4255,
      "step": 9640
    },
    {
      "epoch": 6.781447645818693,
      "grad_norm": 50.66984176635742,
      "learning_rate": 3.225579761068166e-05,
      "loss": -11.2986,
      "step": 9650
    },
    {
      "epoch": 6.788475052705552,
      "grad_norm": 45.0932731628418,
      "learning_rate": 3.218552354181307e-05,
      "loss": -11.3622,
      "step": 9660
    },
    {
      "epoch": 6.79550245959241,
      "grad_norm": 37.63602828979492,
      "learning_rate": 3.2115249472944486e-05,
      "loss": -11.4502,
      "step": 9670
    },
    {
      "epoch": 6.8025298664792695,
      "grad_norm": 74.29603576660156,
      "learning_rate": 3.20449754040759e-05,
      "loss": -11.2266,
      "step": 9680
    },
    {
      "epoch": 6.809557273366128,
      "grad_norm": 113.43187713623047,
      "learning_rate": 3.1974701335207306e-05,
      "loss": -11.2251,
      "step": 9690
    },
    {
      "epoch": 6.816584680252987,
      "grad_norm": 93.78741455078125,
      "learning_rate": 3.190442726633872e-05,
      "loss": -11.1735,
      "step": 9700
    },
    {
      "epoch": 6.823612087139845,
      "grad_norm": 83.93143463134766,
      "learning_rate": 3.183415319747013e-05,
      "loss": -11.1123,
      "step": 9710
    },
    {
      "epoch": 6.830639494026704,
      "grad_norm": 133.51669311523438,
      "learning_rate": 3.176387912860155e-05,
      "loss": -11.4289,
      "step": 9720
    },
    {
      "epoch": 6.837666900913563,
      "grad_norm": 55.35807800292969,
      "learning_rate": 3.1693605059732965e-05,
      "loss": -11.2142,
      "step": 9730
    },
    {
      "epoch": 6.844694307800422,
      "grad_norm": 37.26679992675781,
      "learning_rate": 3.1623330990864374e-05,
      "loss": -11.4588,
      "step": 9740
    },
    {
      "epoch": 6.85172171468728,
      "grad_norm": 33.513153076171875,
      "learning_rate": 3.1553056921995784e-05,
      "loss": -11.2271,
      "step": 9750
    },
    {
      "epoch": 6.858749121574139,
      "grad_norm": 78.39920043945312,
      "learning_rate": 3.1482782853127194e-05,
      "loss": -11.4126,
      "step": 9760
    },
    {
      "epoch": 6.865776528460998,
      "grad_norm": 85.61383056640625,
      "learning_rate": 3.141250878425861e-05,
      "loss": -11.0877,
      "step": 9770
    },
    {
      "epoch": 6.872803935347856,
      "grad_norm": 118.7698974609375,
      "learning_rate": 3.134223471539002e-05,
      "loss": -11.246,
      "step": 9780
    },
    {
      "epoch": 6.8798313422347155,
      "grad_norm": 52.61151123046875,
      "learning_rate": 3.1271960646521436e-05,
      "loss": -11.2024,
      "step": 9790
    },
    {
      "epoch": 6.886858749121574,
      "grad_norm": 95.14008331298828,
      "learning_rate": 3.120168657765285e-05,
      "loss": -11.439,
      "step": 9800
    },
    {
      "epoch": 6.893886156008433,
      "grad_norm": 130.8831787109375,
      "learning_rate": 3.1131412508784256e-05,
      "loss": -10.5296,
      "step": 9810
    },
    {
      "epoch": 6.900913562895291,
      "grad_norm": 105.9815902709961,
      "learning_rate": 3.106113843991567e-05,
      "loss": -11.3977,
      "step": 9820
    },
    {
      "epoch": 6.9079409697821506,
      "grad_norm": 79.29308319091797,
      "learning_rate": 3.099086437104708e-05,
      "loss": -11.0689,
      "step": 9830
    },
    {
      "epoch": 6.914968376669009,
      "grad_norm": 120.49591827392578,
      "learning_rate": 3.09205903021785e-05,
      "loss": -11.4525,
      "step": 9840
    },
    {
      "epoch": 6.921995783555868,
      "grad_norm": 94.08132934570312,
      "learning_rate": 3.0850316233309915e-05,
      "loss": -11.3399,
      "step": 9850
    },
    {
      "epoch": 6.929023190442726,
      "grad_norm": 97.76359558105469,
      "learning_rate": 3.0780042164441325e-05,
      "loss": -11.2637,
      "step": 9860
    },
    {
      "epoch": 6.936050597329586,
      "grad_norm": 115.27263641357422,
      "learning_rate": 3.0709768095572734e-05,
      "loss": -11.2037,
      "step": 9870
    },
    {
      "epoch": 6.943078004216444,
      "grad_norm": 52.04743957519531,
      "learning_rate": 3.0639494026704144e-05,
      "loss": -10.5302,
      "step": 9880
    },
    {
      "epoch": 6.950105411103303,
      "grad_norm": 19.8883113861084,
      "learning_rate": 3.056921995783556e-05,
      "loss": -11.3561,
      "step": 9890
    },
    {
      "epoch": 6.9571328179901615,
      "grad_norm": 41.67923355102539,
      "learning_rate": 3.0498945888966974e-05,
      "loss": -11.3564,
      "step": 9900
    },
    {
      "epoch": 6.964160224877021,
      "grad_norm": 85.43221282958984,
      "learning_rate": 3.0428671820098387e-05,
      "loss": -11.3481,
      "step": 9910
    },
    {
      "epoch": 6.971187631763879,
      "grad_norm": 89.55455780029297,
      "learning_rate": 3.03583977512298e-05,
      "loss": -11.2376,
      "step": 9920
    },
    {
      "epoch": 6.978215038650738,
      "grad_norm": 289.018798828125,
      "learning_rate": 3.028812368236121e-05,
      "loss": -10.5609,
      "step": 9930
    },
    {
      "epoch": 6.9852424455375965,
      "grad_norm": 73.447265625,
      "learning_rate": 3.0217849613492623e-05,
      "loss": -11.2674,
      "step": 9940
    },
    {
      "epoch": 6.992269852424456,
      "grad_norm": 90.29679870605469,
      "learning_rate": 3.0147575544624036e-05,
      "loss": -11.4804,
      "step": 9950
    },
    {
      "epoch": 6.999297259311314,
      "grad_norm": 122.89019775390625,
      "learning_rate": 3.007730147575545e-05,
      "loss": -11.1382,
      "step": 9960
    },
    {
      "epoch": 7.0,
      "eval_runtime": 10.4506,
      "eval_samples_per_second": 65585.016,
      "eval_steps_per_second": 16.076,
      "step": 9961
    },
    {
      "epoch": 7.006324666198173,
      "grad_norm": 113.26683807373047,
      "learning_rate": 3.0007027406886862e-05,
      "loss": -11.375,
      "step": 9970
    },
    {
      "epoch": 7.013352073085032,
      "grad_norm": 46.36833572387695,
      "learning_rate": 2.9936753338018275e-05,
      "loss": -11.4362,
      "step": 9980
    },
    {
      "epoch": 7.02037947997189,
      "grad_norm": 44.97401809692383,
      "learning_rate": 2.9866479269149685e-05,
      "loss": -11.3011,
      "step": 9990
    },
    {
      "epoch": 7.027406886858749,
      "grad_norm": 39.6006965637207,
      "learning_rate": 2.9796205200281098e-05,
      "loss": -11.4389,
      "step": 10000
    },
    {
      "epoch": 7.0344342937456075,
      "grad_norm": 68.59600067138672,
      "learning_rate": 2.972593113141251e-05,
      "loss": -11.239,
      "step": 10010
    },
    {
      "epoch": 7.041461700632467,
      "grad_norm": 85.31729125976562,
      "learning_rate": 2.9655657062543924e-05,
      "loss": -11.3048,
      "step": 10020
    },
    {
      "epoch": 7.048489107519325,
      "grad_norm": 45.945648193359375,
      "learning_rate": 2.9585382993675337e-05,
      "loss": -11.5284,
      "step": 10030
    },
    {
      "epoch": 7.055516514406184,
      "grad_norm": 61.179527282714844,
      "learning_rate": 2.951510892480675e-05,
      "loss": -11.2586,
      "step": 10040
    },
    {
      "epoch": 7.0625439212930425,
      "grad_norm": 46.28675079345703,
      "learning_rate": 2.944483485593816e-05,
      "loss": -11.5574,
      "step": 10050
    },
    {
      "epoch": 7.069571328179902,
      "grad_norm": 82.78350067138672,
      "learning_rate": 2.9374560787069573e-05,
      "loss": -11.5304,
      "step": 10060
    },
    {
      "epoch": 7.07659873506676,
      "grad_norm": 81.68531799316406,
      "learning_rate": 2.9304286718200986e-05,
      "loss": -11.4481,
      "step": 10070
    },
    {
      "epoch": 7.083626141953619,
      "grad_norm": 35.9561882019043,
      "learning_rate": 2.92340126493324e-05,
      "loss": -11.451,
      "step": 10080
    },
    {
      "epoch": 7.090653548840478,
      "grad_norm": 35.683006286621094,
      "learning_rate": 2.9163738580463812e-05,
      "loss": -11.4163,
      "step": 10090
    },
    {
      "epoch": 7.097680955727337,
      "grad_norm": 41.08517074584961,
      "learning_rate": 2.9093464511595225e-05,
      "loss": -10.3945,
      "step": 10100
    },
    {
      "epoch": 7.104708362614195,
      "grad_norm": 38.16370391845703,
      "learning_rate": 2.9023190442726635e-05,
      "loss": -11.4073,
      "step": 10110
    },
    {
      "epoch": 7.111735769501054,
      "grad_norm": 72.74711608886719,
      "learning_rate": 2.8952916373858048e-05,
      "loss": -10.445,
      "step": 10120
    },
    {
      "epoch": 7.118763176387913,
      "grad_norm": 24.173934936523438,
      "learning_rate": 2.888264230498946e-05,
      "loss": -11.5726,
      "step": 10130
    },
    {
      "epoch": 7.125790583274772,
      "grad_norm": 65.33992004394531,
      "learning_rate": 2.8812368236120874e-05,
      "loss": -11.536,
      "step": 10140
    },
    {
      "epoch": 7.13281799016163,
      "grad_norm": 32.74353790283203,
      "learning_rate": 2.8742094167252287e-05,
      "loss": -10.7763,
      "step": 10150
    },
    {
      "epoch": 7.139845397048489,
      "grad_norm": 34.51007843017578,
      "learning_rate": 2.86718200983837e-05,
      "loss": -11.5515,
      "step": 10160
    },
    {
      "epoch": 7.146872803935348,
      "grad_norm": 70.92231750488281,
      "learning_rate": 2.8601546029515107e-05,
      "loss": -11.1868,
      "step": 10170
    },
    {
      "epoch": 7.153900210822207,
      "grad_norm": 146.598876953125,
      "learning_rate": 2.8531271960646523e-05,
      "loss": -11.2594,
      "step": 10180
    },
    {
      "epoch": 7.160927617709065,
      "grad_norm": 65.46358489990234,
      "learning_rate": 2.8460997891777936e-05,
      "loss": -11.1547,
      "step": 10190
    },
    {
      "epoch": 7.1679550245959245,
      "grad_norm": 40.6716423034668,
      "learning_rate": 2.839072382290935e-05,
      "loss": -11.4743,
      "step": 10200
    },
    {
      "epoch": 7.174982431482783,
      "grad_norm": 164.24241638183594,
      "learning_rate": 2.8320449754040762e-05,
      "loss": -11.4447,
      "step": 10210
    },
    {
      "epoch": 7.182009838369642,
      "grad_norm": 76.10997009277344,
      "learning_rate": 2.8250175685172175e-05,
      "loss": -11.3046,
      "step": 10220
    },
    {
      "epoch": 7.1890372452565,
      "grad_norm": 109.41170501708984,
      "learning_rate": 2.8179901616303582e-05,
      "loss": -11.1504,
      "step": 10230
    },
    {
      "epoch": 7.1960646521433596,
      "grad_norm": 55.75437927246094,
      "learning_rate": 2.8109627547434998e-05,
      "loss": -11.3585,
      "step": 10240
    },
    {
      "epoch": 7.203092059030218,
      "grad_norm": 58.7758903503418,
      "learning_rate": 2.803935347856641e-05,
      "loss": -11.5943,
      "step": 10250
    },
    {
      "epoch": 7.210119465917076,
      "grad_norm": 57.18503952026367,
      "learning_rate": 2.7969079409697824e-05,
      "loss": -11.3649,
      "step": 10260
    },
    {
      "epoch": 7.217146872803935,
      "grad_norm": 41.973365783691406,
      "learning_rate": 2.7898805340829238e-05,
      "loss": -11.2436,
      "step": 10270
    },
    {
      "epoch": 7.224174279690794,
      "grad_norm": 38.42483901977539,
      "learning_rate": 2.782853127196065e-05,
      "loss": -11.3024,
      "step": 10280
    },
    {
      "epoch": 7.231201686577653,
      "grad_norm": 114.70344543457031,
      "learning_rate": 2.7758257203092057e-05,
      "loss": -11.3033,
      "step": 10290
    },
    {
      "epoch": 7.238229093464511,
      "grad_norm": 44.47332000732422,
      "learning_rate": 2.7687983134223473e-05,
      "loss": -11.4544,
      "step": 10300
    },
    {
      "epoch": 7.2452565003513705,
      "grad_norm": 53.7940559387207,
      "learning_rate": 2.7617709065354887e-05,
      "loss": -11.4803,
      "step": 10310
    },
    {
      "epoch": 7.252283907238229,
      "grad_norm": 24.903179168701172,
      "learning_rate": 2.75474349964863e-05,
      "loss": -11.5822,
      "step": 10320
    },
    {
      "epoch": 7.259311314125088,
      "grad_norm": 38.670677185058594,
      "learning_rate": 2.7477160927617713e-05,
      "loss": -11.3393,
      "step": 10330
    },
    {
      "epoch": 7.266338721011946,
      "grad_norm": 45.17827224731445,
      "learning_rate": 2.7406886858749126e-05,
      "loss": -11.5177,
      "step": 10340
    },
    {
      "epoch": 7.2733661278988055,
      "grad_norm": 47.89738082885742,
      "learning_rate": 2.7336612789880532e-05,
      "loss": -11.5486,
      "step": 10350
    },
    {
      "epoch": 7.280393534785664,
      "grad_norm": 59.07412338256836,
      "learning_rate": 2.726633872101195e-05,
      "loss": -11.546,
      "step": 10360
    },
    {
      "epoch": 7.287420941672523,
      "grad_norm": 72.98616790771484,
      "learning_rate": 2.719606465214336e-05,
      "loss": -11.54,
      "step": 10370
    },
    {
      "epoch": 7.294448348559381,
      "grad_norm": 69.57888793945312,
      "learning_rate": 2.7125790583274775e-05,
      "loss": -11.5525,
      "step": 10380
    },
    {
      "epoch": 7.301475755446241,
      "grad_norm": 125.59573364257812,
      "learning_rate": 2.7055516514406188e-05,
      "loss": -11.4037,
      "step": 10390
    },
    {
      "epoch": 7.308503162333099,
      "grad_norm": 34.031551361083984,
      "learning_rate": 2.6985242445537594e-05,
      "loss": -11.4051,
      "step": 10400
    },
    {
      "epoch": 7.315530569219958,
      "grad_norm": 55.61785888671875,
      "learning_rate": 2.6914968376669007e-05,
      "loss": -11.4841,
      "step": 10410
    },
    {
      "epoch": 7.3225579761068165,
      "grad_norm": 82.9910659790039,
      "learning_rate": 2.6844694307800424e-05,
      "loss": -11.482,
      "step": 10420
    },
    {
      "epoch": 7.329585382993676,
      "grad_norm": 78.72028350830078,
      "learning_rate": 2.6774420238931837e-05,
      "loss": -11.3344,
      "step": 10430
    },
    {
      "epoch": 7.336612789880534,
      "grad_norm": 95.18013000488281,
      "learning_rate": 2.670414617006325e-05,
      "loss": -11.3245,
      "step": 10440
    },
    {
      "epoch": 7.343640196767393,
      "grad_norm": 310.7828674316406,
      "learning_rate": 2.6633872101194663e-05,
      "loss": -10.717,
      "step": 10450
    },
    {
      "epoch": 7.3506676036542515,
      "grad_norm": 44.495059967041016,
      "learning_rate": 2.656359803232607e-05,
      "loss": -10.5811,
      "step": 10460
    },
    {
      "epoch": 7.357695010541111,
      "grad_norm": 38.03813552856445,
      "learning_rate": 2.6493323963457482e-05,
      "loss": -11.4104,
      "step": 10470
    },
    {
      "epoch": 7.364722417427969,
      "grad_norm": 95.9964828491211,
      "learning_rate": 2.6423049894588895e-05,
      "loss": -10.7605,
      "step": 10480
    },
    {
      "epoch": 7.371749824314827,
      "grad_norm": 27.62965965270996,
      "learning_rate": 2.6352775825720312e-05,
      "loss": -11.5122,
      "step": 10490
    },
    {
      "epoch": 7.378777231201687,
      "grad_norm": 40.179107666015625,
      "learning_rate": 2.6282501756851725e-05,
      "loss": -10.796,
      "step": 10500
    },
    {
      "epoch": 7.385804638088545,
      "grad_norm": 77.43260192871094,
      "learning_rate": 2.6212227687983138e-05,
      "loss": -11.1279,
      "step": 10510
    },
    {
      "epoch": 7.392832044975404,
      "grad_norm": 82.23867797851562,
      "learning_rate": 2.6141953619114544e-05,
      "loss": -11.3349,
      "step": 10520
    },
    {
      "epoch": 7.3998594518622625,
      "grad_norm": 47.88168716430664,
      "learning_rate": 2.6071679550245958e-05,
      "loss": -10.8401,
      "step": 10530
    },
    {
      "epoch": 7.406886858749122,
      "grad_norm": 46.043827056884766,
      "learning_rate": 2.600140548137737e-05,
      "loss": -11.3849,
      "step": 10540
    },
    {
      "epoch": 7.41391426563598,
      "grad_norm": 16.201757431030273,
      "learning_rate": 2.5931131412508787e-05,
      "loss": -11.5037,
      "step": 10550
    },
    {
      "epoch": 7.420941672522839,
      "grad_norm": 90.55365753173828,
      "learning_rate": 2.58608573436402e-05,
      "loss": -11.3423,
      "step": 10560
    },
    {
      "epoch": 7.4279690794096975,
      "grad_norm": 53.71520233154297,
      "learning_rate": 2.5790583274771613e-05,
      "loss": -11.5417,
      "step": 10570
    },
    {
      "epoch": 7.434996486296557,
      "grad_norm": 40.83891296386719,
      "learning_rate": 2.572030920590302e-05,
      "loss": -11.1953,
      "step": 10580
    },
    {
      "epoch": 7.442023893183415,
      "grad_norm": 43.017818450927734,
      "learning_rate": 2.5650035137034433e-05,
      "loss": -11.4272,
      "step": 10590
    },
    {
      "epoch": 7.449051300070274,
      "grad_norm": 59.97991943359375,
      "learning_rate": 2.5579761068165846e-05,
      "loss": -11.6094,
      "step": 10600
    },
    {
      "epoch": 7.456078706957133,
      "grad_norm": 43.61699676513672,
      "learning_rate": 2.5509486999297262e-05,
      "loss": -10.7381,
      "step": 10610
    },
    {
      "epoch": 7.463106113843992,
      "grad_norm": 105.94182586669922,
      "learning_rate": 2.5439212930428675e-05,
      "loss": -11.5076,
      "step": 10620
    },
    {
      "epoch": 7.47013352073085,
      "grad_norm": 146.6622772216797,
      "learning_rate": 2.536893886156009e-05,
      "loss": -11.4621,
      "step": 10630
    },
    {
      "epoch": 7.477160927617709,
      "grad_norm": 146.53445434570312,
      "learning_rate": 2.5298664792691495e-05,
      "loss": -11.3824,
      "step": 10640
    },
    {
      "epoch": 7.484188334504568,
      "grad_norm": 38.70935821533203,
      "learning_rate": 2.5228390723822908e-05,
      "loss": -11.6057,
      "step": 10650
    },
    {
      "epoch": 7.491215741391427,
      "grad_norm": 73.68689727783203,
      "learning_rate": 2.515811665495432e-05,
      "loss": -11.5352,
      "step": 10660
    },
    {
      "epoch": 7.498243148278285,
      "grad_norm": 88.20087432861328,
      "learning_rate": 2.5087842586085737e-05,
      "loss": -11.5933,
      "step": 10670
    },
    {
      "epoch": 7.505270555165144,
      "grad_norm": 69.21859741210938,
      "learning_rate": 2.501756851721715e-05,
      "loss": -11.3783,
      "step": 10680
    },
    {
      "epoch": 7.512297962052003,
      "grad_norm": 73.25106048583984,
      "learning_rate": 2.494729444834856e-05,
      "loss": -11.3794,
      "step": 10690
    },
    {
      "epoch": 7.519325368938862,
      "grad_norm": 107.70374298095703,
      "learning_rate": 2.4877020379479973e-05,
      "loss": -11.3965,
      "step": 10700
    },
    {
      "epoch": 7.52635277582572,
      "grad_norm": 58.942623138427734,
      "learning_rate": 2.4806746310611386e-05,
      "loss": -11.19,
      "step": 10710
    },
    {
      "epoch": 7.533380182712579,
      "grad_norm": 72.05843353271484,
      "learning_rate": 2.4736472241742796e-05,
      "loss": -11.3197,
      "step": 10720
    },
    {
      "epoch": 7.540407589599438,
      "grad_norm": 74.92314910888672,
      "learning_rate": 2.4666198172874213e-05,
      "loss": -11.3234,
      "step": 10730
    },
    {
      "epoch": 7.547434996486297,
      "grad_norm": 69.01043701171875,
      "learning_rate": 2.4595924104005626e-05,
      "loss": -11.6086,
      "step": 10740
    },
    {
      "epoch": 7.554462403373155,
      "grad_norm": 90.19316864013672,
      "learning_rate": 2.4525650035137035e-05,
      "loss": -11.1846,
      "step": 10750
    },
    {
      "epoch": 7.561489810260014,
      "grad_norm": 51.477210998535156,
      "learning_rate": 2.445537596626845e-05,
      "loss": -11.5602,
      "step": 10760
    },
    {
      "epoch": 7.568517217146873,
      "grad_norm": 51.68558120727539,
      "learning_rate": 2.438510189739986e-05,
      "loss": -11.5484,
      "step": 10770
    },
    {
      "epoch": 7.575544624033731,
      "grad_norm": 45.676605224609375,
      "learning_rate": 2.431482782853127e-05,
      "loss": -11.685,
      "step": 10780
    },
    {
      "epoch": 7.58257203092059,
      "grad_norm": 54.11227035522461,
      "learning_rate": 2.4244553759662684e-05,
      "loss": -11.3984,
      "step": 10790
    },
    {
      "epoch": 7.589599437807449,
      "grad_norm": 83.4341049194336,
      "learning_rate": 2.41742796907941e-05,
      "loss": -11.4748,
      "step": 10800
    },
    {
      "epoch": 7.596626844694308,
      "grad_norm": 36.496131896972656,
      "learning_rate": 2.410400562192551e-05,
      "loss": -11.4841,
      "step": 10810
    },
    {
      "epoch": 7.603654251581166,
      "grad_norm": 55.157752990722656,
      "learning_rate": 2.4033731553056924e-05,
      "loss": -11.5877,
      "step": 10820
    },
    {
      "epoch": 7.6106816584680255,
      "grad_norm": 43.443321228027344,
      "learning_rate": 2.3963457484188337e-05,
      "loss": -11.5014,
      "step": 10830
    },
    {
      "epoch": 7.617709065354884,
      "grad_norm": 48.97034454345703,
      "learning_rate": 2.3893183415319746e-05,
      "loss": -11.4067,
      "step": 10840
    },
    {
      "epoch": 7.624736472241743,
      "grad_norm": 74.53884887695312,
      "learning_rate": 2.382290934645116e-05,
      "loss": -11.3943,
      "step": 10850
    },
    {
      "epoch": 7.631763879128601,
      "grad_norm": 97.46280670166016,
      "learning_rate": 2.3752635277582576e-05,
      "loss": -11.4737,
      "step": 10860
    },
    {
      "epoch": 7.6387912860154605,
      "grad_norm": 85.08724212646484,
      "learning_rate": 2.3682361208713986e-05,
      "loss": -11.5707,
      "step": 10870
    },
    {
      "epoch": 7.645818692902319,
      "grad_norm": 40.328636169433594,
      "learning_rate": 2.36120871398454e-05,
      "loss": -11.4288,
      "step": 10880
    },
    {
      "epoch": 7.652846099789178,
      "grad_norm": 72.80975341796875,
      "learning_rate": 2.3541813070976812e-05,
      "loss": -10.6671,
      "step": 10890
    },
    {
      "epoch": 7.659873506676036,
      "grad_norm": 45.58534622192383,
      "learning_rate": 2.347153900210822e-05,
      "loss": -11.6391,
      "step": 10900
    },
    {
      "epoch": 7.666900913562896,
      "grad_norm": 56.98851013183594,
      "learning_rate": 2.3401264933239635e-05,
      "loss": -11.4241,
      "step": 10910
    },
    {
      "epoch": 7.673928320449754,
      "grad_norm": 75.11083984375,
      "learning_rate": 2.333099086437105e-05,
      "loss": -11.3454,
      "step": 10920
    },
    {
      "epoch": 7.680955727336613,
      "grad_norm": 118.17156982421875,
      "learning_rate": 2.326071679550246e-05,
      "loss": -11.5032,
      "step": 10930
    },
    {
      "epoch": 7.6879831342234715,
      "grad_norm": 54.313743591308594,
      "learning_rate": 2.3190442726633874e-05,
      "loss": -11.4923,
      "step": 10940
    },
    {
      "epoch": 7.695010541110331,
      "grad_norm": 56.4376335144043,
      "learning_rate": 2.3120168657765287e-05,
      "loss": -11.5223,
      "step": 10950
    },
    {
      "epoch": 7.702037947997189,
      "grad_norm": 53.7808837890625,
      "learning_rate": 2.3049894588896697e-05,
      "loss": -11.3726,
      "step": 10960
    },
    {
      "epoch": 7.709065354884048,
      "grad_norm": 42.979190826416016,
      "learning_rate": 2.297962052002811e-05,
      "loss": -11.5654,
      "step": 10970
    },
    {
      "epoch": 7.7160927617709065,
      "grad_norm": 212.1936492919922,
      "learning_rate": 2.2909346451159526e-05,
      "loss": -11.2016,
      "step": 10980
    },
    {
      "epoch": 7.723120168657765,
      "grad_norm": 96.83491516113281,
      "learning_rate": 2.2839072382290936e-05,
      "loss": -11.1947,
      "step": 10990
    },
    {
      "epoch": 7.730147575544624,
      "grad_norm": 63.25535583496094,
      "learning_rate": 2.2775825720309207e-05,
      "loss": -9.9425,
      "step": 11000
    },
    {
      "epoch": 7.737174982431483,
      "grad_norm": 29.1693115234375,
      "learning_rate": 2.270555165144062e-05,
      "loss": -10.1137,
      "step": 11010
    },
    {
      "epoch": 7.744202389318342,
      "grad_norm": 39.477577209472656,
      "learning_rate": 2.2635277582572033e-05,
      "loss": -11.7029,
      "step": 11020
    },
    {
      "epoch": 7.7512297962052,
      "grad_norm": 23.100290298461914,
      "learning_rate": 2.2565003513703446e-05,
      "loss": -11.6065,
      "step": 11030
    },
    {
      "epoch": 7.758257203092059,
      "grad_norm": 74.83458709716797,
      "learning_rate": 2.249472944483486e-05,
      "loss": -11.6239,
      "step": 11040
    },
    {
      "epoch": 7.7652846099789175,
      "grad_norm": 104.84127044677734,
      "learning_rate": 2.242445537596627e-05,
      "loss": -11.2968,
      "step": 11050
    },
    {
      "epoch": 7.772312016865777,
      "grad_norm": 51.23561096191406,
      "learning_rate": 2.2354181307097682e-05,
      "loss": -11.5433,
      "step": 11060
    },
    {
      "epoch": 7.779339423752635,
      "grad_norm": 38.90870666503906,
      "learning_rate": 2.2283907238229095e-05,
      "loss": -11.3419,
      "step": 11070
    },
    {
      "epoch": 7.786366830639494,
      "grad_norm": 31.81033706665039,
      "learning_rate": 2.2213633169360508e-05,
      "loss": -11.7772,
      "step": 11080
    },
    {
      "epoch": 7.7933942375263525,
      "grad_norm": 32.48053741455078,
      "learning_rate": 2.214335910049192e-05,
      "loss": -11.755,
      "step": 11090
    },
    {
      "epoch": 7.800421644413212,
      "grad_norm": 105.14104461669922,
      "learning_rate": 2.2073085031623334e-05,
      "loss": -11.6993,
      "step": 11100
    },
    {
      "epoch": 7.80744905130007,
      "grad_norm": 29.790393829345703,
      "learning_rate": 2.2002810962754744e-05,
      "loss": -11.2999,
      "step": 11110
    },
    {
      "epoch": 7.814476458186929,
      "grad_norm": 85.64761352539062,
      "learning_rate": 2.1932536893886157e-05,
      "loss": -11.6316,
      "step": 11120
    },
    {
      "epoch": 7.821503865073788,
      "grad_norm": 28.850156784057617,
      "learning_rate": 2.186226282501757e-05,
      "loss": -11.7731,
      "step": 11130
    },
    {
      "epoch": 7.828531271960647,
      "grad_norm": 65.84664916992188,
      "learning_rate": 2.1791988756148983e-05,
      "loss": -11.6972,
      "step": 11140
    },
    {
      "epoch": 7.835558678847505,
      "grad_norm": 72.07759857177734,
      "learning_rate": 2.1721714687280396e-05,
      "loss": -11.5405,
      "step": 11150
    },
    {
      "epoch": 7.842586085734364,
      "grad_norm": 72.05180358886719,
      "learning_rate": 2.1651440618411806e-05,
      "loss": -11.4062,
      "step": 11160
    },
    {
      "epoch": 7.849613492621223,
      "grad_norm": 54.43324661254883,
      "learning_rate": 2.158116654954322e-05,
      "loss": -11.7905,
      "step": 11170
    },
    {
      "epoch": 7.856640899508082,
      "grad_norm": 31.057241439819336,
      "learning_rate": 2.1510892480674632e-05,
      "loss": -11.8045,
      "step": 11180
    },
    {
      "epoch": 7.86366830639494,
      "grad_norm": 47.54100799560547,
      "learning_rate": 2.1440618411806042e-05,
      "loss": -11.7987,
      "step": 11190
    },
    {
      "epoch": 7.870695713281799,
      "grad_norm": 53.89853286743164,
      "learning_rate": 2.1370344342937458e-05,
      "loss": -11.3831,
      "step": 11200
    },
    {
      "epoch": 7.877723120168658,
      "grad_norm": 38.47034454345703,
      "learning_rate": 2.130007027406887e-05,
      "loss": -10.9076,
      "step": 11210
    },
    {
      "epoch": 7.884750527055516,
      "grad_norm": 57.88434600830078,
      "learning_rate": 2.122979620520028e-05,
      "loss": -11.8138,
      "step": 11220
    },
    {
      "epoch": 7.891777933942375,
      "grad_norm": 29.251314163208008,
      "learning_rate": 2.1159522136331694e-05,
      "loss": -11.6089,
      "step": 11230
    },
    {
      "epoch": 7.8988053408292345,
      "grad_norm": 50.397647857666016,
      "learning_rate": 2.1089248067463107e-05,
      "loss": -10.9706,
      "step": 11240
    },
    {
      "epoch": 7.905832747716093,
      "grad_norm": 57.33661651611328,
      "learning_rate": 2.1018973998594517e-05,
      "loss": -11.6099,
      "step": 11250
    },
    {
      "epoch": 7.912860154602951,
      "grad_norm": 41.70273208618164,
      "learning_rate": 2.0948699929725933e-05,
      "loss": -11.7978,
      "step": 11260
    },
    {
      "epoch": 7.91988756148981,
      "grad_norm": 69.8932113647461,
      "learning_rate": 2.0878425860857346e-05,
      "loss": -11.5744,
      "step": 11270
    },
    {
      "epoch": 7.926914968376669,
      "grad_norm": 44.456695556640625,
      "learning_rate": 2.0808151791988756e-05,
      "loss": -11.6595,
      "step": 11280
    },
    {
      "epoch": 7.933942375263528,
      "grad_norm": 109.9281234741211,
      "learning_rate": 2.073787772312017e-05,
      "loss": -11.5179,
      "step": 11290
    },
    {
      "epoch": 7.940969782150386,
      "grad_norm": 36.33203887939453,
      "learning_rate": 2.0667603654251582e-05,
      "loss": -11.6709,
      "step": 11300
    },
    {
      "epoch": 7.947997189037245,
      "grad_norm": 36.29737091064453,
      "learning_rate": 2.0597329585382992e-05,
      "loss": -11.3592,
      "step": 11310
    },
    {
      "epoch": 7.955024595924104,
      "grad_norm": 35.78778076171875,
      "learning_rate": 2.0527055516514405e-05,
      "loss": -11.5855,
      "step": 11320
    },
    {
      "epoch": 7.962052002810963,
      "grad_norm": 42.4402961730957,
      "learning_rate": 2.045678144764582e-05,
      "loss": -11.6729,
      "step": 11330
    },
    {
      "epoch": 7.969079409697821,
      "grad_norm": 31.614093780517578,
      "learning_rate": 2.038650737877723e-05,
      "loss": -11.6585,
      "step": 11340
    },
    {
      "epoch": 7.9761068165846805,
      "grad_norm": 118.8606948852539,
      "learning_rate": 2.0316233309908644e-05,
      "loss": -11.7375,
      "step": 11350
    },
    {
      "epoch": 7.983134223471539,
      "grad_norm": 51.671241760253906,
      "learning_rate": 2.0245959241040058e-05,
      "loss": -11.5186,
      "step": 11360
    },
    {
      "epoch": 7.990161630358398,
      "grad_norm": 50.18342971801758,
      "learning_rate": 2.0175685172171467e-05,
      "loss": -10.8202,
      "step": 11370
    },
    {
      "epoch": 7.997189037245256,
      "grad_norm": 30.616607666015625,
      "learning_rate": 2.010541110330288e-05,
      "loss": -11.649,
      "step": 11380
    },
    {
      "epoch": 8.0,
      "eval_runtime": 10.3917,
      "eval_samples_per_second": 65957.03,
      "eval_steps_per_second": 16.167,
      "step": 11384
    },
    {
      "epoch": 8.004216444132116,
      "grad_norm": 53.208736419677734,
      "learning_rate": 2.0035137034434297e-05,
      "loss": -11.6743,
      "step": 11390
    },
    {
      "epoch": 8.011243851018975,
      "grad_norm": 55.128639221191406,
      "learning_rate": 1.9964862965565706e-05,
      "loss": -11.7391,
      "step": 11400
    },
    {
      "epoch": 8.018271257905832,
      "grad_norm": 35.97663879394531,
      "learning_rate": 1.989458889669712e-05,
      "loss": -11.7582,
      "step": 11410
    },
    {
      "epoch": 8.025298664792691,
      "grad_norm": 84.22913360595703,
      "learning_rate": 1.9824314827828533e-05,
      "loss": -11.5746,
      "step": 11420
    },
    {
      "epoch": 8.03232607167955,
      "grad_norm": 39.03883361816406,
      "learning_rate": 1.9754040758959942e-05,
      "loss": -11.7311,
      "step": 11430
    },
    {
      "epoch": 8.03935347856641,
      "grad_norm": 104.95687103271484,
      "learning_rate": 1.9683766690091355e-05,
      "loss": -10.6214,
      "step": 11440
    },
    {
      "epoch": 8.046380885453267,
      "grad_norm": 47.208309173583984,
      "learning_rate": 1.9613492621222772e-05,
      "loss": -11.5749,
      "step": 11450
    },
    {
      "epoch": 8.053408292340126,
      "grad_norm": 82.44551086425781,
      "learning_rate": 1.954321855235418e-05,
      "loss": -11.8184,
      "step": 11460
    },
    {
      "epoch": 8.060435699226986,
      "grad_norm": 46.75033187866211,
      "learning_rate": 1.9472944483485595e-05,
      "loss": -11.4063,
      "step": 11470
    },
    {
      "epoch": 8.067463106113845,
      "grad_norm": 46.87919616699219,
      "learning_rate": 1.9402670414617008e-05,
      "loss": -11.6864,
      "step": 11480
    },
    {
      "epoch": 8.074490513000702,
      "grad_norm": 72.1878890991211,
      "learning_rate": 1.9332396345748418e-05,
      "loss": -11.3487,
      "step": 11490
    },
    {
      "epoch": 8.081517919887562,
      "grad_norm": 76.79497528076172,
      "learning_rate": 1.926212227687983e-05,
      "loss": -10.7033,
      "step": 11500
    },
    {
      "epoch": 8.08854532677442,
      "grad_norm": 70.44031524658203,
      "learning_rate": 1.9191848208011247e-05,
      "loss": -11.8482,
      "step": 11510
    },
    {
      "epoch": 8.095572733661278,
      "grad_norm": 84.2323226928711,
      "learning_rate": 1.9121574139142657e-05,
      "loss": -11.8186,
      "step": 11520
    },
    {
      "epoch": 8.102600140548137,
      "grad_norm": 53.61427307128906,
      "learning_rate": 1.905130007027407e-05,
      "loss": -11.3978,
      "step": 11530
    },
    {
      "epoch": 8.109627547434997,
      "grad_norm": 88.82294464111328,
      "learning_rate": 1.8981026001405483e-05,
      "loss": -11.5266,
      "step": 11540
    },
    {
      "epoch": 8.116654954321856,
      "grad_norm": 33.37442398071289,
      "learning_rate": 1.8910751932536893e-05,
      "loss": -11.7663,
      "step": 11550
    },
    {
      "epoch": 8.123682361208713,
      "grad_norm": 89.56771850585938,
      "learning_rate": 1.8840477863668306e-05,
      "loss": -11.59,
      "step": 11560
    },
    {
      "epoch": 8.130709768095572,
      "grad_norm": 21.70923614501953,
      "learning_rate": 1.8770203794799722e-05,
      "loss": -11.8564,
      "step": 11570
    },
    {
      "epoch": 8.137737174982432,
      "grad_norm": 99.63512420654297,
      "learning_rate": 1.8699929725931132e-05,
      "loss": -11.6858,
      "step": 11580
    },
    {
      "epoch": 8.14476458186929,
      "grad_norm": 42.353309631347656,
      "learning_rate": 1.8629655657062545e-05,
      "loss": -11.3911,
      "step": 11590
    },
    {
      "epoch": 8.151791988756148,
      "grad_norm": 23.64620018005371,
      "learning_rate": 1.8559381588193958e-05,
      "loss": -11.7673,
      "step": 11600
    },
    {
      "epoch": 8.158819395643008,
      "grad_norm": 107.63330078125,
      "learning_rate": 1.8489107519325368e-05,
      "loss": -11.6085,
      "step": 11610
    },
    {
      "epoch": 8.165846802529867,
      "grad_norm": 46.23460006713867,
      "learning_rate": 1.841883345045678e-05,
      "loss": -11.6867,
      "step": 11620
    },
    {
      "epoch": 8.172874209416726,
      "grad_norm": 81.13777923583984,
      "learning_rate": 1.8348559381588194e-05,
      "loss": -11.4731,
      "step": 11630
    },
    {
      "epoch": 8.179901616303583,
      "grad_norm": 75.83899688720703,
      "learning_rate": 1.8278285312719607e-05,
      "loss": -11.5997,
      "step": 11640
    },
    {
      "epoch": 8.186929023190443,
      "grad_norm": 27.844768524169922,
      "learning_rate": 1.820801124385102e-05,
      "loss": -11.7051,
      "step": 11650
    },
    {
      "epoch": 8.193956430077302,
      "grad_norm": 39.0735969543457,
      "learning_rate": 1.8137737174982433e-05,
      "loss": -11.5449,
      "step": 11660
    },
    {
      "epoch": 8.200983836964161,
      "grad_norm": 26.887971878051758,
      "learning_rate": 1.8067463106113843e-05,
      "loss": -10.9604,
      "step": 11670
    },
    {
      "epoch": 8.208011243851018,
      "grad_norm": 66.45561218261719,
      "learning_rate": 1.7997189037245256e-05,
      "loss": -11.5269,
      "step": 11680
    },
    {
      "epoch": 8.215038650737878,
      "grad_norm": 170.49818420410156,
      "learning_rate": 1.792691496837667e-05,
      "loss": -11.4851,
      "step": 11690
    },
    {
      "epoch": 8.222066057624737,
      "grad_norm": 33.852054595947266,
      "learning_rate": 1.7856640899508082e-05,
      "loss": -11.7166,
      "step": 11700
    },
    {
      "epoch": 8.229093464511596,
      "grad_norm": 37.34960174560547,
      "learning_rate": 1.7786366830639495e-05,
      "loss": -11.7976,
      "step": 11710
    },
    {
      "epoch": 8.236120871398454,
      "grad_norm": 35.47004318237305,
      "learning_rate": 1.771609276177091e-05,
      "loss": -11.5669,
      "step": 11720
    },
    {
      "epoch": 8.243148278285313,
      "grad_norm": 54.7630500793457,
      "learning_rate": 1.7645818692902318e-05,
      "loss": -11.7949,
      "step": 11730
    },
    {
      "epoch": 8.250175685172172,
      "grad_norm": 28.543609619140625,
      "learning_rate": 1.757554462403373e-05,
      "loss": -11.8053,
      "step": 11740
    },
    {
      "epoch": 8.25720309205903,
      "grad_norm": 26.512168884277344,
      "learning_rate": 1.7505270555165144e-05,
      "loss": -11.4045,
      "step": 11750
    },
    {
      "epoch": 8.264230498945889,
      "grad_norm": 37.74763107299805,
      "learning_rate": 1.7434996486296557e-05,
      "loss": -11.8653,
      "step": 11760
    },
    {
      "epoch": 8.271257905832748,
      "grad_norm": 55.80563735961914,
      "learning_rate": 1.736472241742797e-05,
      "loss": -11.5531,
      "step": 11770
    },
    {
      "epoch": 8.278285312719607,
      "grad_norm": 22.805713653564453,
      "learning_rate": 1.7294448348559384e-05,
      "loss": -11.7168,
      "step": 11780
    },
    {
      "epoch": 8.285312719606464,
      "grad_norm": 12.696261405944824,
      "learning_rate": 1.7224174279690793e-05,
      "loss": -10.8123,
      "step": 11790
    },
    {
      "epoch": 8.292340126493324,
      "grad_norm": 30.839868545532227,
      "learning_rate": 1.7153900210822206e-05,
      "loss": -10.9073,
      "step": 11800
    },
    {
      "epoch": 8.299367533380183,
      "grad_norm": 64.05496215820312,
      "learning_rate": 1.708362614195362e-05,
      "loss": -10.629,
      "step": 11810
    },
    {
      "epoch": 8.306394940267042,
      "grad_norm": 84.74911499023438,
      "learning_rate": 1.7013352073085032e-05,
      "loss": -11.3929,
      "step": 11820
    },
    {
      "epoch": 8.3134223471539,
      "grad_norm": 26.340913772583008,
      "learning_rate": 1.6943078004216446e-05,
      "loss": -11.4221,
      "step": 11830
    },
    {
      "epoch": 8.320449754040759,
      "grad_norm": 39.85670852661133,
      "learning_rate": 1.687280393534786e-05,
      "loss": -10.9345,
      "step": 11840
    },
    {
      "epoch": 8.327477160927618,
      "grad_norm": 39.67416763305664,
      "learning_rate": 1.680252986647927e-05,
      "loss": -11.5079,
      "step": 11850
    },
    {
      "epoch": 8.334504567814477,
      "grad_norm": 75.21939086914062,
      "learning_rate": 1.673225579761068e-05,
      "loss": -11.4788,
      "step": 11860
    },
    {
      "epoch": 8.341531974701335,
      "grad_norm": 108.55315399169922,
      "learning_rate": 1.6661981728742095e-05,
      "loss": -11.3713,
      "step": 11870
    },
    {
      "epoch": 8.348559381588194,
      "grad_norm": 53.48568344116211,
      "learning_rate": 1.6591707659873508e-05,
      "loss": -11.5812,
      "step": 11880
    },
    {
      "epoch": 8.355586788475053,
      "grad_norm": 43.86680603027344,
      "learning_rate": 1.652143359100492e-05,
      "loss": -11.5439,
      "step": 11890
    },
    {
      "epoch": 8.362614195361912,
      "grad_norm": 32.105865478515625,
      "learning_rate": 1.6451159522136334e-05,
      "loss": -11.5816,
      "step": 11900
    },
    {
      "epoch": 8.36964160224877,
      "grad_norm": 27.568347930908203,
      "learning_rate": 1.6380885453267744e-05,
      "loss": -11.902,
      "step": 11910
    },
    {
      "epoch": 8.376669009135629,
      "grad_norm": 49.15993881225586,
      "learning_rate": 1.6310611384399157e-05,
      "loss": -11.6345,
      "step": 11920
    },
    {
      "epoch": 8.383696416022488,
      "grad_norm": 26.499361038208008,
      "learning_rate": 1.624033731553057e-05,
      "loss": -11.5215,
      "step": 11930
    },
    {
      "epoch": 8.390723822909347,
      "grad_norm": 17.8367862701416,
      "learning_rate": 1.6170063246661983e-05,
      "loss": -11.8922,
      "step": 11940
    },
    {
      "epoch": 8.397751229796205,
      "grad_norm": 12.589048385620117,
      "learning_rate": 1.6099789177793396e-05,
      "loss": -11.8284,
      "step": 11950
    },
    {
      "epoch": 8.404778636683064,
      "grad_norm": 60.403865814208984,
      "learning_rate": 1.602951510892481e-05,
      "loss": -11.5088,
      "step": 11960
    },
    {
      "epoch": 8.411806043569923,
      "grad_norm": 79.38309478759766,
      "learning_rate": 1.595924104005622e-05,
      "loss": -11.819,
      "step": 11970
    },
    {
      "epoch": 8.41883345045678,
      "grad_norm": 43.484832763671875,
      "learning_rate": 1.5888966971187632e-05,
      "loss": -11.5226,
      "step": 11980
    },
    {
      "epoch": 8.42586085734364,
      "grad_norm": 61.396705627441406,
      "learning_rate": 1.5818692902319045e-05,
      "loss": -10.8474,
      "step": 11990
    },
    {
      "epoch": 8.432888264230499,
      "grad_norm": 57.659236907958984,
      "learning_rate": 1.5748418833450458e-05,
      "loss": -11.6598,
      "step": 12000
    },
    {
      "epoch": 8.439915671117358,
      "grad_norm": 29.63235855102539,
      "learning_rate": 1.567814476458187e-05,
      "loss": -11.8379,
      "step": 12010
    },
    {
      "epoch": 8.446943078004216,
      "grad_norm": 18.310922622680664,
      "learning_rate": 1.5607870695713284e-05,
      "loss": -11.5882,
      "step": 12020
    },
    {
      "epoch": 8.453970484891075,
      "grad_norm": 46.96921157836914,
      "learning_rate": 1.5537596626844694e-05,
      "loss": -11.9237,
      "step": 12030
    },
    {
      "epoch": 8.460997891777934,
      "grad_norm": 97.45426177978516,
      "learning_rate": 1.5467322557976107e-05,
      "loss": -11.6606,
      "step": 12040
    },
    {
      "epoch": 8.468025298664793,
      "grad_norm": 27.771974563598633,
      "learning_rate": 1.539704848910752e-05,
      "loss": -11.2872,
      "step": 12050
    },
    {
      "epoch": 8.47505270555165,
      "grad_norm": 46.722145080566406,
      "learning_rate": 1.5326774420238933e-05,
      "loss": -11.7504,
      "step": 12060
    },
    {
      "epoch": 8.48208011243851,
      "grad_norm": 82.61245727539062,
      "learning_rate": 1.5256500351370344e-05,
      "loss": -11.6082,
      "step": 12070
    },
    {
      "epoch": 8.489107519325369,
      "grad_norm": 46.65434646606445,
      "learning_rate": 1.518622628250176e-05,
      "loss": -11.4754,
      "step": 12080
    },
    {
      "epoch": 8.496134926212228,
      "grad_norm": 29.349702835083008,
      "learning_rate": 1.5115952213633169e-05,
      "loss": -11.9029,
      "step": 12090
    },
    {
      "epoch": 8.503162333099086,
      "grad_norm": 33.650108337402344,
      "learning_rate": 1.5045678144764582e-05,
      "loss": -11.7619,
      "step": 12100
    },
    {
      "epoch": 8.510189739985945,
      "grad_norm": 25.78431510925293,
      "learning_rate": 1.4975404075895997e-05,
      "loss": -11.7796,
      "step": 12110
    },
    {
      "epoch": 8.517217146872804,
      "grad_norm": 47.814697265625,
      "learning_rate": 1.4905130007027407e-05,
      "loss": -11.832,
      "step": 12120
    },
    {
      "epoch": 8.524244553759663,
      "grad_norm": 32.876644134521484,
      "learning_rate": 1.483485593815882e-05,
      "loss": -10.9744,
      "step": 12130
    },
    {
      "epoch": 8.53127196064652,
      "grad_norm": 43.12583541870117,
      "learning_rate": 1.4764581869290234e-05,
      "loss": -11.5397,
      "step": 12140
    },
    {
      "epoch": 8.53829936753338,
      "grad_norm": 41.530025482177734,
      "learning_rate": 1.4694307800421644e-05,
      "loss": -11.8477,
      "step": 12150
    },
    {
      "epoch": 8.54532677442024,
      "grad_norm": 24.752283096313477,
      "learning_rate": 1.4624033731553057e-05,
      "loss": -11.8606,
      "step": 12160
    },
    {
      "epoch": 8.552354181307098,
      "grad_norm": 34.31748580932617,
      "learning_rate": 1.4553759662684472e-05,
      "loss": -11.9329,
      "step": 12170
    },
    {
      "epoch": 8.559381588193956,
      "grad_norm": 66.8243408203125,
      "learning_rate": 1.4483485593815882e-05,
      "loss": -11.6832,
      "step": 12180
    },
    {
      "epoch": 8.566408995080815,
      "grad_norm": 17.903358459472656,
      "learning_rate": 1.4413211524947295e-05,
      "loss": -11.1217,
      "step": 12190
    },
    {
      "epoch": 8.573436401967674,
      "grad_norm": 149.9329833984375,
      "learning_rate": 1.434293745607871e-05,
      "loss": -11.6737,
      "step": 12200
    },
    {
      "epoch": 8.580463808854532,
      "grad_norm": 48.48931884765625,
      "learning_rate": 1.427266338721012e-05,
      "loss": -11.859,
      "step": 12210
    },
    {
      "epoch": 8.587491215741391,
      "grad_norm": 35.03912353515625,
      "learning_rate": 1.4202389318341532e-05,
      "loss": -11.6848,
      "step": 12220
    },
    {
      "epoch": 8.59451862262825,
      "grad_norm": 63.922996520996094,
      "learning_rate": 1.4132115249472947e-05,
      "loss": -11.9322,
      "step": 12230
    },
    {
      "epoch": 8.60154602951511,
      "grad_norm": 319.85235595703125,
      "learning_rate": 1.4061841180604357e-05,
      "loss": -10.8493,
      "step": 12240
    },
    {
      "epoch": 8.608573436401969,
      "grad_norm": 33.34087371826172,
      "learning_rate": 1.399156711173577e-05,
      "loss": -11.9355,
      "step": 12250
    },
    {
      "epoch": 8.615600843288826,
      "grad_norm": 33.27643966674805,
      "learning_rate": 1.3921293042867185e-05,
      "loss": -11.8823,
      "step": 12260
    },
    {
      "epoch": 8.622628250175685,
      "grad_norm": 38.60805130004883,
      "learning_rate": 1.3851018973998594e-05,
      "loss": -11.6084,
      "step": 12270
    },
    {
      "epoch": 8.629655657062544,
      "grad_norm": 30.15827751159668,
      "learning_rate": 1.3780744905130007e-05,
      "loss": -11.0597,
      "step": 12280
    },
    {
      "epoch": 8.636683063949402,
      "grad_norm": 39.82501983642578,
      "learning_rate": 1.3710470836261422e-05,
      "loss": -11.5181,
      "step": 12290
    },
    {
      "epoch": 8.643710470836261,
      "grad_norm": 61.89808654785156,
      "learning_rate": 1.3640196767392832e-05,
      "loss": -11.5355,
      "step": 12300
    },
    {
      "epoch": 8.65073787772312,
      "grad_norm": 46.72439956665039,
      "learning_rate": 1.3569922698524245e-05,
      "loss": -11.8667,
      "step": 12310
    },
    {
      "epoch": 8.65776528460998,
      "grad_norm": 35.833595275878906,
      "learning_rate": 1.349964862965566e-05,
      "loss": -11.6405,
      "step": 12320
    },
    {
      "epoch": 8.664792691496837,
      "grad_norm": 41.836238861083984,
      "learning_rate": 1.342937456078707e-05,
      "loss": -11.7991,
      "step": 12330
    },
    {
      "epoch": 8.671820098383696,
      "grad_norm": 28.317813873291016,
      "learning_rate": 1.3359100491918483e-05,
      "loss": -11.7095,
      "step": 12340
    },
    {
      "epoch": 8.678847505270555,
      "grad_norm": 49.15666198730469,
      "learning_rate": 1.3288826423049897e-05,
      "loss": -11.8843,
      "step": 12350
    },
    {
      "epoch": 8.685874912157415,
      "grad_norm": 41.733272552490234,
      "learning_rate": 1.3218552354181307e-05,
      "loss": -11.5194,
      "step": 12360
    },
    {
      "epoch": 8.692902319044272,
      "grad_norm": 45.52893829345703,
      "learning_rate": 1.314827828531272e-05,
      "loss": -11.7101,
      "step": 12370
    },
    {
      "epoch": 8.699929725931131,
      "grad_norm": 29.581478118896484,
      "learning_rate": 1.3078004216444133e-05,
      "loss": -11.8666,
      "step": 12380
    },
    {
      "epoch": 8.70695713281799,
      "grad_norm": 14.674317359924316,
      "learning_rate": 1.3007730147575545e-05,
      "loss": -11.8882,
      "step": 12390
    },
    {
      "epoch": 8.71398453970485,
      "grad_norm": 30.226913452148438,
      "learning_rate": 1.2937456078706958e-05,
      "loss": -11.5591,
      "step": 12400
    },
    {
      "epoch": 8.721011946591707,
      "grad_norm": 54.57102584838867,
      "learning_rate": 1.2867182009838371e-05,
      "loss": -11.6191,
      "step": 12410
    },
    {
      "epoch": 8.728039353478566,
      "grad_norm": 37.246437072753906,
      "learning_rate": 1.2796907940969782e-05,
      "loss": -11.6131,
      "step": 12420
    },
    {
      "epoch": 8.735066760365426,
      "grad_norm": 57.55324935913086,
      "learning_rate": 1.2726633872101195e-05,
      "loss": -11.4826,
      "step": 12430
    },
    {
      "epoch": 8.742094167252285,
      "grad_norm": 30.979101181030273,
      "learning_rate": 1.2656359803232607e-05,
      "loss": -11.9859,
      "step": 12440
    },
    {
      "epoch": 8.749121574139142,
      "grad_norm": 43.2026252746582,
      "learning_rate": 1.258608573436402e-05,
      "loss": -11.7173,
      "step": 12450
    },
    {
      "epoch": 8.756148981026001,
      "grad_norm": 124.55361938476562,
      "learning_rate": 1.2515811665495433e-05,
      "loss": -11.7171,
      "step": 12460
    },
    {
      "epoch": 8.76317638791286,
      "grad_norm": 29.091615676879883,
      "learning_rate": 1.2445537596626846e-05,
      "loss": -11.8272,
      "step": 12470
    },
    {
      "epoch": 8.77020379479972,
      "grad_norm": 48.6833610534668,
      "learning_rate": 1.2375263527758257e-05,
      "loss": -11.9649,
      "step": 12480
    },
    {
      "epoch": 8.777231201686577,
      "grad_norm": 74.18712615966797,
      "learning_rate": 1.230498945888967e-05,
      "loss": -11.3953,
      "step": 12490
    },
    {
      "epoch": 8.784258608573436,
      "grad_norm": 18.373638153076172,
      "learning_rate": 1.2234715390021084e-05,
      "loss": -11.9968,
      "step": 12500
    },
    {
      "epoch": 8.791286015460296,
      "grad_norm": 80.791259765625,
      "learning_rate": 1.2164441321152495e-05,
      "loss": -11.4998,
      "step": 12510
    },
    {
      "epoch": 8.798313422347153,
      "grad_norm": 44.27953338623047,
      "learning_rate": 1.2094167252283908e-05,
      "loss": -11.8845,
      "step": 12520
    },
    {
      "epoch": 8.805340829234012,
      "grad_norm": 45.13335037231445,
      "learning_rate": 1.2023893183415321e-05,
      "loss": -11.8804,
      "step": 12530
    },
    {
      "epoch": 8.812368236120872,
      "grad_norm": 29.268535614013672,
      "learning_rate": 1.1953619114546733e-05,
      "loss": -11.6543,
      "step": 12540
    },
    {
      "epoch": 8.81939564300773,
      "grad_norm": 49.01974105834961,
      "learning_rate": 1.1883345045678146e-05,
      "loss": -11.7461,
      "step": 12550
    },
    {
      "epoch": 8.826423049894588,
      "grad_norm": 24.177719116210938,
      "learning_rate": 1.1813070976809559e-05,
      "loss": -11.7546,
      "step": 12560
    },
    {
      "epoch": 8.833450456781447,
      "grad_norm": 89.56155395507812,
      "learning_rate": 1.174279690794097e-05,
      "loss": -11.5527,
      "step": 12570
    },
    {
      "epoch": 8.840477863668307,
      "grad_norm": 28.610448837280273,
      "learning_rate": 1.1672522839072383e-05,
      "loss": -11.8334,
      "step": 12580
    },
    {
      "epoch": 8.847505270555166,
      "grad_norm": 45.86874771118164,
      "learning_rate": 1.1602248770203796e-05,
      "loss": -11.7257,
      "step": 12590
    },
    {
      "epoch": 8.854532677442023,
      "grad_norm": 61.85297393798828,
      "learning_rate": 1.1531974701335208e-05,
      "loss": -11.8463,
      "step": 12600
    },
    {
      "epoch": 8.861560084328882,
      "grad_norm": 29.108238220214844,
      "learning_rate": 1.146170063246662e-05,
      "loss": -11.9012,
      "step": 12610
    },
    {
      "epoch": 8.868587491215742,
      "grad_norm": 25.995607376098633,
      "learning_rate": 1.1391426563598032e-05,
      "loss": -9.8752,
      "step": 12620
    },
    {
      "epoch": 8.8756148981026,
      "grad_norm": 23.26056671142578,
      "learning_rate": 1.1321152494729445e-05,
      "loss": -11.5932,
      "step": 12630
    },
    {
      "epoch": 8.882642304989458,
      "grad_norm": 27.719104766845703,
      "learning_rate": 1.1250878425860858e-05,
      "loss": -11.9966,
      "step": 12640
    },
    {
      "epoch": 8.889669711876317,
      "grad_norm": 40.053531646728516,
      "learning_rate": 1.118060435699227e-05,
      "loss": -11.7478,
      "step": 12650
    },
    {
      "epoch": 8.896697118763177,
      "grad_norm": 36.32432556152344,
      "learning_rate": 1.1110330288123683e-05,
      "loss": -11.6713,
      "step": 12660
    },
    {
      "epoch": 8.903724525650036,
      "grad_norm": 43.001182556152344,
      "learning_rate": 1.1040056219255096e-05,
      "loss": -11.5699,
      "step": 12670
    },
    {
      "epoch": 8.910751932536893,
      "grad_norm": 20.060518264770508,
      "learning_rate": 1.0969782150386507e-05,
      "loss": -11.7632,
      "step": 12680
    },
    {
      "epoch": 8.917779339423753,
      "grad_norm": 31.293231964111328,
      "learning_rate": 1.089950808151792e-05,
      "loss": -11.5798,
      "step": 12690
    },
    {
      "epoch": 8.924806746310612,
      "grad_norm": 22.592506408691406,
      "learning_rate": 1.0829234012649333e-05,
      "loss": -11.6055,
      "step": 12700
    },
    {
      "epoch": 8.931834153197471,
      "grad_norm": 14.442816734313965,
      "learning_rate": 1.0758959943780745e-05,
      "loss": -11.9392,
      "step": 12710
    },
    {
      "epoch": 8.938861560084328,
      "grad_norm": 89.09557342529297,
      "learning_rate": 1.0688685874912158e-05,
      "loss": -11.7529,
      "step": 12720
    },
    {
      "epoch": 8.945888966971188,
      "grad_norm": 67.52786254882812,
      "learning_rate": 1.0618411806043571e-05,
      "loss": -11.7684,
      "step": 12730
    },
    {
      "epoch": 8.952916373858047,
      "grad_norm": 41.76145553588867,
      "learning_rate": 1.0548137737174982e-05,
      "loss": -11.8389,
      "step": 12740
    },
    {
      "epoch": 8.959943780744904,
      "grad_norm": 20.017425537109375,
      "learning_rate": 1.0477863668306396e-05,
      "loss": -11.8032,
      "step": 12750
    },
    {
      "epoch": 8.966971187631763,
      "grad_norm": 80.12448120117188,
      "learning_rate": 1.0407589599437807e-05,
      "loss": -11.7532,
      "step": 12760
    },
    {
      "epoch": 8.973998594518623,
      "grad_norm": 47.937225341796875,
      "learning_rate": 1.033731553056922e-05,
      "loss": -11.6776,
      "step": 12770
    },
    {
      "epoch": 8.981026001405482,
      "grad_norm": 38.299537658691406,
      "learning_rate": 1.0267041461700633e-05,
      "loss": -11.5072,
      "step": 12780
    },
    {
      "epoch": 8.98805340829234,
      "grad_norm": 120.6097412109375,
      "learning_rate": 1.0196767392832045e-05,
      "loss": -11.7082,
      "step": 12790
    },
    {
      "epoch": 8.995080815179199,
      "grad_norm": 20.532560348510742,
      "learning_rate": 1.0126493323963458e-05,
      "loss": -11.7041,
      "step": 12800
    },
    {
      "epoch": 9.0,
      "eval_runtime": 10.6233,
      "eval_samples_per_second": 64518.807,
      "eval_steps_per_second": 15.814,
      "step": 12807
    },
    {
      "epoch": 9.002108222066058,
      "grad_norm": 23.213001251220703,
      "learning_rate": 1.005621925509487e-05,
      "loss": -11.8552,
      "step": 12810
    },
    {
      "epoch": 9.009135628952917,
      "grad_norm": 36.14106750488281,
      "learning_rate": 9.985945186226282e-06,
      "loss": -11.9217,
      "step": 12820
    },
    {
      "epoch": 9.016163035839774,
      "grad_norm": 47.539852142333984,
      "learning_rate": 9.915671117357695e-06,
      "loss": -11.6597,
      "step": 12830
    },
    {
      "epoch": 9.023190442726634,
      "grad_norm": 23.88654899597168,
      "learning_rate": 9.845397048489108e-06,
      "loss": -11.8126,
      "step": 12840
    },
    {
      "epoch": 9.030217849613493,
      "grad_norm": 42.96556854248047,
      "learning_rate": 9.77512297962052e-06,
      "loss": -11.8459,
      "step": 12850
    },
    {
      "epoch": 9.037245256500352,
      "grad_norm": 334.9452819824219,
      "learning_rate": 9.704848910751933e-06,
      "loss": -10.9944,
      "step": 12860
    },
    {
      "epoch": 9.04427266338721,
      "grad_norm": 60.09976577758789,
      "learning_rate": 9.634574841883346e-06,
      "loss": -11.6198,
      "step": 12870
    },
    {
      "epoch": 9.051300070274069,
      "grad_norm": 21.755390167236328,
      "learning_rate": 9.564300773014757e-06,
      "loss": -11.5924,
      "step": 12880
    },
    {
      "epoch": 9.058327477160928,
      "grad_norm": 29.198856353759766,
      "learning_rate": 9.49402670414617e-06,
      "loss": -11.6057,
      "step": 12890
    },
    {
      "epoch": 9.065354884047787,
      "grad_norm": 45.2401123046875,
      "learning_rate": 9.423752635277583e-06,
      "loss": -11.6952,
      "step": 12900
    },
    {
      "epoch": 9.072382290934645,
      "grad_norm": 21.103546142578125,
      "learning_rate": 9.353478566408995e-06,
      "loss": -11.6621,
      "step": 12910
    },
    {
      "epoch": 9.079409697821504,
      "grad_norm": 33.479583740234375,
      "learning_rate": 9.283204497540408e-06,
      "loss": -12.0204,
      "step": 12920
    },
    {
      "epoch": 9.086437104708363,
      "grad_norm": 91.75601959228516,
      "learning_rate": 9.212930428671821e-06,
      "loss": -10.8659,
      "step": 12930
    },
    {
      "epoch": 9.093464511595222,
      "grad_norm": 26.40155792236328,
      "learning_rate": 9.142656359803232e-06,
      "loss": -11.8134,
      "step": 12940
    },
    {
      "epoch": 9.10049191848208,
      "grad_norm": 26.236743927001953,
      "learning_rate": 9.072382290934645e-06,
      "loss": -11.9191,
      "step": 12950
    },
    {
      "epoch": 9.107519325368939,
      "grad_norm": 316.3606872558594,
      "learning_rate": 9.002108222066059e-06,
      "loss": -11.1781,
      "step": 12960
    },
    {
      "epoch": 9.114546732255798,
      "grad_norm": 43.15142059326172,
      "learning_rate": 8.93183415319747e-06,
      "loss": -11.8355,
      "step": 12970
    },
    {
      "epoch": 9.121574139142655,
      "grad_norm": 30.792869567871094,
      "learning_rate": 8.861560084328883e-06,
      "loss": -12.0269,
      "step": 12980
    },
    {
      "epoch": 9.128601546029515,
      "grad_norm": 33.4876823425293,
      "learning_rate": 8.791286015460296e-06,
      "loss": -11.934,
      "step": 12990
    },
    {
      "epoch": 9.135628952916374,
      "grad_norm": 95.69486236572266,
      "learning_rate": 8.721011946591708e-06,
      "loss": -11.5819,
      "step": 13000
    },
    {
      "epoch": 9.142656359803233,
      "grad_norm": 44.91890335083008,
      "learning_rate": 8.65073787772312e-06,
      "loss": -12.0191,
      "step": 13010
    },
    {
      "epoch": 9.14968376669009,
      "grad_norm": 24.374486923217773,
      "learning_rate": 8.580463808854534e-06,
      "loss": -11.8472,
      "step": 13020
    },
    {
      "epoch": 9.15671117357695,
      "grad_norm": 99.1837158203125,
      "learning_rate": 8.510189739985945e-06,
      "loss": -11.5864,
      "step": 13030
    },
    {
      "epoch": 9.163738580463809,
      "grad_norm": 69.99658966064453,
      "learning_rate": 8.439915671117358e-06,
      "loss": -11.9459,
      "step": 13040
    },
    {
      "epoch": 9.170765987350668,
      "grad_norm": 49.18947219848633,
      "learning_rate": 8.369641602248771e-06,
      "loss": -11.9494,
      "step": 13050
    },
    {
      "epoch": 9.177793394237526,
      "grad_norm": 25.665498733520508,
      "learning_rate": 8.299367533380183e-06,
      "loss": -12.0533,
      "step": 13060
    },
    {
      "epoch": 9.184820801124385,
      "grad_norm": 126.25289916992188,
      "learning_rate": 8.229093464511596e-06,
      "loss": -11.5663,
      "step": 13070
    },
    {
      "epoch": 9.191848208011244,
      "grad_norm": 36.270606994628906,
      "learning_rate": 8.158819395643009e-06,
      "loss": -11.845,
      "step": 13080
    },
    {
      "epoch": 9.198875614898103,
      "grad_norm": 63.8846321105957,
      "learning_rate": 8.08854532677442e-06,
      "loss": -11.8275,
      "step": 13090
    },
    {
      "epoch": 9.20590302178496,
      "grad_norm": 36.94062042236328,
      "learning_rate": 8.018271257905833e-06,
      "loss": -11.9522,
      "step": 13100
    },
    {
      "epoch": 9.21293042867182,
      "grad_norm": 31.595661163330078,
      "learning_rate": 7.947997189037246e-06,
      "loss": -11.353,
      "step": 13110
    },
    {
      "epoch": 9.219957835558679,
      "grad_norm": 50.190242767333984,
      "learning_rate": 7.877723120168658e-06,
      "loss": -11.5303,
      "step": 13120
    },
    {
      "epoch": 9.226985242445538,
      "grad_norm": 34.09358596801758,
      "learning_rate": 7.807449051300071e-06,
      "loss": -11.7063,
      "step": 13130
    },
    {
      "epoch": 9.234012649332396,
      "grad_norm": 33.52705764770508,
      "learning_rate": 7.737174982431484e-06,
      "loss": -11.617,
      "step": 13140
    },
    {
      "epoch": 9.241040056219255,
      "grad_norm": 20.867549896240234,
      "learning_rate": 7.666900913562895e-06,
      "loss": -11.6704,
      "step": 13150
    },
    {
      "epoch": 9.248067463106114,
      "grad_norm": 24.090150833129883,
      "learning_rate": 7.5966268446943085e-06,
      "loss": -12.051,
      "step": 13160
    },
    {
      "epoch": 9.255094869992973,
      "grad_norm": 29.313953399658203,
      "learning_rate": 7.526352775825721e-06,
      "loss": -12.0278,
      "step": 13170
    },
    {
      "epoch": 9.26212227687983,
      "grad_norm": 44.75120162963867,
      "learning_rate": 7.456078706957133e-06,
      "loss": -11.8824,
      "step": 13180
    },
    {
      "epoch": 9.26914968376669,
      "grad_norm": 32.870628356933594,
      "learning_rate": 7.385804638088546e-06,
      "loss": -12.0525,
      "step": 13190
    },
    {
      "epoch": 9.27617709065355,
      "grad_norm": 38.098365783691406,
      "learning_rate": 7.315530569219958e-06,
      "loss": -11.7235,
      "step": 13200
    },
    {
      "epoch": 9.283204497540408,
      "grad_norm": 49.2414665222168,
      "learning_rate": 7.2452565003513705e-06,
      "loss": -11.946,
      "step": 13210
    },
    {
      "epoch": 9.290231904427266,
      "grad_norm": 43.80906295776367,
      "learning_rate": 7.174982431482784e-06,
      "loss": -11.8726,
      "step": 13220
    },
    {
      "epoch": 9.297259311314125,
      "grad_norm": 33.907020568847656,
      "learning_rate": 7.104708362614196e-06,
      "loss": -11.866,
      "step": 13230
    },
    {
      "epoch": 9.304286718200984,
      "grad_norm": 27.968265533447266,
      "learning_rate": 7.034434293745608e-06,
      "loss": -11.9723,
      "step": 13240
    },
    {
      "epoch": 9.311314125087842,
      "grad_norm": 33.8652458190918,
      "learning_rate": 6.964160224877021e-06,
      "loss": -11.4999,
      "step": 13250
    },
    {
      "epoch": 9.318341531974701,
      "grad_norm": 123.13638305664062,
      "learning_rate": 6.893886156008433e-06,
      "loss": -11.5885,
      "step": 13260
    },
    {
      "epoch": 9.32536893886156,
      "grad_norm": 41.36563491821289,
      "learning_rate": 6.823612087139846e-06,
      "loss": -11.8666,
      "step": 13270
    },
    {
      "epoch": 9.33239634574842,
      "grad_norm": 25.96115493774414,
      "learning_rate": 6.753338018271259e-06,
      "loss": -11.6926,
      "step": 13280
    },
    {
      "epoch": 9.339423752635277,
      "grad_norm": 24.102880477905273,
      "learning_rate": 6.683063949402671e-06,
      "loss": -12.0655,
      "step": 13290
    },
    {
      "epoch": 9.346451159522136,
      "grad_norm": 33.644832611083984,
      "learning_rate": 6.612789880534083e-06,
      "loss": -11.782,
      "step": 13300
    },
    {
      "epoch": 9.353478566408995,
      "grad_norm": 28.286426544189453,
      "learning_rate": 6.542515811665496e-06,
      "loss": -11.8762,
      "step": 13310
    },
    {
      "epoch": 9.360505973295854,
      "grad_norm": 50.78089141845703,
      "learning_rate": 6.4722417427969086e-06,
      "loss": -11.7,
      "step": 13320
    },
    {
      "epoch": 9.367533380182712,
      "grad_norm": 24.106592178344727,
      "learning_rate": 6.401967673928321e-06,
      "loss": -11.7949,
      "step": 13330
    },
    {
      "epoch": 9.374560787069571,
      "grad_norm": Infinity,
      "learning_rate": 6.338721011946592e-06,
      "loss": -11.0108,
      "step": 13340
    },
    {
      "epoch": 9.38158819395643,
      "grad_norm": 51.55275344848633,
      "learning_rate": 6.268446943078005e-06,
      "loss": -11.806,
      "step": 13350
    },
    {
      "epoch": 9.38861560084329,
      "grad_norm": 35.7076416015625,
      "learning_rate": 6.198172874209418e-06,
      "loss": -11.7947,
      "step": 13360
    },
    {
      "epoch": 9.395643007730147,
      "grad_norm": 25.779460906982422,
      "learning_rate": 6.127898805340829e-06,
      "loss": -11.7152,
      "step": 13370
    },
    {
      "epoch": 9.402670414617006,
      "grad_norm": 26.385961532592773,
      "learning_rate": 6.057624736472242e-06,
      "loss": -11.8013,
      "step": 13380
    },
    {
      "epoch": 9.409697821503865,
      "grad_norm": 17.316097259521484,
      "learning_rate": 5.9873506676036545e-06,
      "loss": -11.7721,
      "step": 13390
    },
    {
      "epoch": 9.416725228390725,
      "grad_norm": 51.433048248291016,
      "learning_rate": 5.917076598735067e-06,
      "loss": -11.8026,
      "step": 13400
    },
    {
      "epoch": 9.423752635277582,
      "grad_norm": 35.572322845458984,
      "learning_rate": 5.84680252986648e-06,
      "loss": -12.059,
      "step": 13410
    },
    {
      "epoch": 9.430780042164441,
      "grad_norm": 43.465675354003906,
      "learning_rate": 5.776528460997892e-06,
      "loss": -11.4672,
      "step": 13420
    },
    {
      "epoch": 9.4378074490513,
      "grad_norm": 36.58182144165039,
      "learning_rate": 5.706254392129304e-06,
      "loss": -11.7116,
      "step": 13430
    },
    {
      "epoch": 9.44483485593816,
      "grad_norm": 73.69405364990234,
      "learning_rate": 5.635980323260717e-06,
      "loss": -10.5476,
      "step": 13440
    },
    {
      "epoch": 9.451862262825017,
      "grad_norm": 47.0312614440918,
      "learning_rate": 5.56570625439213e-06,
      "loss": -11.789,
      "step": 13450
    },
    {
      "epoch": 9.458889669711876,
      "grad_norm": 18.441604614257812,
      "learning_rate": 5.495432185523542e-06,
      "loss": -11.7954,
      "step": 13460
    },
    {
      "epoch": 9.465917076598735,
      "grad_norm": 20.114456176757812,
      "learning_rate": 5.425158116654955e-06,
      "loss": -11.7907,
      "step": 13470
    },
    {
      "epoch": 9.472944483485595,
      "grad_norm": 19.285091400146484,
      "learning_rate": 5.354884047786367e-06,
      "loss": -11.9062,
      "step": 13480
    },
    {
      "epoch": 9.479971890372452,
      "grad_norm": 19.900516510009766,
      "learning_rate": 5.2846099789177794e-06,
      "loss": -12.0393,
      "step": 13490
    },
    {
      "epoch": 9.486999297259311,
      "grad_norm": 330.0072937011719,
      "learning_rate": 5.2143359100491925e-06,
      "loss": -10.8939,
      "step": 13500
    },
    {
      "epoch": 9.49402670414617,
      "grad_norm": 22.260828018188477,
      "learning_rate": 5.144061841180605e-06,
      "loss": -11.7235,
      "step": 13510
    },
    {
      "epoch": 9.501054111033028,
      "grad_norm": 34.35255813598633,
      "learning_rate": 5.073787772312017e-06,
      "loss": -12.0495,
      "step": 13520
    },
    {
      "epoch": 9.508081517919887,
      "grad_norm": 33.366050720214844,
      "learning_rate": 5.00351370344343e-06,
      "loss": -11.7806,
      "step": 13530
    },
    {
      "epoch": 9.515108924806746,
      "grad_norm": 20.291587829589844,
      "learning_rate": 4.9332396345748415e-06,
      "loss": -11.8754,
      "step": 13540
    },
    {
      "epoch": 9.522136331693606,
      "grad_norm": 30.65069007873535,
      "learning_rate": 4.862965565706255e-06,
      "loss": -11.9791,
      "step": 13550
    },
    {
      "epoch": 9.529163738580463,
      "grad_norm": 15.549654960632324,
      "learning_rate": 4.792691496837668e-06,
      "loss": -11.983,
      "step": 13560
    },
    {
      "epoch": 9.536191145467322,
      "grad_norm": 20.64632225036621,
      "learning_rate": 4.722417427969079e-06,
      "loss": -11.8839,
      "step": 13570
    },
    {
      "epoch": 9.543218552354181,
      "grad_norm": 16.52248764038086,
      "learning_rate": 4.652143359100492e-06,
      "loss": -11.8029,
      "step": 13580
    },
    {
      "epoch": 9.55024595924104,
      "grad_norm": 25.413124084472656,
      "learning_rate": 4.581869290231905e-06,
      "loss": -11.8009,
      "step": 13590
    },
    {
      "epoch": 9.557273366127898,
      "grad_norm": 90.28250122070312,
      "learning_rate": 4.511595221363317e-06,
      "loss": -11.9038,
      "step": 13600
    },
    {
      "epoch": 9.564300773014757,
      "grad_norm": 324.5247802734375,
      "learning_rate": 4.44132115249473e-06,
      "loss": -10.8408,
      "step": 13610
    },
    {
      "epoch": 9.571328179901617,
      "grad_norm": 21.838403701782227,
      "learning_rate": 4.371047083626143e-06,
      "loss": -11.961,
      "step": 13620
    },
    {
      "epoch": 9.578355586788476,
      "grad_norm": 26.2536563873291,
      "learning_rate": 4.300773014757554e-06,
      "loss": -12.0443,
      "step": 13630
    },
    {
      "epoch": 9.585382993675333,
      "grad_norm": 35.71236801147461,
      "learning_rate": 4.230498945888967e-06,
      "loss": -11.9905,
      "step": 13640
    },
    {
      "epoch": 9.592410400562192,
      "grad_norm": 25.415756225585938,
      "learning_rate": 4.16022487702038e-06,
      "loss": -12.0017,
      "step": 13650
    },
    {
      "epoch": 9.599437807449052,
      "grad_norm": 19.88515853881836,
      "learning_rate": 4.089950808151792e-06,
      "loss": -11.7197,
      "step": 13660
    },
    {
      "epoch": 9.60646521433591,
      "grad_norm": 72.85547637939453,
      "learning_rate": 4.019676739283205e-06,
      "loss": -11.8203,
      "step": 13670
    },
    {
      "epoch": 9.613492621222768,
      "grad_norm": 84.24689483642578,
      "learning_rate": 3.949402670414618e-06,
      "loss": -11.8028,
      "step": 13680
    },
    {
      "epoch": 9.620520028109627,
      "grad_norm": 51.64821243286133,
      "learning_rate": 3.879128601546029e-06,
      "loss": -11.452,
      "step": 13690
    },
    {
      "epoch": 9.627547434996487,
      "grad_norm": 22.786510467529297,
      "learning_rate": 3.8088545326774424e-06,
      "loss": -11.9694,
      "step": 13700
    },
    {
      "epoch": 9.634574841883346,
      "grad_norm": 39.15122985839844,
      "learning_rate": 3.7385804638088543e-06,
      "loss": -11.6236,
      "step": 13710
    },
    {
      "epoch": 9.641602248770203,
      "grad_norm": 14.647618293762207,
      "learning_rate": 3.668306394940267e-06,
      "loss": -11.8248,
      "step": 13720
    },
    {
      "epoch": 9.648629655657063,
      "grad_norm": 16.380380630493164,
      "learning_rate": 3.59803232607168e-06,
      "loss": -11.7976,
      "step": 13730
    },
    {
      "epoch": 9.655657062543922,
      "grad_norm": 18.347490310668945,
      "learning_rate": 3.527758257203092e-06,
      "loss": -12.091,
      "step": 13740
    },
    {
      "epoch": 9.66268446943078,
      "grad_norm": 15.87154483795166,
      "learning_rate": 3.4574841883345045e-06,
      "loss": -11.8984,
      "step": 13750
    },
    {
      "epoch": 9.669711876317638,
      "grad_norm": 27.988449096679688,
      "learning_rate": 3.3872101194659176e-06,
      "loss": -11.1316,
      "step": 13760
    },
    {
      "epoch": 9.676739283204498,
      "grad_norm": 35.08831787109375,
      "learning_rate": 3.3169360505973294e-06,
      "loss": -11.8959,
      "step": 13770
    },
    {
      "epoch": 9.683766690091357,
      "grad_norm": 20.24333381652832,
      "learning_rate": 3.246661981728742e-06,
      "loss": -11.8191,
      "step": 13780
    },
    {
      "epoch": 9.690794096978214,
      "grad_norm": 22.07221794128418,
      "learning_rate": 3.1763879128601547e-06,
      "loss": -11.8961,
      "step": 13790
    },
    {
      "epoch": 9.697821503865073,
      "grad_norm": 23.54315948486328,
      "learning_rate": 3.1061138439915674e-06,
      "loss": -11.8923,
      "step": 13800
    },
    {
      "epoch": 9.704848910751933,
      "grad_norm": 41.35714340209961,
      "learning_rate": 3.0358397751229796e-06,
      "loss": -10.9463,
      "step": 13810
    },
    {
      "epoch": 9.711876317638792,
      "grad_norm": 32.698333740234375,
      "learning_rate": 2.9655657062543923e-06,
      "loss": -11.9205,
      "step": 13820
    },
    {
      "epoch": 9.71890372452565,
      "grad_norm": 20.519287109375,
      "learning_rate": 2.895291637385805e-06,
      "loss": -11.9044,
      "step": 13830
    },
    {
      "epoch": 9.725931131412509,
      "grad_norm": 13.553871154785156,
      "learning_rate": 2.825017568517217e-06,
      "loss": -11.9056,
      "step": 13840
    },
    {
      "epoch": 9.732958538299368,
      "grad_norm": 27.141263961791992,
      "learning_rate": 2.75474349964863e-06,
      "loss": -11.7192,
      "step": 13850
    },
    {
      "epoch": 9.739985945186227,
      "grad_norm": 31.397632598876953,
      "learning_rate": 2.6844694307800425e-06,
      "loss": -12.0623,
      "step": 13860
    },
    {
      "epoch": 9.747013352073084,
      "grad_norm": 39.46271896362305,
      "learning_rate": 2.6141953619114548e-06,
      "loss": -11.6486,
      "step": 13870
    },
    {
      "epoch": 9.754040758959944,
      "grad_norm": 119.5503921508789,
      "learning_rate": 2.543921293042867e-06,
      "loss": -11.8437,
      "step": 13880
    },
    {
      "epoch": 9.761068165846803,
      "grad_norm": 76.65440368652344,
      "learning_rate": 2.47364722417428e-06,
      "loss": -11.7268,
      "step": 13890
    },
    {
      "epoch": 9.768095572733662,
      "grad_norm": 51.84571838378906,
      "learning_rate": 2.4033731553056924e-06,
      "loss": -11.459,
      "step": 13900
    },
    {
      "epoch": 9.77512297962052,
      "grad_norm": 35.77824783325195,
      "learning_rate": 2.3330990864371046e-06,
      "loss": -11.9944,
      "step": 13910
    },
    {
      "epoch": 9.782150386507379,
      "grad_norm": 18.230043411254883,
      "learning_rate": 2.2628250175685177e-06,
      "loss": -11.9864,
      "step": 13920
    },
    {
      "epoch": 9.789177793394238,
      "grad_norm": 8.959959030151367,
      "learning_rate": 2.19255094869993e-06,
      "loss": -11.7654,
      "step": 13930
    },
    {
      "epoch": 9.796205200281097,
      "grad_norm": 31.875629425048828,
      "learning_rate": 2.122276879831342e-06,
      "loss": -12.0565,
      "step": 13940
    },
    {
      "epoch": 9.803232607167955,
      "grad_norm": 20.686931610107422,
      "learning_rate": 2.052002810962755e-06,
      "loss": -11.8226,
      "step": 13950
    },
    {
      "epoch": 9.810260014054814,
      "grad_norm": 77.58417510986328,
      "learning_rate": 1.9817287420941675e-06,
      "loss": -11.7266,
      "step": 13960
    },
    {
      "epoch": 9.817287420941673,
      "grad_norm": 11.8236665725708,
      "learning_rate": 1.9114546732255797e-06,
      "loss": -11.8985,
      "step": 13970
    },
    {
      "epoch": 9.82431482782853,
      "grad_norm": 82.76438903808594,
      "learning_rate": 1.8411806043569922e-06,
      "loss": -11.8142,
      "step": 13980
    },
    {
      "epoch": 9.83134223471539,
      "grad_norm": 22.18767738342285,
      "learning_rate": 1.7709065354884049e-06,
      "loss": -12.0612,
      "step": 13990
    },
    {
      "epoch": 9.838369641602249,
      "grad_norm": 75.78157806396484,
      "learning_rate": 1.7006324666198173e-06,
      "loss": -11.9041,
      "step": 14000
    },
    {
      "epoch": 9.845397048489108,
      "grad_norm": 139.13121032714844,
      "learning_rate": 1.6303583977512298e-06,
      "loss": -11.5326,
      "step": 14010
    },
    {
      "epoch": 9.852424455375965,
      "grad_norm": 14.948513984680176,
      "learning_rate": 1.5600843288826422e-06,
      "loss": -11.9939,
      "step": 14020
    },
    {
      "epoch": 9.859451862262825,
      "grad_norm": 20.904239654541016,
      "learning_rate": 1.4898102600140549e-06,
      "loss": -10.8652,
      "step": 14030
    },
    {
      "epoch": 9.866479269149684,
      "grad_norm": 66.57386779785156,
      "learning_rate": 1.4195361911454676e-06,
      "loss": -11.7465,
      "step": 14040
    },
    {
      "epoch": 9.873506676036543,
      "grad_norm": 30.984111785888672,
      "learning_rate": 1.3492621222768798e-06,
      "loss": -11.3731,
      "step": 14050
    },
    {
      "epoch": 9.8805340829234,
      "grad_norm": 52.45573043823242,
      "learning_rate": 1.2789880534082925e-06,
      "loss": -11.7274,
      "step": 14060
    },
    {
      "epoch": 9.88756148981026,
      "grad_norm": 19.944740295410156,
      "learning_rate": 1.208713984539705e-06,
      "loss": -11.9439,
      "step": 14070
    },
    {
      "epoch": 9.894588896697119,
      "grad_norm": 322.3452453613281,
      "learning_rate": 1.1384399156711174e-06,
      "loss": -10.9708,
      "step": 14080
    },
    {
      "epoch": 9.901616303583978,
      "grad_norm": 17.551422119140625,
      "learning_rate": 1.06816584680253e-06,
      "loss": -11.9127,
      "step": 14090
    },
    {
      "epoch": 9.908643710470836,
      "grad_norm": 326.5729675292969,
      "learning_rate": 9.978917779339423e-07,
      "loss": -11.1229,
      "step": 14100
    },
    {
      "epoch": 9.915671117357695,
      "grad_norm": 26.8031005859375,
      "learning_rate": 9.276177090653549e-07,
      "loss": -11.6301,
      "step": 14110
    },
    {
      "epoch": 9.922698524244554,
      "grad_norm": 25.093610763549805,
      "learning_rate": 8.573436401967675e-07,
      "loss": -11.8269,
      "step": 14120
    },
    {
      "epoch": 9.929725931131413,
      "grad_norm": 25.86585807800293,
      "learning_rate": 7.8706957132818e-07,
      "loss": -11.907,
      "step": 14130
    },
    {
      "epoch": 9.93675333801827,
      "grad_norm": 28.969493865966797,
      "learning_rate": 7.167955024595925e-07,
      "loss": -11.5461,
      "step": 14140
    },
    {
      "epoch": 9.94378074490513,
      "grad_norm": 11.235214233398438,
      "learning_rate": 6.46521433591005e-07,
      "loss": -11.8933,
      "step": 14150
    },
    {
      "epoch": 9.950808151791989,
      "grad_norm": 15.264004707336426,
      "learning_rate": 5.762473647224174e-07,
      "loss": -11.9234,
      "step": 14160
    },
    {
      "epoch": 9.957835558678848,
      "grad_norm": 6.983262062072754,
      "learning_rate": 5.0597329585383e-07,
      "loss": -11.9159,
      "step": 14170
    },
    {
      "epoch": 9.964862965565706,
      "grad_norm": 69.8463134765625,
      "learning_rate": 4.356992269852425e-07,
      "loss": -11.5389,
      "step": 14180
    },
    {
      "epoch": 9.971890372452565,
      "grad_norm": 21.82143211364746,
      "learning_rate": 3.65425158116655e-07,
      "loss": -11.9306,
      "step": 14190
    },
    {
      "epoch": 9.978917779339424,
      "grad_norm": 15.9830904006958,
      "learning_rate": 2.9515108924806744e-07,
      "loss": -11.1227,
      "step": 14200
    },
    {
      "epoch": 9.985945186226282,
      "grad_norm": 76.48733520507812,
      "learning_rate": 2.2487702037948e-07,
      "loss": -10.8845,
      "step": 14210
    },
    {
      "epoch": 9.99297259311314,
      "grad_norm": 73.57192993164062,
      "learning_rate": 1.546029515108925e-07,
      "loss": -11.8211,
      "step": 14220
    },
    {
      "epoch": 10.0,
      "grad_norm": 12.320646286010742,
      "learning_rate": 8.432888264230499e-08,
      "loss": -11.726,
      "step": 14230
    },
    {
      "epoch": 10.0,
      "eval_runtime": 10.6399,
      "eval_samples_per_second": 64418.128,
      "eval_steps_per_second": 15.79,
      "step": 14230
    }
  ],
  "logging_steps": 10,
  "max_steps": 14230,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5495306371530752.0,
  "train_batch_size": 1024,
  "trial_name": null,
  "trial_params": null
}
