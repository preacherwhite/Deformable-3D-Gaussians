{
  "best_metric": 10.7188,
  "best_model_checkpoint": "./results/checkpoint-2846",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 5692,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007027406886858749,
      "grad_norm": 1.6860004663467407,
      "learning_rate": 9.993675333801827e-05,
      "loss": -1.2921,
      "step": 10
    },
    {
      "epoch": 0.014054813773717497,
      "grad_norm": 1.5223720073699951,
      "learning_rate": 9.98664792691497e-05,
      "loss": -1.3807,
      "step": 20
    },
    {
      "epoch": 0.02108222066057625,
      "grad_norm": 0.8838928937911987,
      "learning_rate": 9.97962052002811e-05,
      "loss": -1.4051,
      "step": 30
    },
    {
      "epoch": 0.028109627547434995,
      "grad_norm": 0.9422695636749268,
      "learning_rate": 9.97259311314125e-05,
      "loss": -1.4302,
      "step": 40
    },
    {
      "epoch": 0.035137034434293744,
      "grad_norm": 2.1020593643188477,
      "learning_rate": 9.965565706254392e-05,
      "loss": -1.4926,
      "step": 50
    },
    {
      "epoch": 0.0421644413211525,
      "grad_norm": 1.212904453277588,
      "learning_rate": 9.958538299367534e-05,
      "loss": -1.5129,
      "step": 60
    },
    {
      "epoch": 0.049191848208011243,
      "grad_norm": 1.2417880296707153,
      "learning_rate": 9.951510892480675e-05,
      "loss": -1.5735,
      "step": 70
    },
    {
      "epoch": 0.05621925509486999,
      "grad_norm": 1.4859538078308105,
      "learning_rate": 9.944483485593816e-05,
      "loss": -1.657,
      "step": 80
    },
    {
      "epoch": 0.06324666198172874,
      "grad_norm": 2.792626142501831,
      "learning_rate": 9.937456078706959e-05,
      "loss": -1.772,
      "step": 90
    },
    {
      "epoch": 0.07027406886858749,
      "grad_norm": 4.062569618225098,
      "learning_rate": 9.930428671820099e-05,
      "loss": -2.0051,
      "step": 100
    },
    {
      "epoch": 0.07730147575544624,
      "grad_norm": 10.54062557220459,
      "learning_rate": 9.924104005621925e-05,
      "loss": -2.3975,
      "step": 110
    },
    {
      "epoch": 0.084328882642305,
      "grad_norm": Infinity,
      "learning_rate": 9.918482080112439e-05,
      "loss": -2.6963,
      "step": 120
    },
    {
      "epoch": 0.09135628952916373,
      "grad_norm": 11.13524341583252,
      "learning_rate": 9.91145467322558e-05,
      "loss": -2.8551,
      "step": 130
    },
    {
      "epoch": 0.09838369641602249,
      "grad_norm": 27.765460968017578,
      "learning_rate": 9.904427266338721e-05,
      "loss": -3.1458,
      "step": 140
    },
    {
      "epoch": 0.10541110330288124,
      "grad_norm": 50.406158447265625,
      "learning_rate": 9.897399859451863e-05,
      "loss": -3.0714,
      "step": 150
    },
    {
      "epoch": 0.11243851018973998,
      "grad_norm": 7.153792858123779,
      "learning_rate": 9.890372452565004e-05,
      "loss": -3.0842,
      "step": 160
    },
    {
      "epoch": 0.11946591707659873,
      "grad_norm": 19.641448974609375,
      "learning_rate": 9.883345045678145e-05,
      "loss": -3.6879,
      "step": 170
    },
    {
      "epoch": 0.12649332396345747,
      "grad_norm": Infinity,
      "learning_rate": 9.877020379479973e-05,
      "loss": -3.393,
      "step": 180
    },
    {
      "epoch": 0.13352073085031624,
      "grad_norm": 50.81373596191406,
      "learning_rate": 9.869992972593114e-05,
      "loss": -3.653,
      "step": 190
    },
    {
      "epoch": 0.14054813773717498,
      "grad_norm": 45.54872512817383,
      "learning_rate": 9.862965565706254e-05,
      "loss": -3.8452,
      "step": 200
    },
    {
      "epoch": 0.14757554462403374,
      "grad_norm": 20.347394943237305,
      "learning_rate": 9.855938158819396e-05,
      "loss": -4.0533,
      "step": 210
    },
    {
      "epoch": 0.15460295151089248,
      "grad_norm": 25.921110153198242,
      "learning_rate": 9.848910751932538e-05,
      "loss": -4.2427,
      "step": 220
    },
    {
      "epoch": 0.16163035839775122,
      "grad_norm": 13.012264251708984,
      "learning_rate": 9.841883345045678e-05,
      "loss": -4.3694,
      "step": 230
    },
    {
      "epoch": 0.16865776528461,
      "grad_norm": 60.69106674194336,
      "learning_rate": 9.83485593815882e-05,
      "loss": -4.5216,
      "step": 240
    },
    {
      "epoch": 0.17568517217146873,
      "grad_norm": 18.223241806030273,
      "learning_rate": 9.827828531271961e-05,
      "loss": -4.592,
      "step": 250
    },
    {
      "epoch": 0.18271257905832747,
      "grad_norm": 16.20248794555664,
      "learning_rate": 9.820801124385103e-05,
      "loss": -4.5475,
      "step": 260
    },
    {
      "epoch": 0.18973998594518623,
      "grad_norm": 54.024654388427734,
      "learning_rate": 9.813773717498243e-05,
      "loss": -4.8331,
      "step": 270
    },
    {
      "epoch": 0.19676739283204497,
      "grad_norm": 21.992366790771484,
      "learning_rate": 9.806746310611385e-05,
      "loss": -4.6675,
      "step": 280
    },
    {
      "epoch": 0.2037947997189037,
      "grad_norm": Infinity,
      "learning_rate": 9.800421644413213e-05,
      "loss": -4.5446,
      "step": 290
    },
    {
      "epoch": 0.21082220660576248,
      "grad_norm": 58.36254119873047,
      "learning_rate": 9.793394237526353e-05,
      "loss": -4.593,
      "step": 300
    },
    {
      "epoch": 0.21784961349262122,
      "grad_norm": 33.2662467956543,
      "learning_rate": 9.786366830639495e-05,
      "loss": -4.8962,
      "step": 310
    },
    {
      "epoch": 0.22487702037947996,
      "grad_norm": 43.39387512207031,
      "learning_rate": 9.779339423752636e-05,
      "loss": -4.9067,
      "step": 320
    },
    {
      "epoch": 0.23190442726633873,
      "grad_norm": 31.868099212646484,
      "learning_rate": 9.772312016865777e-05,
      "loss": -4.8554,
      "step": 330
    },
    {
      "epoch": 0.23893183415319746,
      "grad_norm": 32.99828338623047,
      "learning_rate": 9.765284609978918e-05,
      "loss": -4.7624,
      "step": 340
    },
    {
      "epoch": 0.24595924104005623,
      "grad_norm": 22.345361709594727,
      "learning_rate": 9.75825720309206e-05,
      "loss": -4.3312,
      "step": 350
    },
    {
      "epoch": 0.25298664792691494,
      "grad_norm": 19.150009155273438,
      "learning_rate": 9.751229796205202e-05,
      "loss": -5.3253,
      "step": 360
    },
    {
      "epoch": 0.2600140548137737,
      "grad_norm": 73.74205017089844,
      "learning_rate": 9.744202389318342e-05,
      "loss": -4.8292,
      "step": 370
    },
    {
      "epoch": 0.2670414617006325,
      "grad_norm": 66.38163757324219,
      "learning_rate": 9.737174982431483e-05,
      "loss": -5.1897,
      "step": 380
    },
    {
      "epoch": 0.27406886858749124,
      "grad_norm": 49.51173782348633,
      "learning_rate": 9.730147575544625e-05,
      "loss": -5.6284,
      "step": 390
    },
    {
      "epoch": 0.28109627547434995,
      "grad_norm": 35.72504806518555,
      "learning_rate": 9.723120168657765e-05,
      "loss": -5.4332,
      "step": 400
    },
    {
      "epoch": 0.2881236823612087,
      "grad_norm": 35.065677642822266,
      "learning_rate": 9.716092761770907e-05,
      "loss": -5.3912,
      "step": 410
    },
    {
      "epoch": 0.2951510892480675,
      "grad_norm": 64.38773345947266,
      "learning_rate": 9.709065354884049e-05,
      "loss": -5.3189,
      "step": 420
    },
    {
      "epoch": 0.3021784961349262,
      "grad_norm": 48.378482818603516,
      "learning_rate": 9.702037947997189e-05,
      "loss": -5.4603,
      "step": 430
    },
    {
      "epoch": 0.30920590302178497,
      "grad_norm": 23.19247055053711,
      "learning_rate": 9.69501054111033e-05,
      "loss": -5.4158,
      "step": 440
    },
    {
      "epoch": 0.31623330990864373,
      "grad_norm": 64.5969467163086,
      "learning_rate": 9.687983134223472e-05,
      "loss": -5.1215,
      "step": 450
    },
    {
      "epoch": 0.32326071679550245,
      "grad_norm": 44.53466033935547,
      "learning_rate": 9.680955727336614e-05,
      "loss": -5.9027,
      "step": 460
    },
    {
      "epoch": 0.3302881236823612,
      "grad_norm": 30.518274307250977,
      "learning_rate": 9.673928320449754e-05,
      "loss": -5.459,
      "step": 470
    },
    {
      "epoch": 0.33731553056922,
      "grad_norm": 101.69618225097656,
      "learning_rate": 9.666900913562896e-05,
      "loss": -5.2915,
      "step": 480
    },
    {
      "epoch": 0.3443429374560787,
      "grad_norm": 71.34317016601562,
      "learning_rate": 9.659873506676038e-05,
      "loss": -4.9095,
      "step": 490
    },
    {
      "epoch": 0.35137034434293746,
      "grad_norm": 84.34651184082031,
      "learning_rate": 9.652846099789178e-05,
      "loss": -5.6687,
      "step": 500
    },
    {
      "epoch": 0.3583977512297962,
      "grad_norm": 42.30650329589844,
      "learning_rate": 9.64581869290232e-05,
      "loss": -5.7226,
      "step": 510
    },
    {
      "epoch": 0.36542515811665494,
      "grad_norm": 79.91998291015625,
      "learning_rate": 9.638791286015461e-05,
      "loss": -5.7669,
      "step": 520
    },
    {
      "epoch": 0.3724525650035137,
      "grad_norm": 63.55778503417969,
      "learning_rate": 9.631763879128603e-05,
      "loss": -5.9739,
      "step": 530
    },
    {
      "epoch": 0.37947997189037247,
      "grad_norm": 23.731168746948242,
      "learning_rate": 9.624736472241743e-05,
      "loss": -5.8008,
      "step": 540
    },
    {
      "epoch": 0.3865073787772312,
      "grad_norm": 58.560874938964844,
      "learning_rate": 9.617709065354885e-05,
      "loss": -5.2713,
      "step": 550
    },
    {
      "epoch": 0.39353478566408995,
      "grad_norm": 48.000518798828125,
      "learning_rate": 9.610681658468026e-05,
      "loss": -5.8234,
      "step": 560
    },
    {
      "epoch": 0.4005621925509487,
      "grad_norm": 64.36412048339844,
      "learning_rate": 9.603654251581167e-05,
      "loss": -5.5284,
      "step": 570
    },
    {
      "epoch": 0.4075895994378074,
      "grad_norm": 53.271366119384766,
      "learning_rate": 9.596626844694308e-05,
      "loss": -5.5427,
      "step": 580
    },
    {
      "epoch": 0.4146170063246662,
      "grad_norm": 79.79280090332031,
      "learning_rate": 9.58959943780745e-05,
      "loss": -5.9536,
      "step": 590
    },
    {
      "epoch": 0.42164441321152496,
      "grad_norm": 150.04550170898438,
      "learning_rate": 9.582572030920592e-05,
      "loss": -5.8571,
      "step": 600
    },
    {
      "epoch": 0.42867182009838367,
      "grad_norm": 93.87496948242188,
      "learning_rate": 9.575544624033732e-05,
      "loss": -6.0092,
      "step": 610
    },
    {
      "epoch": 0.43569922698524244,
      "grad_norm": 117.1666259765625,
      "learning_rate": 9.568517217146874e-05,
      "loss": -6.087,
      "step": 620
    },
    {
      "epoch": 0.4427266338721012,
      "grad_norm": 50.84917449951172,
      "learning_rate": 9.561489810260015e-05,
      "loss": -6.1588,
      "step": 630
    },
    {
      "epoch": 0.4497540407589599,
      "grad_norm": 51.93496322631836,
      "learning_rate": 9.554462403373155e-05,
      "loss": -6.3485,
      "step": 640
    },
    {
      "epoch": 0.4567814476458187,
      "grad_norm": 32.84553527832031,
      "learning_rate": 9.547434996486297e-05,
      "loss": -6.2964,
      "step": 650
    },
    {
      "epoch": 0.46380885453267745,
      "grad_norm": 60.058799743652344,
      "learning_rate": 9.540407589599439e-05,
      "loss": -6.2155,
      "step": 660
    },
    {
      "epoch": 0.4708362614195362,
      "grad_norm": 32.49545669555664,
      "learning_rate": 9.533380182712579e-05,
      "loss": -6.1921,
      "step": 670
    },
    {
      "epoch": 0.47786366830639493,
      "grad_norm": 27.035213470458984,
      "learning_rate": 9.526352775825721e-05,
      "loss": -6.3931,
      "step": 680
    },
    {
      "epoch": 0.4848910751932537,
      "grad_norm": 81.44160461425781,
      "learning_rate": 9.519325368938862e-05,
      "loss": -6.3278,
      "step": 690
    },
    {
      "epoch": 0.49191848208011246,
      "grad_norm": 158.89794921875,
      "learning_rate": 9.512297962052004e-05,
      "loss": -6.2272,
      "step": 700
    },
    {
      "epoch": 0.4989458889669712,
      "grad_norm": 130.32418823242188,
      "learning_rate": 9.505270555165144e-05,
      "loss": -6.2297,
      "step": 710
    },
    {
      "epoch": 0.5059732958538299,
      "grad_norm": 74.09990692138672,
      "learning_rate": 9.498243148278286e-05,
      "loss": -6.4421,
      "step": 720
    },
    {
      "epoch": 0.5130007027406887,
      "grad_norm": 49.37168884277344,
      "learning_rate": 9.491215741391428e-05,
      "loss": -6.165,
      "step": 730
    },
    {
      "epoch": 0.5200281096275474,
      "grad_norm": 75.5392074584961,
      "learning_rate": 9.484188334504568e-05,
      "loss": -6.5732,
      "step": 740
    },
    {
      "epoch": 0.5270555165144062,
      "grad_norm": 45.11758041381836,
      "learning_rate": 9.47716092761771e-05,
      "loss": -6.3273,
      "step": 750
    },
    {
      "epoch": 0.534082923401265,
      "grad_norm": 95.66844940185547,
      "learning_rate": 9.470133520730851e-05,
      "loss": -6.5303,
      "step": 760
    },
    {
      "epoch": 0.5411103302881237,
      "grad_norm": 59.14422607421875,
      "learning_rate": 9.463106113843993e-05,
      "loss": -6.4698,
      "step": 770
    },
    {
      "epoch": 0.5481377371749825,
      "grad_norm": 68.3644790649414,
      "learning_rate": 9.456078706957133e-05,
      "loss": -6.5819,
      "step": 780
    },
    {
      "epoch": 0.5551651440618411,
      "grad_norm": 72.96343994140625,
      "learning_rate": 9.449754040758961e-05,
      "loss": -6.4701,
      "step": 790
    },
    {
      "epoch": 0.5621925509486999,
      "grad_norm": 47.393516540527344,
      "learning_rate": 9.442726633872103e-05,
      "loss": -6.5356,
      "step": 800
    },
    {
      "epoch": 0.5692199578355587,
      "grad_norm": 99.93370819091797,
      "learning_rate": 9.435699226985243e-05,
      "loss": -6.5243,
      "step": 810
    },
    {
      "epoch": 0.5762473647224174,
      "grad_norm": 168.73931884765625,
      "learning_rate": 9.428671820098383e-05,
      "loss": -6.6569,
      "step": 820
    },
    {
      "epoch": 0.5832747716092762,
      "grad_norm": 50.17271041870117,
      "learning_rate": 9.421644413211526e-05,
      "loss": -6.8741,
      "step": 830
    },
    {
      "epoch": 0.590302178496135,
      "grad_norm": 78.69847869873047,
      "learning_rate": 9.414617006324666e-05,
      "loss": -6.6368,
      "step": 840
    },
    {
      "epoch": 0.5973295853829936,
      "grad_norm": 89.92803192138672,
      "learning_rate": 9.407589599437808e-05,
      "loss": -6.7147,
      "step": 850
    },
    {
      "epoch": 0.6043569922698524,
      "grad_norm": 277.0621337890625,
      "learning_rate": 9.40056219255095e-05,
      "loss": -6.0667,
      "step": 860
    },
    {
      "epoch": 0.6113843991567112,
      "grad_norm": 86.7811508178711,
      "learning_rate": 9.393534785664091e-05,
      "loss": -6.646,
      "step": 870
    },
    {
      "epoch": 0.6184118060435699,
      "grad_norm": 64.3265151977539,
      "learning_rate": 9.386507378777232e-05,
      "loss": -6.1169,
      "step": 880
    },
    {
      "epoch": 0.6254392129304287,
      "grad_norm": 246.79415893554688,
      "learning_rate": 9.379479971890372e-05,
      "loss": -6.7719,
      "step": 890
    },
    {
      "epoch": 0.6324666198172875,
      "grad_norm": 291.45452880859375,
      "learning_rate": 9.372452565003515e-05,
      "loss": -6.7775,
      "step": 900
    },
    {
      "epoch": 0.6394940267041461,
      "grad_norm": 158.0874481201172,
      "learning_rate": 9.365425158116655e-05,
      "loss": -6.8678,
      "step": 910
    },
    {
      "epoch": 0.6465214335910049,
      "grad_norm": 98.92765045166016,
      "learning_rate": 9.358397751229797e-05,
      "loss": -6.6101,
      "step": 920
    },
    {
      "epoch": 0.6535488404778637,
      "grad_norm": 177.81846618652344,
      "learning_rate": 9.351370344342939e-05,
      "loss": -6.7756,
      "step": 930
    },
    {
      "epoch": 0.6605762473647224,
      "grad_norm": 209.10328674316406,
      "learning_rate": 9.344342937456079e-05,
      "loss": -6.7317,
      "step": 940
    },
    {
      "epoch": 0.6676036542515812,
      "grad_norm": 79.2008285522461,
      "learning_rate": 9.33731553056922e-05,
      "loss": -6.856,
      "step": 950
    },
    {
      "epoch": 0.67463106113844,
      "grad_norm": 35.080535888671875,
      "learning_rate": 9.330288123682361e-05,
      "loss": -6.784,
      "step": 960
    },
    {
      "epoch": 0.6816584680252986,
      "grad_norm": 123.33367919921875,
      "learning_rate": 9.323260716795504e-05,
      "loss": -6.8228,
      "step": 970
    },
    {
      "epoch": 0.6886858749121574,
      "grad_norm": 110.54161071777344,
      "learning_rate": 9.316233309908644e-05,
      "loss": -6.8982,
      "step": 980
    },
    {
      "epoch": 0.6957132817990161,
      "grad_norm": 86.40049743652344,
      "learning_rate": 9.309205903021784e-05,
      "loss": -7.0877,
      "step": 990
    },
    {
      "epoch": 0.7027406886858749,
      "grad_norm": 49.513816833496094,
      "learning_rate": 9.302178496134927e-05,
      "loss": -7.0558,
      "step": 1000
    },
    {
      "epoch": 0.7097680955727337,
      "grad_norm": 55.642215728759766,
      "learning_rate": 9.295151089248068e-05,
      "loss": -6.8424,
      "step": 1010
    },
    {
      "epoch": 0.7167955024595924,
      "grad_norm": 65.7772445678711,
      "learning_rate": 9.288123682361209e-05,
      "loss": -6.943,
      "step": 1020
    },
    {
      "epoch": 0.7238229093464511,
      "grad_norm": 68.26837158203125,
      "learning_rate": 9.28109627547435e-05,
      "loss": -6.6938,
      "step": 1030
    },
    {
      "epoch": 0.7308503162333099,
      "grad_norm": 63.334434509277344,
      "learning_rate": 9.274068868587493e-05,
      "loss": -7.1175,
      "step": 1040
    },
    {
      "epoch": 0.7378777231201686,
      "grad_norm": 261.530029296875,
      "learning_rate": 9.267041461700633e-05,
      "loss": -6.9909,
      "step": 1050
    },
    {
      "epoch": 0.7449051300070274,
      "grad_norm": 159.00860595703125,
      "learning_rate": 9.260014054813773e-05,
      "loss": -6.7705,
      "step": 1060
    },
    {
      "epoch": 0.7519325368938862,
      "grad_norm": 91.65581512451172,
      "learning_rate": 9.252986647926916e-05,
      "loss": -6.9613,
      "step": 1070
    },
    {
      "epoch": 0.7589599437807449,
      "grad_norm": 102.63330078125,
      "learning_rate": 9.245959241040056e-05,
      "loss": -7.0755,
      "step": 1080
    },
    {
      "epoch": 0.7659873506676037,
      "grad_norm": 148.8389434814453,
      "learning_rate": 9.238931834153198e-05,
      "loss": -7.1456,
      "step": 1090
    },
    {
      "epoch": 0.7730147575544624,
      "grad_norm": 209.2189178466797,
      "learning_rate": 9.23190442726634e-05,
      "loss": -6.9934,
      "step": 1100
    },
    {
      "epoch": 0.7800421644413211,
      "grad_norm": 120.55187225341797,
      "learning_rate": 9.22487702037948e-05,
      "loss": -6.174,
      "step": 1110
    },
    {
      "epoch": 0.7870695713281799,
      "grad_norm": 129.6073760986328,
      "learning_rate": 9.217849613492622e-05,
      "loss": -6.8899,
      "step": 1120
    },
    {
      "epoch": 0.7940969782150387,
      "grad_norm": 104.16826629638672,
      "learning_rate": 9.210822206605762e-05,
      "loss": -6.9409,
      "step": 1130
    },
    {
      "epoch": 0.8011243851018974,
      "grad_norm": 66.002197265625,
      "learning_rate": 9.203794799718905e-05,
      "loss": -7.2038,
      "step": 1140
    },
    {
      "epoch": 0.8081517919887562,
      "grad_norm": 88.42974853515625,
      "learning_rate": 9.196767392832045e-05,
      "loss": -7.2089,
      "step": 1150
    },
    {
      "epoch": 0.8151791988756149,
      "grad_norm": 219.88674926757812,
      "learning_rate": 9.189739985945187e-05,
      "loss": -7.2009,
      "step": 1160
    },
    {
      "epoch": 0.8222066057624736,
      "grad_norm": 72.87736511230469,
      "learning_rate": 9.182712579058329e-05,
      "loss": -7.2629,
      "step": 1170
    },
    {
      "epoch": 0.8292340126493324,
      "grad_norm": 82.43689727783203,
      "learning_rate": 9.175685172171469e-05,
      "loss": -7.1921,
      "step": 1180
    },
    {
      "epoch": 0.8362614195361912,
      "grad_norm": 59.75246047973633,
      "learning_rate": 9.16865776528461e-05,
      "loss": -7.2145,
      "step": 1190
    },
    {
      "epoch": 0.8432888264230499,
      "grad_norm": 268.36041259765625,
      "learning_rate": 9.161630358397751e-05,
      "loss": -7.0839,
      "step": 1200
    },
    {
      "epoch": 0.8503162333099087,
      "grad_norm": 62.51845932006836,
      "learning_rate": 9.154602951510894e-05,
      "loss": -7.308,
      "step": 1210
    },
    {
      "epoch": 0.8573436401967673,
      "grad_norm": 96.8796157836914,
      "learning_rate": 9.147575544624034e-05,
      "loss": -7.2365,
      "step": 1220
    },
    {
      "epoch": 0.8643710470836261,
      "grad_norm": 63.921627044677734,
      "learning_rate": 9.140548137737174e-05,
      "loss": -7.1622,
      "step": 1230
    },
    {
      "epoch": 0.8713984539704849,
      "grad_norm": 97.95970153808594,
      "learning_rate": 9.133520730850317e-05,
      "loss": -7.3567,
      "step": 1240
    },
    {
      "epoch": 0.8784258608573436,
      "grad_norm": 85.88983154296875,
      "learning_rate": 9.126493323963458e-05,
      "loss": -6.9544,
      "step": 1250
    },
    {
      "epoch": 0.8854532677442024,
      "grad_norm": 231.1924591064453,
      "learning_rate": 9.1194659170766e-05,
      "loss": -7.2324,
      "step": 1260
    },
    {
      "epoch": 0.8924806746310612,
      "grad_norm": 39.58894348144531,
      "learning_rate": 9.11243851018974e-05,
      "loss": -7.4257,
      "step": 1270
    },
    {
      "epoch": 0.8995080815179198,
      "grad_norm": 196.9145965576172,
      "learning_rate": 9.105411103302883e-05,
      "loss": -7.4067,
      "step": 1280
    },
    {
      "epoch": 0.9065354884047786,
      "grad_norm": 72.6319580078125,
      "learning_rate": 9.098383696416023e-05,
      "loss": -7.3203,
      "step": 1290
    },
    {
      "epoch": 0.9135628952916374,
      "grad_norm": 152.64486694335938,
      "learning_rate": 9.091356289529163e-05,
      "loss": -7.3962,
      "step": 1300
    },
    {
      "epoch": 0.9205903021784961,
      "grad_norm": 25.05042266845703,
      "learning_rate": 9.084328882642306e-05,
      "loss": -7.3108,
      "step": 1310
    },
    {
      "epoch": 0.9276177090653549,
      "grad_norm": 151.17747497558594,
      "learning_rate": 9.077301475755447e-05,
      "loss": -7.3855,
      "step": 1320
    },
    {
      "epoch": 0.9346451159522137,
      "grad_norm": 107.43682861328125,
      "learning_rate": 9.070274068868588e-05,
      "loss": -7.4443,
      "step": 1330
    },
    {
      "epoch": 0.9416725228390724,
      "grad_norm": 350.61492919921875,
      "learning_rate": 9.063246661981728e-05,
      "loss": -7.3617,
      "step": 1340
    },
    {
      "epoch": 0.9486999297259311,
      "grad_norm": 83.1407241821289,
      "learning_rate": 9.05621925509487e-05,
      "loss": -7.5029,
      "step": 1350
    },
    {
      "epoch": 0.9557273366127899,
      "grad_norm": 88.5924301147461,
      "learning_rate": 9.049191848208012e-05,
      "loss": -7.1782,
      "step": 1360
    },
    {
      "epoch": 0.9627547434996486,
      "grad_norm": 104.07402038574219,
      "learning_rate": 9.042164441321152e-05,
      "loss": -7.4722,
      "step": 1370
    },
    {
      "epoch": 0.9697821503865074,
      "grad_norm": 79.9546890258789,
      "learning_rate": 9.035137034434295e-05,
      "loss": -7.3738,
      "step": 1380
    },
    {
      "epoch": 0.9768095572733662,
      "grad_norm": 157.92059326171875,
      "learning_rate": 9.028109627547435e-05,
      "loss": -7.5088,
      "step": 1390
    },
    {
      "epoch": 0.9838369641602249,
      "grad_norm": 197.66197204589844,
      "learning_rate": 9.021082220660577e-05,
      "loss": -7.5222,
      "step": 1400
    },
    {
      "epoch": 0.9908643710470836,
      "grad_norm": 66.03207397460938,
      "learning_rate": 9.014054813773719e-05,
      "loss": -7.4772,
      "step": 1410
    },
    {
      "epoch": 0.9978917779339423,
      "grad_norm": 52.583370208740234,
      "learning_rate": 9.007027406886859e-05,
      "loss": -7.5506,
      "step": 1420
    },
    {
      "epoch": 1.0,
      "eval_runtime": 10.4756,
      "eval_samples_per_second": 65428.547,
      "eval_steps_per_second": 16.037,
      "step": 1423
    },
    {
      "epoch": 1.0049191848208012,
      "grad_norm": 110.1646957397461,
      "learning_rate": 9e-05,
      "loss": -6.5827,
      "step": 1430
    },
    {
      "epoch": 1.0119465917076598,
      "grad_norm": 80.48126983642578,
      "learning_rate": 8.992972593113141e-05,
      "loss": -7.5213,
      "step": 1440
    },
    {
      "epoch": 1.0189739985945185,
      "grad_norm": 101.3356704711914,
      "learning_rate": 8.985945186226284e-05,
      "loss": -6.6798,
      "step": 1450
    },
    {
      "epoch": 1.0260014054813773,
      "grad_norm": 76.03248596191406,
      "learning_rate": 8.978917779339424e-05,
      "loss": -7.5439,
      "step": 1460
    },
    {
      "epoch": 1.033028812368236,
      "grad_norm": 44.563987731933594,
      "learning_rate": 8.971890372452564e-05,
      "loss": -7.5398,
      "step": 1470
    },
    {
      "epoch": 1.0400562192550948,
      "grad_norm": 46.822689056396484,
      "learning_rate": 8.964862965565707e-05,
      "loss": -7.6593,
      "step": 1480
    },
    {
      "epoch": 1.0470836261419536,
      "grad_norm": 111.41635131835938,
      "learning_rate": 8.957835558678848e-05,
      "loss": -7.4396,
      "step": 1490
    },
    {
      "epoch": 1.0541110330288124,
      "grad_norm": 74.81256103515625,
      "learning_rate": 8.95080815179199e-05,
      "loss": -7.4216,
      "step": 1500
    },
    {
      "epoch": 1.0611384399156711,
      "grad_norm": 253.42327880859375,
      "learning_rate": 8.94378074490513e-05,
      "loss": -7.4702,
      "step": 1510
    },
    {
      "epoch": 1.06816584680253,
      "grad_norm": 109.47811889648438,
      "learning_rate": 8.936753338018273e-05,
      "loss": -7.5352,
      "step": 1520
    },
    {
      "epoch": 1.0751932536893887,
      "grad_norm": 212.60894775390625,
      "learning_rate": 8.929725931131413e-05,
      "loss": -7.6768,
      "step": 1530
    },
    {
      "epoch": 1.0822206605762474,
      "grad_norm": 307.6495666503906,
      "learning_rate": 8.922698524244553e-05,
      "loss": -7.457,
      "step": 1540
    },
    {
      "epoch": 1.0892480674631062,
      "grad_norm": 94.30987548828125,
      "learning_rate": 8.915671117357696e-05,
      "loss": -7.6864,
      "step": 1550
    },
    {
      "epoch": 1.096275474349965,
      "grad_norm": 190.77357482910156,
      "learning_rate": 8.908643710470837e-05,
      "loss": -7.5173,
      "step": 1560
    },
    {
      "epoch": 1.1033028812368235,
      "grad_norm": 118.46442413330078,
      "learning_rate": 8.901616303583978e-05,
      "loss": -7.7033,
      "step": 1570
    },
    {
      "epoch": 1.1103302881236823,
      "grad_norm": 62.27548599243164,
      "learning_rate": 8.894588896697119e-05,
      "loss": -6.8829,
      "step": 1580
    },
    {
      "epoch": 1.117357695010541,
      "grad_norm": 306.39337158203125,
      "learning_rate": 8.88756148981026e-05,
      "loss": -7.4595,
      "step": 1590
    },
    {
      "epoch": 1.1243851018973998,
      "grad_norm": 169.19017028808594,
      "learning_rate": 8.880534082923402e-05,
      "loss": -7.7259,
      "step": 1600
    },
    {
      "epoch": 1.1314125087842586,
      "grad_norm": 127.43582916259766,
      "learning_rate": 8.873506676036542e-05,
      "loss": -7.6918,
      "step": 1610
    },
    {
      "epoch": 1.1384399156711174,
      "grad_norm": 34.56240463256836,
      "learning_rate": 8.866479269149685e-05,
      "loss": -7.7454,
      "step": 1620
    },
    {
      "epoch": 1.1454673225579761,
      "grad_norm": 67.09613800048828,
      "learning_rate": 8.859451862262825e-05,
      "loss": -7.6953,
      "step": 1630
    },
    {
      "epoch": 1.1524947294448349,
      "grad_norm": 415.13470458984375,
      "learning_rate": 8.852424455375967e-05,
      "loss": -6.8616,
      "step": 1640
    },
    {
      "epoch": 1.1595221363316937,
      "grad_norm": 242.69825744628906,
      "learning_rate": 8.845397048489107e-05,
      "loss": -7.7246,
      "step": 1650
    },
    {
      "epoch": 1.1665495432185524,
      "grad_norm": 437.3819274902344,
      "learning_rate": 8.838369641602249e-05,
      "loss": -7.6091,
      "step": 1660
    },
    {
      "epoch": 1.1735769501054112,
      "grad_norm": 114.82805633544922,
      "learning_rate": 8.83134223471539e-05,
      "loss": -7.7485,
      "step": 1670
    },
    {
      "epoch": 1.1806043569922697,
      "grad_norm": 200.8641815185547,
      "learning_rate": 8.824314827828531e-05,
      "loss": -7.7532,
      "step": 1680
    },
    {
      "epoch": 1.1876317638791285,
      "grad_norm": 210.88645935058594,
      "learning_rate": 8.817287420941674e-05,
      "loss": -7.6673,
      "step": 1690
    },
    {
      "epoch": 1.1946591707659873,
      "grad_norm": 147.242431640625,
      "learning_rate": 8.810260014054814e-05,
      "loss": -7.6826,
      "step": 1700
    },
    {
      "epoch": 1.201686577652846,
      "grad_norm": 45.30035400390625,
      "learning_rate": 8.803232607167955e-05,
      "loss": -7.6831,
      "step": 1710
    },
    {
      "epoch": 1.2087139845397048,
      "grad_norm": 375.5536193847656,
      "learning_rate": 8.796205200281098e-05,
      "loss": -7.6217,
      "step": 1720
    },
    {
      "epoch": 1.2157413914265636,
      "grad_norm": 149.2346649169922,
      "learning_rate": 8.789177793394238e-05,
      "loss": -7.488,
      "step": 1730
    },
    {
      "epoch": 1.2227687983134223,
      "grad_norm": 39.424407958984375,
      "learning_rate": 8.78215038650738e-05,
      "loss": -7.7215,
      "step": 1740
    },
    {
      "epoch": 1.229796205200281,
      "grad_norm": 138.93104553222656,
      "learning_rate": 8.77512297962052e-05,
      "loss": -7.6875,
      "step": 1750
    },
    {
      "epoch": 1.2368236120871399,
      "grad_norm": 201.68557739257812,
      "learning_rate": 8.768095572733663e-05,
      "loss": -7.8683,
      "step": 1760
    },
    {
      "epoch": 1.2438510189739986,
      "grad_norm": 91.5733642578125,
      "learning_rate": 8.761068165846803e-05,
      "loss": -7.8843,
      "step": 1770
    },
    {
      "epoch": 1.2508784258608574,
      "grad_norm": 128.1421356201172,
      "learning_rate": 8.754040758959943e-05,
      "loss": -7.0119,
      "step": 1780
    },
    {
      "epoch": 1.2579058327477162,
      "grad_norm": 216.029052734375,
      "learning_rate": 8.747013352073086e-05,
      "loss": -6.133,
      "step": 1790
    },
    {
      "epoch": 1.264933239634575,
      "grad_norm": 132.05491638183594,
      "learning_rate": 8.739985945186227e-05,
      "loss": -7.9004,
      "step": 1800
    },
    {
      "epoch": 1.2719606465214337,
      "grad_norm": 125.29669189453125,
      "learning_rate": 8.732958538299368e-05,
      "loss": -7.7738,
      "step": 1810
    },
    {
      "epoch": 1.2789880534082925,
      "grad_norm": 135.6445770263672,
      "learning_rate": 8.725931131412509e-05,
      "loss": -7.7767,
      "step": 1820
    },
    {
      "epoch": 1.286015460295151,
      "grad_norm": 87.72846221923828,
      "learning_rate": 8.71890372452565e-05,
      "loss": -7.8891,
      "step": 1830
    },
    {
      "epoch": 1.2930428671820098,
      "grad_norm": 12.66505241394043,
      "learning_rate": 8.711876317638792e-05,
      "loss": -7.764,
      "step": 1840
    },
    {
      "epoch": 1.3000702740688685,
      "grad_norm": 241.4489288330078,
      "learning_rate": 8.704848910751932e-05,
      "loss": -7.8857,
      "step": 1850
    },
    {
      "epoch": 1.3070976809557273,
      "grad_norm": 185.24317932128906,
      "learning_rate": 8.697821503865075e-05,
      "loss": -7.7664,
      "step": 1860
    },
    {
      "epoch": 1.314125087842586,
      "grad_norm": 218.0685272216797,
      "learning_rate": 8.690794096978215e-05,
      "loss": -7.9495,
      "step": 1870
    },
    {
      "epoch": 1.3211524947294448,
      "grad_norm": 24.654024124145508,
      "learning_rate": 8.683766690091357e-05,
      "loss": -7.825,
      "step": 1880
    },
    {
      "epoch": 1.3281799016163036,
      "grad_norm": 203.9728546142578,
      "learning_rate": 8.676739283204497e-05,
      "loss": -7.6977,
      "step": 1890
    },
    {
      "epoch": 1.3352073085031624,
      "grad_norm": 60.6337890625,
      "learning_rate": 8.669711876317639e-05,
      "loss": -7.8872,
      "step": 1900
    },
    {
      "epoch": 1.3422347153900211,
      "grad_norm": 311.719482421875,
      "learning_rate": 8.662684469430781e-05,
      "loss": -7.8994,
      "step": 1910
    },
    {
      "epoch": 1.3492621222768797,
      "grad_norm": 333.6035461425781,
      "learning_rate": 8.655657062543921e-05,
      "loss": -7.1058,
      "step": 1920
    },
    {
      "epoch": 1.3562895291637385,
      "grad_norm": 130.58074951171875,
      "learning_rate": 8.648629655657064e-05,
      "loss": -7.8661,
      "step": 1930
    },
    {
      "epoch": 1.3633169360505972,
      "grad_norm": 222.85052490234375,
      "learning_rate": 8.641602248770204e-05,
      "loss": -7.8809,
      "step": 1940
    },
    {
      "epoch": 1.370344342937456,
      "grad_norm": 129.05902099609375,
      "learning_rate": 8.634574841883345e-05,
      "loss": -7.9062,
      "step": 1950
    },
    {
      "epoch": 1.3773717498243148,
      "grad_norm": 105.25391387939453,
      "learning_rate": 8.627547434996486e-05,
      "loss": -7.8071,
      "step": 1960
    },
    {
      "epoch": 1.3843991567111735,
      "grad_norm": 307.2703552246094,
      "learning_rate": 8.620520028109628e-05,
      "loss": -7.9027,
      "step": 1970
    },
    {
      "epoch": 1.3914265635980323,
      "grad_norm": 62.729393005371094,
      "learning_rate": 8.61349262122277e-05,
      "loss": -7.9062,
      "step": 1980
    },
    {
      "epoch": 1.398453970484891,
      "grad_norm": 116.59447479248047,
      "learning_rate": 8.60646521433591e-05,
      "loss": -7.7431,
      "step": 1990
    },
    {
      "epoch": 1.4054813773717498,
      "grad_norm": 292.3125,
      "learning_rate": 8.599437807449053e-05,
      "loss": -7.9295,
      "step": 2000
    },
    {
      "epoch": 1.4125087842586086,
      "grad_norm": 250.04989624023438,
      "learning_rate": 8.592410400562193e-05,
      "loss": -8.0243,
      "step": 2010
    },
    {
      "epoch": 1.4195361911454674,
      "grad_norm": 105.00770568847656,
      "learning_rate": 8.585382993675333e-05,
      "loss": -7.8226,
      "step": 2020
    },
    {
      "epoch": 1.4265635980323261,
      "grad_norm": 21.951568603515625,
      "learning_rate": 8.578355586788476e-05,
      "loss": -7.9054,
      "step": 2030
    },
    {
      "epoch": 1.433591004919185,
      "grad_norm": 50.0550537109375,
      "learning_rate": 8.571328179901617e-05,
      "loss": -7.9653,
      "step": 2040
    },
    {
      "epoch": 1.4406184118060437,
      "grad_norm": 133.7836456298828,
      "learning_rate": 8.564300773014758e-05,
      "loss": -8.0536,
      "step": 2050
    },
    {
      "epoch": 1.4476458186929024,
      "grad_norm": Infinity,
      "learning_rate": 8.557976106816585e-05,
      "loss": -7.1716,
      "step": 2060
    },
    {
      "epoch": 1.4546732255797612,
      "grad_norm": 176.17286682128906,
      "learning_rate": 8.550948699929726e-05,
      "loss": -8.0077,
      "step": 2070
    },
    {
      "epoch": 1.4617006324666197,
      "grad_norm": 159.12123107910156,
      "learning_rate": 8.543921293042868e-05,
      "loss": -8.056,
      "step": 2080
    },
    {
      "epoch": 1.4687280393534785,
      "grad_norm": 191.763671875,
      "learning_rate": 8.536893886156008e-05,
      "loss": -8.0518,
      "step": 2090
    },
    {
      "epoch": 1.4757554462403373,
      "grad_norm": 108.61103057861328,
      "learning_rate": 8.52986647926915e-05,
      "loss": -8.0676,
      "step": 2100
    },
    {
      "epoch": 1.482782853127196,
      "grad_norm": 111.61298370361328,
      "learning_rate": 8.522839072382292e-05,
      "loss": -7.9058,
      "step": 2110
    },
    {
      "epoch": 1.4898102600140548,
      "grad_norm": 250.8838348388672,
      "learning_rate": 8.515811665495432e-05,
      "loss": -8.0135,
      "step": 2120
    },
    {
      "epoch": 1.4968376669009136,
      "grad_norm": 82.99273681640625,
      "learning_rate": 8.508784258608574e-05,
      "loss": -8.1104,
      "step": 2130
    },
    {
      "epoch": 1.5038650737877723,
      "grad_norm": 118.11593627929688,
      "learning_rate": 8.501756851721715e-05,
      "loss": -7.9839,
      "step": 2140
    },
    {
      "epoch": 1.510892480674631,
      "grad_norm": 357.0450744628906,
      "learning_rate": 8.494729444834857e-05,
      "loss": -7.9512,
      "step": 2150
    },
    {
      "epoch": 1.5179198875614897,
      "grad_norm": 152.9487762451172,
      "learning_rate": 8.487702037947997e-05,
      "loss": -7.9291,
      "step": 2160
    },
    {
      "epoch": 1.5249472944483484,
      "grad_norm": 103.45035552978516,
      "learning_rate": 8.480674631061139e-05,
      "loss": -8.0236,
      "step": 2170
    },
    {
      "epoch": 1.5319747013352072,
      "grad_norm": 271.2438659667969,
      "learning_rate": 8.47364722417428e-05,
      "loss": -8.0018,
      "step": 2180
    },
    {
      "epoch": 1.539002108222066,
      "grad_norm": 270.0941467285156,
      "learning_rate": 8.466619817287421e-05,
      "loss": -7.2476,
      "step": 2190
    },
    {
      "epoch": 1.5460295151089247,
      "grad_norm": 235.96742248535156,
      "learning_rate": 8.459592410400562e-05,
      "loss": -7.9856,
      "step": 2200
    },
    {
      "epoch": 1.5530569219957835,
      "grad_norm": 280.56396484375,
      "learning_rate": 8.452565003513704e-05,
      "loss": -8.0417,
      "step": 2210
    },
    {
      "epoch": 1.5600843288826423,
      "grad_norm": 100.65755462646484,
      "learning_rate": 8.445537596626844e-05,
      "loss": -7.9256,
      "step": 2220
    },
    {
      "epoch": 1.567111735769501,
      "grad_norm": 220.64414978027344,
      "learning_rate": 8.438510189739986e-05,
      "loss": -8.1307,
      "step": 2230
    },
    {
      "epoch": 1.5741391426563598,
      "grad_norm": 175.27894592285156,
      "learning_rate": 8.431482782853128e-05,
      "loss": -7.9889,
      "step": 2240
    },
    {
      "epoch": 1.5811665495432186,
      "grad_norm": 317.344970703125,
      "learning_rate": 8.424455375966269e-05,
      "loss": -8.0626,
      "step": 2250
    },
    {
      "epoch": 1.5881939564300773,
      "grad_norm": 207.9340362548828,
      "learning_rate": 8.41742796907941e-05,
      "loss": -7.9755,
      "step": 2260
    },
    {
      "epoch": 1.595221363316936,
      "grad_norm": 198.05422973632812,
      "learning_rate": 8.410400562192553e-05,
      "loss": -8.1633,
      "step": 2270
    },
    {
      "epoch": 1.6022487702037949,
      "grad_norm": 65.14700317382812,
      "learning_rate": 8.403373155305693e-05,
      "loss": -8.0752,
      "step": 2280
    },
    {
      "epoch": 1.6092761770906536,
      "grad_norm": 454.77374267578125,
      "learning_rate": 8.396345748418833e-05,
      "loss": -8.0676,
      "step": 2290
    },
    {
      "epoch": 1.6163035839775124,
      "grad_norm": 200.0966796875,
      "learning_rate": 8.389318341531975e-05,
      "loss": -8.0332,
      "step": 2300
    },
    {
      "epoch": 1.6233309908643712,
      "grad_norm": 103.19322204589844,
      "learning_rate": 8.382290934645116e-05,
      "loss": -8.122,
      "step": 2310
    },
    {
      "epoch": 1.63035839775123,
      "grad_norm": 149.4123992919922,
      "learning_rate": 8.375263527758258e-05,
      "loss": -8.1029,
      "step": 2320
    },
    {
      "epoch": 1.6373858046380887,
      "grad_norm": 103.67337036132812,
      "learning_rate": 8.368236120871398e-05,
      "loss": -8.0326,
      "step": 2330
    },
    {
      "epoch": 1.6444132115249475,
      "grad_norm": 185.35316467285156,
      "learning_rate": 8.36120871398454e-05,
      "loss": -8.0803,
      "step": 2340
    },
    {
      "epoch": 1.651440618411806,
      "grad_norm": 197.25575256347656,
      "learning_rate": 8.354181307097682e-05,
      "loss": -8.1533,
      "step": 2350
    },
    {
      "epoch": 1.6584680252986648,
      "grad_norm": 249.06040954589844,
      "learning_rate": 8.347153900210822e-05,
      "loss": -8.1724,
      "step": 2360
    },
    {
      "epoch": 1.6654954321855235,
      "grad_norm": 333.2807312011719,
      "learning_rate": 8.340126493323964e-05,
      "loss": -7.5979,
      "step": 2370
    },
    {
      "epoch": 1.6725228390723823,
      "grad_norm": 145.76206970214844,
      "learning_rate": 8.333099086437105e-05,
      "loss": -8.2499,
      "step": 2380
    },
    {
      "epoch": 1.679550245959241,
      "grad_norm": 187.86170959472656,
      "learning_rate": 8.326071679550246e-05,
      "loss": -8.1486,
      "step": 2390
    },
    {
      "epoch": 1.6865776528460998,
      "grad_norm": 167.01622009277344,
      "learning_rate": 8.319044272663387e-05,
      "loss": -8.0498,
      "step": 2400
    },
    {
      "epoch": 1.6936050597329584,
      "grad_norm": 177.0873260498047,
      "learning_rate": 8.312016865776529e-05,
      "loss": -8.1046,
      "step": 2410
    },
    {
      "epoch": 1.7006324666198172,
      "grad_norm": 198.8045654296875,
      "learning_rate": 8.30498945888967e-05,
      "loss": -8.1156,
      "step": 2420
    },
    {
      "epoch": 1.707659873506676,
      "grad_norm": 151.84422302246094,
      "learning_rate": 8.297962052002811e-05,
      "loss": -8.1786,
      "step": 2430
    },
    {
      "epoch": 1.7146872803935347,
      "grad_norm": 159.3452606201172,
      "learning_rate": 8.290934645115952e-05,
      "loss": -8.1629,
      "step": 2440
    },
    {
      "epoch": 1.7217146872803935,
      "grad_norm": 142.29541015625,
      "learning_rate": 8.283907238229094e-05,
      "loss": -8.1284,
      "step": 2450
    },
    {
      "epoch": 1.7287420941672522,
      "grad_norm": 160.29884338378906,
      "learning_rate": 8.276879831342234e-05,
      "loss": -8.2265,
      "step": 2460
    },
    {
      "epoch": 1.735769501054111,
      "grad_norm": 27.566295623779297,
      "learning_rate": 8.269852424455376e-05,
      "loss": -8.177,
      "step": 2470
    },
    {
      "epoch": 1.7427969079409698,
      "grad_norm": 148.38868713378906,
      "learning_rate": 8.262825017568518e-05,
      "loss": -8.1977,
      "step": 2480
    },
    {
      "epoch": 1.7498243148278285,
      "grad_norm": 146.3419647216797,
      "learning_rate": 8.255797610681659e-05,
      "loss": -8.1051,
      "step": 2490
    },
    {
      "epoch": 1.7568517217146873,
      "grad_norm": 127.4921646118164,
      "learning_rate": 8.2487702037948e-05,
      "loss": -7.8588,
      "step": 2500
    },
    {
      "epoch": 1.763879128601546,
      "grad_norm": 270.7547607421875,
      "learning_rate": 8.241742796907941e-05,
      "loss": -8.2274,
      "step": 2510
    },
    {
      "epoch": 1.7709065354884048,
      "grad_norm": 92.5652084350586,
      "learning_rate": 8.234715390021083e-05,
      "loss": -8.1875,
      "step": 2520
    },
    {
      "epoch": 1.7779339423752636,
      "grad_norm": 108.85939025878906,
      "learning_rate": 8.227687983134223e-05,
      "loss": -8.2365,
      "step": 2530
    },
    {
      "epoch": 1.7849613492621224,
      "grad_norm": 136.16384887695312,
      "learning_rate": 8.220660576247365e-05,
      "loss": -8.1736,
      "step": 2540
    },
    {
      "epoch": 1.7919887561489811,
      "grad_norm": 174.91600036621094,
      "learning_rate": 8.213633169360506e-05,
      "loss": -8.273,
      "step": 2550
    },
    {
      "epoch": 1.7990161630358399,
      "grad_norm": 190.63568115234375,
      "learning_rate": 8.206605762473648e-05,
      "loss": -8.2195,
      "step": 2560
    },
    {
      "epoch": 1.8060435699226987,
      "grad_norm": 118.7540054321289,
      "learning_rate": 8.199578355586788e-05,
      "loss": -8.3201,
      "step": 2570
    },
    {
      "epoch": 1.8130709768095574,
      "grad_norm": 325.76190185546875,
      "learning_rate": 8.19255094869993e-05,
      "loss": -8.2348,
      "step": 2580
    },
    {
      "epoch": 1.8200983836964162,
      "grad_norm": 212.48995971679688,
      "learning_rate": 8.185523541813072e-05,
      "loss": -8.3127,
      "step": 2590
    },
    {
      "epoch": 1.8271257905832747,
      "grad_norm": 73.72135162353516,
      "learning_rate": 8.178496134926212e-05,
      "loss": -7.4182,
      "step": 2600
    },
    {
      "epoch": 1.8341531974701335,
      "grad_norm": 67.7585220336914,
      "learning_rate": 8.171468728039354e-05,
      "loss": -8.1626,
      "step": 2610
    },
    {
      "epoch": 1.8411806043569923,
      "grad_norm": 161.43724060058594,
      "learning_rate": 8.164441321152495e-05,
      "loss": -8.2256,
      "step": 2620
    },
    {
      "epoch": 1.848208011243851,
      "grad_norm": 324.5807800292969,
      "learning_rate": 8.157413914265636e-05,
      "loss": -8.2551,
      "step": 2630
    },
    {
      "epoch": 1.8552354181307098,
      "grad_norm": 104.27599334716797,
      "learning_rate": 8.150386507378777e-05,
      "loss": -8.3348,
      "step": 2640
    },
    {
      "epoch": 1.8622628250175686,
      "grad_norm": 75.72303771972656,
      "learning_rate": 8.143359100491919e-05,
      "loss": -7.4876,
      "step": 2650
    },
    {
      "epoch": 1.8692902319044271,
      "grad_norm": 136.56875610351562,
      "learning_rate": 8.13633169360506e-05,
      "loss": -8.4072,
      "step": 2660
    },
    {
      "epoch": 1.8763176387912859,
      "grad_norm": 86.91016387939453,
      "learning_rate": 8.129304286718201e-05,
      "loss": -7.5135,
      "step": 2670
    },
    {
      "epoch": 1.8833450456781446,
      "grad_norm": 79.09539794921875,
      "learning_rate": 8.122276879831342e-05,
      "loss": -8.4511,
      "step": 2680
    },
    {
      "epoch": 1.8903724525650034,
      "grad_norm": 219.55218505859375,
      "learning_rate": 8.115249472944484e-05,
      "loss": -8.3619,
      "step": 2690
    },
    {
      "epoch": 1.8973998594518622,
      "grad_norm": 65.51472473144531,
      "learning_rate": 8.108222066057624e-05,
      "loss": -8.3804,
      "step": 2700
    },
    {
      "epoch": 1.904427266338721,
      "grad_norm": 125.93338775634766,
      "learning_rate": 8.101194659170766e-05,
      "loss": -8.3732,
      "step": 2710
    },
    {
      "epoch": 1.9114546732255797,
      "grad_norm": 330.94012451171875,
      "learning_rate": 8.094167252283908e-05,
      "loss": -8.3931,
      "step": 2720
    },
    {
      "epoch": 1.9184820801124385,
      "grad_norm": 290.2658996582031,
      "learning_rate": 8.08713984539705e-05,
      "loss": -8.3797,
      "step": 2730
    },
    {
      "epoch": 1.9255094869992972,
      "grad_norm": 207.47683715820312,
      "learning_rate": 8.08011243851019e-05,
      "loss": -8.2656,
      "step": 2740
    },
    {
      "epoch": 1.932536893886156,
      "grad_norm": 216.8863067626953,
      "learning_rate": 8.073085031623331e-05,
      "loss": -7.5076,
      "step": 2750
    },
    {
      "epoch": 1.9395643007730148,
      "grad_norm": 91.2572250366211,
      "learning_rate": 8.066057624736473e-05,
      "loss": -7.6686,
      "step": 2760
    },
    {
      "epoch": 1.9465917076598735,
      "grad_norm": 205.54518127441406,
      "learning_rate": 8.059030217849613e-05,
      "loss": -8.3054,
      "step": 2770
    },
    {
      "epoch": 1.9536191145467323,
      "grad_norm": 84.35154724121094,
      "learning_rate": 8.052002810962755e-05,
      "loss": -8.365,
      "step": 2780
    },
    {
      "epoch": 1.960646521433591,
      "grad_norm": 114.12862396240234,
      "learning_rate": 8.044975404075897e-05,
      "loss": -8.4789,
      "step": 2790
    },
    {
      "epoch": 1.9676739283204498,
      "grad_norm": 294.4913635253906,
      "learning_rate": 8.037947997189038e-05,
      "loss": -8.3305,
      "step": 2800
    },
    {
      "epoch": 1.9747013352073086,
      "grad_norm": 160.31134033203125,
      "learning_rate": 8.030920590302178e-05,
      "loss": -7.5235,
      "step": 2810
    },
    {
      "epoch": 1.9817287420941674,
      "grad_norm": 198.88284301757812,
      "learning_rate": 8.02389318341532e-05,
      "loss": -8.4697,
      "step": 2820
    },
    {
      "epoch": 1.9887561489810262,
      "grad_norm": 263.5756530761719,
      "learning_rate": 8.016865776528462e-05,
      "loss": -8.3798,
      "step": 2830
    },
    {
      "epoch": 1.9957835558678847,
      "grad_norm": 387.53594970703125,
      "learning_rate": 8.009838369641602e-05,
      "loss": -7.7596,
      "step": 2840
    },
    {
      "epoch": 2.0,
      "eval_runtime": 10.7188,
      "eval_samples_per_second": 63944.128,
      "eval_steps_per_second": 15.673,
      "step": 2846
    },
    {
      "epoch": 2.0028109627547437,
      "grad_norm": 225.02691650390625,
      "learning_rate": 8.002810962754744e-05,
      "loss": -8.4811,
      "step": 2850
    },
    {
      "epoch": 2.0098383696416025,
      "grad_norm": 110.37256622314453,
      "learning_rate": 7.995783555867885e-05,
      "loss": -8.3994,
      "step": 2860
    },
    {
      "epoch": 2.016865776528461,
      "grad_norm": 125.79314422607422,
      "learning_rate": 7.988756148981026e-05,
      "loss": -8.2953,
      "step": 2870
    },
    {
      "epoch": 2.0238931834153195,
      "grad_norm": 169.89019775390625,
      "learning_rate": 7.981728742094167e-05,
      "loss": -7.7113,
      "step": 2880
    },
    {
      "epoch": 2.0309205903021783,
      "grad_norm": 166.33570861816406,
      "learning_rate": 7.974701335207309e-05,
      "loss": -8.4608,
      "step": 2890
    },
    {
      "epoch": 2.037947997189037,
      "grad_norm": 164.51760864257812,
      "learning_rate": 7.96767392832045e-05,
      "loss": -8.3889,
      "step": 2900
    },
    {
      "epoch": 2.044975404075896,
      "grad_norm": 162.68081665039062,
      "learning_rate": 7.960646521433591e-05,
      "loss": -8.5235,
      "step": 2910
    },
    {
      "epoch": 2.0520028109627546,
      "grad_norm": 173.57479858398438,
      "learning_rate": 7.953619114546733e-05,
      "loss": -8.3284,
      "step": 2920
    },
    {
      "epoch": 2.0590302178496134,
      "grad_norm": 103.26721954345703,
      "learning_rate": 7.946591707659874e-05,
      "loss": -8.5108,
      "step": 2930
    },
    {
      "epoch": 2.066057624736472,
      "grad_norm": 94.17555236816406,
      "learning_rate": 7.939564300773014e-05,
      "loss": -8.4024,
      "step": 2940
    },
    {
      "epoch": 2.073085031623331,
      "grad_norm": 157.7161102294922,
      "learning_rate": 7.932536893886156e-05,
      "loss": -8.2318,
      "step": 2950
    },
    {
      "epoch": 2.0801124385101897,
      "grad_norm": 291.6960754394531,
      "learning_rate": 7.925509486999298e-05,
      "loss": -7.595,
      "step": 2960
    },
    {
      "epoch": 2.0871398453970484,
      "grad_norm": 309.52581787109375,
      "learning_rate": 7.91848208011244e-05,
      "loss": -8.386,
      "step": 2970
    },
    {
      "epoch": 2.094167252283907,
      "grad_norm": 203.21226501464844,
      "learning_rate": 7.91145467322558e-05,
      "loss": -8.3198,
      "step": 2980
    },
    {
      "epoch": 2.101194659170766,
      "grad_norm": 420.1835632324219,
      "learning_rate": 7.904427266338721e-05,
      "loss": -8.4286,
      "step": 2990
    },
    {
      "epoch": 2.1082220660576247,
      "grad_norm": 115.77474975585938,
      "learning_rate": 7.897399859451863e-05,
      "loss": -8.505,
      "step": 3000
    },
    {
      "epoch": 2.1152494729444835,
      "grad_norm": 99.93772888183594,
      "learning_rate": 7.890372452565003e-05,
      "loss": -7.8865,
      "step": 3010
    },
    {
      "epoch": 2.1222768798313423,
      "grad_norm": 200.03050231933594,
      "learning_rate": 7.883345045678145e-05,
      "loss": -8.4979,
      "step": 3020
    },
    {
      "epoch": 2.129304286718201,
      "grad_norm": 287.7288818359375,
      "learning_rate": 7.876317638791287e-05,
      "loss": -8.3979,
      "step": 3030
    },
    {
      "epoch": 2.13633169360506,
      "grad_norm": 143.4256591796875,
      "learning_rate": 7.869290231904428e-05,
      "loss": -8.5013,
      "step": 3040
    },
    {
      "epoch": 2.1433591004919186,
      "grad_norm": 135.6341094970703,
      "learning_rate": 7.862262825017569e-05,
      "loss": -8.5607,
      "step": 3050
    },
    {
      "epoch": 2.1503865073787773,
      "grad_norm": 54.66648864746094,
      "learning_rate": 7.85523541813071e-05,
      "loss": -8.4154,
      "step": 3060
    },
    {
      "epoch": 2.157413914265636,
      "grad_norm": 51.61063003540039,
      "learning_rate": 7.848208011243852e-05,
      "loss": -7.9824,
      "step": 3070
    },
    {
      "epoch": 2.164441321152495,
      "grad_norm": 126.29732513427734,
      "learning_rate": 7.841180604356992e-05,
      "loss": -8.4601,
      "step": 3080
    },
    {
      "epoch": 2.1714687280393536,
      "grad_norm": 149.10711669921875,
      "learning_rate": 7.834153197470134e-05,
      "loss": -8.5592,
      "step": 3090
    },
    {
      "epoch": 2.1784961349262124,
      "grad_norm": 124.78160858154297,
      "learning_rate": 7.827125790583275e-05,
      "loss": -8.5193,
      "step": 3100
    },
    {
      "epoch": 2.185523541813071,
      "grad_norm": 14.085470199584961,
      "learning_rate": 7.820098383696416e-05,
      "loss": -7.9478,
      "step": 3110
    },
    {
      "epoch": 2.19255094869993,
      "grad_norm": 115.70513916015625,
      "learning_rate": 7.813070976809557e-05,
      "loss": -8.5576,
      "step": 3120
    },
    {
      "epoch": 2.1995783555867883,
      "grad_norm": 116.06289672851562,
      "learning_rate": 7.806043569922699e-05,
      "loss": -8.3945,
      "step": 3130
    },
    {
      "epoch": 2.206605762473647,
      "grad_norm": 109.96200561523438,
      "learning_rate": 7.79901616303584e-05,
      "loss": -8.5986,
      "step": 3140
    },
    {
      "epoch": 2.213633169360506,
      "grad_norm": 235.173828125,
      "learning_rate": 7.791988756148981e-05,
      "loss": -8.5781,
      "step": 3150
    },
    {
      "epoch": 2.2206605762473646,
      "grad_norm": 257.8872375488281,
      "learning_rate": 7.784961349262123e-05,
      "loss": -8.5678,
      "step": 3160
    },
    {
      "epoch": 2.2276879831342233,
      "grad_norm": 341.38323974609375,
      "learning_rate": 7.777933942375264e-05,
      "loss": -8.0313,
      "step": 3170
    },
    {
      "epoch": 2.234715390021082,
      "grad_norm": 244.72068786621094,
      "learning_rate": 7.770906535488405e-05,
      "loss": -7.9912,
      "step": 3180
    },
    {
      "epoch": 2.241742796907941,
      "grad_norm": 82.59126281738281,
      "learning_rate": 7.763879128601546e-05,
      "loss": -8.5919,
      "step": 3190
    },
    {
      "epoch": 2.2487702037947996,
      "grad_norm": 120.18185424804688,
      "learning_rate": 7.756851721714688e-05,
      "loss": -8.474,
      "step": 3200
    },
    {
      "epoch": 2.2557976106816584,
      "grad_norm": 490.7081604003906,
      "learning_rate": 7.74982431482783e-05,
      "loss": -8.4963,
      "step": 3210
    },
    {
      "epoch": 2.262825017568517,
      "grad_norm": 315.64166259765625,
      "learning_rate": 7.74279690794097e-05,
      "loss": -8.5965,
      "step": 3220
    },
    {
      "epoch": 2.269852424455376,
      "grad_norm": 86.918701171875,
      "learning_rate": 7.735769501054111e-05,
      "loss": -8.6432,
      "step": 3230
    },
    {
      "epoch": 2.2768798313422347,
      "grad_norm": 231.6716766357422,
      "learning_rate": 7.728742094167253e-05,
      "loss": -8.685,
      "step": 3240
    },
    {
      "epoch": 2.2839072382290935,
      "grad_norm": 46.87607192993164,
      "learning_rate": 7.721714687280393e-05,
      "loss": -8.6976,
      "step": 3250
    },
    {
      "epoch": 2.2909346451159522,
      "grad_norm": 123.5180892944336,
      "learning_rate": 7.714687280393535e-05,
      "loss": -8.611,
      "step": 3260
    },
    {
      "epoch": 2.297962052002811,
      "grad_norm": 228.0556640625,
      "learning_rate": 7.707659873506677e-05,
      "loss": -8.6457,
      "step": 3270
    },
    {
      "epoch": 2.3049894588896698,
      "grad_norm": 184.5399627685547,
      "learning_rate": 7.700632466619818e-05,
      "loss": -8.6362,
      "step": 3280
    },
    {
      "epoch": 2.3120168657765285,
      "grad_norm": 252.17877197265625,
      "learning_rate": 7.693605059732959e-05,
      "loss": -8.6744,
      "step": 3290
    },
    {
      "epoch": 2.3190442726633873,
      "grad_norm": 164.254150390625,
      "learning_rate": 7.6865776528461e-05,
      "loss": -8.6135,
      "step": 3300
    },
    {
      "epoch": 2.326071679550246,
      "grad_norm": 134.58421325683594,
      "learning_rate": 7.679550245959242e-05,
      "loss": -8.5522,
      "step": 3310
    },
    {
      "epoch": 2.333099086437105,
      "grad_norm": 165.8588104248047,
      "learning_rate": 7.672522839072382e-05,
      "loss": -8.6254,
      "step": 3320
    },
    {
      "epoch": 2.3401264933239636,
      "grad_norm": 215.5231475830078,
      "learning_rate": 7.665495432185524e-05,
      "loss": -8.648,
      "step": 3330
    },
    {
      "epoch": 2.3471539002108224,
      "grad_norm": 87.52716064453125,
      "learning_rate": 7.658468025298665e-05,
      "loss": -8.7261,
      "step": 3340
    },
    {
      "epoch": 2.354181307097681,
      "grad_norm": 224.54930114746094,
      "learning_rate": 7.651440618411806e-05,
      "loss": -8.6498,
      "step": 3350
    },
    {
      "epoch": 2.3612087139845395,
      "grad_norm": 261.2884521484375,
      "learning_rate": 7.644413211524947e-05,
      "loss": -8.5963,
      "step": 3360
    },
    {
      "epoch": 2.3682361208713987,
      "grad_norm": 151.74978637695312,
      "learning_rate": 7.637385804638089e-05,
      "loss": -8.689,
      "step": 3370
    },
    {
      "epoch": 2.375263527758257,
      "grad_norm": 204.74972534179688,
      "learning_rate": 7.630358397751231e-05,
      "loss": -8.5878,
      "step": 3380
    },
    {
      "epoch": 2.3822909346451158,
      "grad_norm": 258.35174560546875,
      "learning_rate": 7.623330990864371e-05,
      "loss": -8.6177,
      "step": 3390
    },
    {
      "epoch": 2.3893183415319745,
      "grad_norm": 113.83328247070312,
      "learning_rate": 7.616303583977513e-05,
      "loss": -8.6938,
      "step": 3400
    },
    {
      "epoch": 2.3963457484188333,
      "grad_norm": 61.120479583740234,
      "learning_rate": 7.609276177090654e-05,
      "loss": -8.7091,
      "step": 3410
    },
    {
      "epoch": 2.403373155305692,
      "grad_norm": 87.94649505615234,
      "learning_rate": 7.602248770203795e-05,
      "loss": -8.6946,
      "step": 3420
    },
    {
      "epoch": 2.410400562192551,
      "grad_norm": 689.940673828125,
      "learning_rate": 7.595221363316936e-05,
      "loss": -8.681,
      "step": 3430
    },
    {
      "epoch": 2.4174279690794096,
      "grad_norm": 91.55699920654297,
      "learning_rate": 7.588193956430078e-05,
      "loss": -8.6879,
      "step": 3440
    },
    {
      "epoch": 2.4244553759662684,
      "grad_norm": 217.81845092773438,
      "learning_rate": 7.58116654954322e-05,
      "loss": -8.7641,
      "step": 3450
    },
    {
      "epoch": 2.431482782853127,
      "grad_norm": 260.87103271484375,
      "learning_rate": 7.57413914265636e-05,
      "loss": -8.8486,
      "step": 3460
    },
    {
      "epoch": 2.438510189739986,
      "grad_norm": 148.4253387451172,
      "learning_rate": 7.567111735769501e-05,
      "loss": -8.7479,
      "step": 3470
    },
    {
      "epoch": 2.4455375966268447,
      "grad_norm": 169.07968139648438,
      "learning_rate": 7.560084328882643e-05,
      "loss": -8.7749,
      "step": 3480
    },
    {
      "epoch": 2.4525650035137034,
      "grad_norm": 103.07903289794922,
      "learning_rate": 7.553056921995783e-05,
      "loss": -8.1678,
      "step": 3490
    },
    {
      "epoch": 2.459592410400562,
      "grad_norm": 42.290470123291016,
      "learning_rate": 7.546029515108925e-05,
      "loss": -8.8987,
      "step": 3500
    },
    {
      "epoch": 2.466619817287421,
      "grad_norm": 165.82789611816406,
      "learning_rate": 7.539002108222067e-05,
      "loss": -8.6965,
      "step": 3510
    },
    {
      "epoch": 2.4736472241742797,
      "grad_norm": 277.54132080078125,
      "learning_rate": 7.531974701335208e-05,
      "loss": -8.7299,
      "step": 3520
    },
    {
      "epoch": 2.4806746310611385,
      "grad_norm": 125.41350555419922,
      "learning_rate": 7.524947294448349e-05,
      "loss": -8.8253,
      "step": 3530
    },
    {
      "epoch": 2.4877020379479973,
      "grad_norm": 170.8343505859375,
      "learning_rate": 7.51791988756149e-05,
      "loss": -8.8679,
      "step": 3540
    },
    {
      "epoch": 2.494729444834856,
      "grad_norm": 58.62925720214844,
      "learning_rate": 7.510892480674632e-05,
      "loss": -8.8178,
      "step": 3550
    },
    {
      "epoch": 2.501756851721715,
      "grad_norm": 125.32413482666016,
      "learning_rate": 7.503865073787772e-05,
      "loss": -8.8724,
      "step": 3560
    },
    {
      "epoch": 2.5087842586085736,
      "grad_norm": 154.8121337890625,
      "learning_rate": 7.496837666900914e-05,
      "loss": -8.7037,
      "step": 3570
    },
    {
      "epoch": 2.5158116654954323,
      "grad_norm": 168.73098754882812,
      "learning_rate": 7.489810260014056e-05,
      "loss": -8.7687,
      "step": 3580
    },
    {
      "epoch": 2.5228390723822907,
      "grad_norm": 484.3903503417969,
      "learning_rate": 7.482782853127196e-05,
      "loss": -8.7116,
      "step": 3590
    },
    {
      "epoch": 2.52986647926915,
      "grad_norm": 130.0102996826172,
      "learning_rate": 7.475755446240337e-05,
      "loss": -8.729,
      "step": 3600
    },
    {
      "epoch": 2.536893886156008,
      "grad_norm": 301.7633056640625,
      "learning_rate": 7.468728039353479e-05,
      "loss": -8.6386,
      "step": 3610
    },
    {
      "epoch": 2.5439212930428674,
      "grad_norm": 70.4582290649414,
      "learning_rate": 7.461700632466621e-05,
      "loss": -8.9356,
      "step": 3620
    },
    {
      "epoch": 2.5509486999297257,
      "grad_norm": 224.5386199951172,
      "learning_rate": 7.454673225579761e-05,
      "loss": -8.8566,
      "step": 3630
    },
    {
      "epoch": 2.557976106816585,
      "grad_norm": 175.39588928222656,
      "learning_rate": 7.447645818692903e-05,
      "loss": -8.7781,
      "step": 3640
    },
    {
      "epoch": 2.5650035137034433,
      "grad_norm": 44.10871505737305,
      "learning_rate": 7.440618411806044e-05,
      "loss": -8.8705,
      "step": 3650
    },
    {
      "epoch": 2.572030920590302,
      "grad_norm": 364.081298828125,
      "learning_rate": 7.433591004919185e-05,
      "loss": -8.1116,
      "step": 3660
    },
    {
      "epoch": 2.579058327477161,
      "grad_norm": 193.93408203125,
      "learning_rate": 7.426563598032326e-05,
      "loss": -8.8242,
      "step": 3670
    },
    {
      "epoch": 2.5860857343640196,
      "grad_norm": 145.48922729492188,
      "learning_rate": 7.419536191145468e-05,
      "loss": -8.8194,
      "step": 3680
    },
    {
      "epoch": 2.5931131412508783,
      "grad_norm": 151.64952087402344,
      "learning_rate": 7.41250878425861e-05,
      "loss": -8.8997,
      "step": 3690
    },
    {
      "epoch": 2.600140548137737,
      "grad_norm": 73.07112884521484,
      "learning_rate": 7.40548137737175e-05,
      "loss": -8.8681,
      "step": 3700
    },
    {
      "epoch": 2.607167955024596,
      "grad_norm": 201.4570770263672,
      "learning_rate": 7.398453970484892e-05,
      "loss": -8.7974,
      "step": 3710
    },
    {
      "epoch": 2.6141953619114546,
      "grad_norm": 71.04912567138672,
      "learning_rate": 7.391426563598033e-05,
      "loss": -8.9505,
      "step": 3720
    },
    {
      "epoch": 2.6212227687983134,
      "grad_norm": 150.9524688720703,
      "learning_rate": 7.384399156711173e-05,
      "loss": -8.3037,
      "step": 3730
    },
    {
      "epoch": 2.628250175685172,
      "grad_norm": 132.1707000732422,
      "learning_rate": 7.377371749824315e-05,
      "loss": -8.9171,
      "step": 3740
    },
    {
      "epoch": 2.635277582572031,
      "grad_norm": 71.94610595703125,
      "learning_rate": 7.370344342937457e-05,
      "loss": -8.9627,
      "step": 3750
    },
    {
      "epoch": 2.6423049894588897,
      "grad_norm": 164.2379608154297,
      "learning_rate": 7.363316936050597e-05,
      "loss": -8.2449,
      "step": 3760
    },
    {
      "epoch": 2.6493323963457485,
      "grad_norm": 111.04104614257812,
      "learning_rate": 7.356289529163739e-05,
      "loss": -8.9378,
      "step": 3770
    },
    {
      "epoch": 2.6563598032326072,
      "grad_norm": 211.9732208251953,
      "learning_rate": 7.34926212227688e-05,
      "loss": -8.7267,
      "step": 3780
    },
    {
      "epoch": 2.663387210119466,
      "grad_norm": 95.69869232177734,
      "learning_rate": 7.342234715390022e-05,
      "loss": -8.8451,
      "step": 3790
    },
    {
      "epoch": 2.6704146170063248,
      "grad_norm": 212.389892578125,
      "learning_rate": 7.335207308503162e-05,
      "loss": -7.5867,
      "step": 3800
    },
    {
      "epoch": 2.6774420238931835,
      "grad_norm": 213.12350463867188,
      "learning_rate": 7.328179901616304e-05,
      "loss": -8.8605,
      "step": 3810
    },
    {
      "epoch": 2.6844694307800423,
      "grad_norm": 216.517333984375,
      "learning_rate": 7.321152494729446e-05,
      "loss": -8.9308,
      "step": 3820
    },
    {
      "epoch": 2.691496837666901,
      "grad_norm": 402.3065185546875,
      "learning_rate": 7.314125087842586e-05,
      "loss": -8.9093,
      "step": 3830
    },
    {
      "epoch": 2.6985242445537594,
      "grad_norm": 171.08599853515625,
      "learning_rate": 7.307097680955728e-05,
      "loss": -8.7313,
      "step": 3840
    },
    {
      "epoch": 2.7055516514406186,
      "grad_norm": 289.0184326171875,
      "learning_rate": 7.300070274068869e-05,
      "loss": -8.7891,
      "step": 3850
    },
    {
      "epoch": 2.712579058327477,
      "grad_norm": 374.5928955078125,
      "learning_rate": 7.293042867182011e-05,
      "loss": -8.8576,
      "step": 3860
    },
    {
      "epoch": 2.719606465214336,
      "grad_norm": 57.69321823120117,
      "learning_rate": 7.286015460295151e-05,
      "loss": -8.9739,
      "step": 3870
    },
    {
      "epoch": 2.7266338721011945,
      "grad_norm": 106.99384307861328,
      "learning_rate": 7.278988053408293e-05,
      "loss": -9.0261,
      "step": 3880
    },
    {
      "epoch": 2.7336612789880537,
      "grad_norm": 193.44566345214844,
      "learning_rate": 7.271960646521434e-05,
      "loss": -9.0087,
      "step": 3890
    },
    {
      "epoch": 2.740688685874912,
      "grad_norm": 107.8094482421875,
      "learning_rate": 7.264933239634575e-05,
      "loss": -8.9441,
      "step": 3900
    },
    {
      "epoch": 2.7477160927617708,
      "grad_norm": 341.1994934082031,
      "learning_rate": 7.257905832747716e-05,
      "loss": -8.9598,
      "step": 3910
    },
    {
      "epoch": 2.7547434996486295,
      "grad_norm": 89.58358764648438,
      "learning_rate": 7.250878425860858e-05,
      "loss": -8.9766,
      "step": 3920
    },
    {
      "epoch": 2.7617709065354883,
      "grad_norm": 100.24834442138672,
      "learning_rate": 7.243851018974e-05,
      "loss": -8.99,
      "step": 3930
    },
    {
      "epoch": 2.768798313422347,
      "grad_norm": 259.99322509765625,
      "learning_rate": 7.23682361208714e-05,
      "loss": -8.2665,
      "step": 3940
    },
    {
      "epoch": 2.775825720309206,
      "grad_norm": 250.20652770996094,
      "learning_rate": 7.229796205200282e-05,
      "loss": -8.9787,
      "step": 3950
    },
    {
      "epoch": 2.7828531271960646,
      "grad_norm": 133.3666534423828,
      "learning_rate": 7.222768798313423e-05,
      "loss": -8.9677,
      "step": 3960
    },
    {
      "epoch": 2.7898805340829234,
      "grad_norm": 274.1477355957031,
      "learning_rate": 7.215741391426564e-05,
      "loss": -8.9262,
      "step": 3970
    },
    {
      "epoch": 2.796907940969782,
      "grad_norm": 173.66744995117188,
      "learning_rate": 7.208713984539705e-05,
      "loss": -8.9989,
      "step": 3980
    },
    {
      "epoch": 2.803935347856641,
      "grad_norm": 263.7831726074219,
      "learning_rate": 7.201686577652847e-05,
      "loss": -9.1023,
      "step": 3990
    },
    {
      "epoch": 2.8109627547434997,
      "grad_norm": 174.24913024902344,
      "learning_rate": 7.194659170765987e-05,
      "loss": -9.0321,
      "step": 4000
    },
    {
      "epoch": 2.8179901616303584,
      "grad_norm": 190.3966522216797,
      "learning_rate": 7.187631763879129e-05,
      "loss": -9.0417,
      "step": 4010
    },
    {
      "epoch": 2.825017568517217,
      "grad_norm": 64.34220123291016,
      "learning_rate": 7.18060435699227e-05,
      "loss": -9.1527,
      "step": 4020
    },
    {
      "epoch": 2.832044975404076,
      "grad_norm": 177.2653045654297,
      "learning_rate": 7.173576950105412e-05,
      "loss": -8.8953,
      "step": 4030
    },
    {
      "epoch": 2.8390723822909347,
      "grad_norm": 188.2138214111328,
      "learning_rate": 7.166549543218552e-05,
      "loss": -9.0743,
      "step": 4040
    },
    {
      "epoch": 2.8460997891777935,
      "grad_norm": 272.4510498046875,
      "learning_rate": 7.159522136331694e-05,
      "loss": -9.0598,
      "step": 4050
    },
    {
      "epoch": 2.8531271960646523,
      "grad_norm": 245.04669189453125,
      "learning_rate": 7.152494729444836e-05,
      "loss": -9.0274,
      "step": 4060
    },
    {
      "epoch": 2.860154602951511,
      "grad_norm": 213.5389404296875,
      "learning_rate": 7.145467322557976e-05,
      "loss": -9.0245,
      "step": 4070
    },
    {
      "epoch": 2.86718200983837,
      "grad_norm": 107.598876953125,
      "learning_rate": 7.138439915671118e-05,
      "loss": -9.1295,
      "step": 4080
    },
    {
      "epoch": 2.874209416725228,
      "grad_norm": 102.602294921875,
      "learning_rate": 7.131412508784259e-05,
      "loss": -9.2079,
      "step": 4090
    },
    {
      "epoch": 2.8812368236120873,
      "grad_norm": 291.1586608886719,
      "learning_rate": 7.124385101897401e-05,
      "loss": -8.9937,
      "step": 4100
    },
    {
      "epoch": 2.8882642304989457,
      "grad_norm": 119.49273681640625,
      "learning_rate": 7.117357695010541e-05,
      "loss": -8.9685,
      "step": 4110
    },
    {
      "epoch": 2.895291637385805,
      "grad_norm": 58.415645599365234,
      "learning_rate": 7.110330288123683e-05,
      "loss": -8.3872,
      "step": 4120
    },
    {
      "epoch": 2.902319044272663,
      "grad_norm": 97.31568908691406,
      "learning_rate": 7.103302881236824e-05,
      "loss": -9.1378,
      "step": 4130
    },
    {
      "epoch": 2.9093464511595224,
      "grad_norm": 54.13397216796875,
      "learning_rate": 7.096275474349965e-05,
      "loss": -9.1776,
      "step": 4140
    },
    {
      "epoch": 2.9163738580463807,
      "grad_norm": 165.9150390625,
      "learning_rate": 7.089248067463106e-05,
      "loss": -9.0784,
      "step": 4150
    },
    {
      "epoch": 2.9234012649332395,
      "grad_norm": 78.35604095458984,
      "learning_rate": 7.082220660576248e-05,
      "loss": -9.1522,
      "step": 4160
    },
    {
      "epoch": 2.9304286718200983,
      "grad_norm": 191.75674438476562,
      "learning_rate": 7.07519325368939e-05,
      "loss": -9.2022,
      "step": 4170
    },
    {
      "epoch": 2.937456078706957,
      "grad_norm": 265.3722229003906,
      "learning_rate": 7.06816584680253e-05,
      "loss": -9.1821,
      "step": 4180
    },
    {
      "epoch": 2.944483485593816,
      "grad_norm": 244.09060668945312,
      "learning_rate": 7.061841180604358e-05,
      "loss": -8.8756,
      "step": 4190
    },
    {
      "epoch": 2.9515108924806746,
      "grad_norm": 76.80900573730469,
      "learning_rate": 7.0548137737175e-05,
      "loss": -9.1553,
      "step": 4200
    },
    {
      "epoch": 2.9585382993675333,
      "grad_norm": 58.860477447509766,
      "learning_rate": 7.04778636683064e-05,
      "loss": -9.2119,
      "step": 4210
    },
    {
      "epoch": 2.965565706254392,
      "grad_norm": 167.87155151367188,
      "learning_rate": 7.04075895994378e-05,
      "loss": -9.1215,
      "step": 4220
    },
    {
      "epoch": 2.972593113141251,
      "grad_norm": 97.35565948486328,
      "learning_rate": 7.033731553056923e-05,
      "loss": -9.0707,
      "step": 4230
    },
    {
      "epoch": 2.9796205200281096,
      "grad_norm": 131.60255432128906,
      "learning_rate": 7.026704146170063e-05,
      "loss": -9.2528,
      "step": 4240
    },
    {
      "epoch": 2.9866479269149684,
      "grad_norm": 122.82623291015625,
      "learning_rate": 7.019676739283205e-05,
      "loss": -9.2308,
      "step": 4250
    },
    {
      "epoch": 2.993675333801827,
      "grad_norm": 73.46363067626953,
      "learning_rate": 7.012649332396347e-05,
      "loss": -9.1556,
      "step": 4260
    },
    {
      "epoch": 3.0,
      "eval_runtime": 10.6885,
      "eval_samples_per_second": 64125.392,
      "eval_steps_per_second": 15.718,
      "step": 4269
    },
    {
      "epoch": 3.000702740688686,
      "grad_norm": 317.7818298339844,
      "learning_rate": 7.005621925509487e-05,
      "loss": -9.0161,
      "step": 4270
    },
    {
      "epoch": 3.0077301475755447,
      "grad_norm": 181.48472595214844,
      "learning_rate": 6.998594518622629e-05,
      "loss": -9.1736,
      "step": 4280
    },
    {
      "epoch": 3.0147575544624035,
      "grad_norm": 75.70367431640625,
      "learning_rate": 6.99156711173577e-05,
      "loss": -9.2918,
      "step": 4290
    },
    {
      "epoch": 3.021784961349262,
      "grad_norm": 105.50285339355469,
      "learning_rate": 6.984539704848912e-05,
      "loss": -9.2493,
      "step": 4300
    },
    {
      "epoch": 3.028812368236121,
      "grad_norm": 91.78470611572266,
      "learning_rate": 6.977512297962052e-05,
      "loss": -9.2013,
      "step": 4310
    },
    {
      "epoch": 3.0358397751229798,
      "grad_norm": 90.83464813232422,
      "learning_rate": 6.970484891075194e-05,
      "loss": -9.149,
      "step": 4320
    },
    {
      "epoch": 3.0428671820098385,
      "grad_norm": 104.18504333496094,
      "learning_rate": 6.963457484188335e-05,
      "loss": -9.0372,
      "step": 4330
    },
    {
      "epoch": 3.0498945888966973,
      "grad_norm": 183.42742919921875,
      "learning_rate": 6.956430077301476e-05,
      "loss": -9.1632,
      "step": 4340
    },
    {
      "epoch": 3.056921995783556,
      "grad_norm": 116.27074432373047,
      "learning_rate": 6.949402670414617e-05,
      "loss": -9.1202,
      "step": 4350
    },
    {
      "epoch": 3.063949402670415,
      "grad_norm": 158.91453552246094,
      "learning_rate": 6.942375263527759e-05,
      "loss": -9.0964,
      "step": 4360
    },
    {
      "epoch": 3.0709768095572736,
      "grad_norm": 639.2203369140625,
      "learning_rate": 6.9353478566409e-05,
      "loss": -9.1161,
      "step": 4370
    },
    {
      "epoch": 3.078004216444132,
      "grad_norm": 370.3136291503906,
      "learning_rate": 6.928320449754041e-05,
      "loss": -8.4117,
      "step": 4380
    },
    {
      "epoch": 3.0850316233309907,
      "grad_norm": 409.3277587890625,
      "learning_rate": 6.921293042867181e-05,
      "loss": -9.1271,
      "step": 4390
    },
    {
      "epoch": 3.0920590302178494,
      "grad_norm": 147.8509521484375,
      "learning_rate": 6.914265635980324e-05,
      "loss": -8.4272,
      "step": 4400
    },
    {
      "epoch": 3.099086437104708,
      "grad_norm": 438.5651550292969,
      "learning_rate": 6.907238229093465e-05,
      "loss": -9.196,
      "step": 4410
    },
    {
      "epoch": 3.106113843991567,
      "grad_norm": 157.9298095703125,
      "learning_rate": 6.900210822206606e-05,
      "loss": -9.2044,
      "step": 4420
    },
    {
      "epoch": 3.1131412508784257,
      "grad_norm": 245.4875946044922,
      "learning_rate": 6.893183415319748e-05,
      "loss": -9.1433,
      "step": 4430
    },
    {
      "epoch": 3.1201686577652845,
      "grad_norm": 245.66226196289062,
      "learning_rate": 6.88615600843289e-05,
      "loss": -9.2921,
      "step": 4440
    },
    {
      "epoch": 3.1271960646521433,
      "grad_norm": 272.2756042480469,
      "learning_rate": 6.87912860154603e-05,
      "loss": -9.2115,
      "step": 4450
    },
    {
      "epoch": 3.134223471539002,
      "grad_norm": 82.20806884765625,
      "learning_rate": 6.87210119465917e-05,
      "loss": -9.323,
      "step": 4460
    },
    {
      "epoch": 3.141250878425861,
      "grad_norm": 83.48697662353516,
      "learning_rate": 6.865073787772313e-05,
      "loss": -9.2051,
      "step": 4470
    },
    {
      "epoch": 3.1482782853127196,
      "grad_norm": 299.7193603515625,
      "learning_rate": 6.858046380885453e-05,
      "loss": -9.0876,
      "step": 4480
    },
    {
      "epoch": 3.1553056921995783,
      "grad_norm": 381.7047424316406,
      "learning_rate": 6.851018973998595e-05,
      "loss": -9.2459,
      "step": 4490
    },
    {
      "epoch": 3.162333099086437,
      "grad_norm": 158.24171447753906,
      "learning_rate": 6.843991567111737e-05,
      "loss": -9.2011,
      "step": 4500
    },
    {
      "epoch": 3.169360505973296,
      "grad_norm": 195.14112854003906,
      "learning_rate": 6.836964160224877e-05,
      "loss": -9.2597,
      "step": 4510
    },
    {
      "epoch": 3.1763879128601546,
      "grad_norm": 198.4600372314453,
      "learning_rate": 6.829936753338019e-05,
      "loss": -9.2076,
      "step": 4520
    },
    {
      "epoch": 3.1834153197470134,
      "grad_norm": 143.38113403320312,
      "learning_rate": 6.822909346451159e-05,
      "loss": -9.3106,
      "step": 4530
    },
    {
      "epoch": 3.190442726633872,
      "grad_norm": 175.71200561523438,
      "learning_rate": 6.815881939564302e-05,
      "loss": -9.3381,
      "step": 4540
    },
    {
      "epoch": 3.197470133520731,
      "grad_norm": 96.3656005859375,
      "learning_rate": 6.808854532677442e-05,
      "loss": -9.2327,
      "step": 4550
    },
    {
      "epoch": 3.2044975404075897,
      "grad_norm": 252.98036193847656,
      "learning_rate": 6.801827125790584e-05,
      "loss": -9.317,
      "step": 4560
    },
    {
      "epoch": 3.2115249472944485,
      "grad_norm": 103.48003387451172,
      "learning_rate": 6.794799718903725e-05,
      "loss": -9.2569,
      "step": 4570
    },
    {
      "epoch": 3.2185523541813073,
      "grad_norm": 288.8014831542969,
      "learning_rate": 6.787772312016866e-05,
      "loss": -9.3468,
      "step": 4580
    },
    {
      "epoch": 3.225579761068166,
      "grad_norm": 172.71327209472656,
      "learning_rate": 6.780744905130007e-05,
      "loss": -9.2644,
      "step": 4590
    },
    {
      "epoch": 3.232607167955025,
      "grad_norm": 235.83187866210938,
      "learning_rate": 6.773717498243149e-05,
      "loss": -9.188,
      "step": 4600
    },
    {
      "epoch": 3.2396345748418836,
      "grad_norm": 167.00633239746094,
      "learning_rate": 6.766690091356291e-05,
      "loss": -9.1988,
      "step": 4610
    },
    {
      "epoch": 3.2466619817287423,
      "grad_norm": 136.1014862060547,
      "learning_rate": 6.759662684469431e-05,
      "loss": -9.2114,
      "step": 4620
    },
    {
      "epoch": 3.2536893886156006,
      "grad_norm": 49.25227737426758,
      "learning_rate": 6.752635277582571e-05,
      "loss": -9.2787,
      "step": 4630
    },
    {
      "epoch": 3.2607167955024594,
      "grad_norm": 187.51524353027344,
      "learning_rate": 6.745607870695714e-05,
      "loss": -9.2623,
      "step": 4640
    },
    {
      "epoch": 3.267744202389318,
      "grad_norm": 122.34105682373047,
      "learning_rate": 6.738580463808855e-05,
      "loss": -8.7257,
      "step": 4650
    },
    {
      "epoch": 3.274771609276177,
      "grad_norm": 152.95921325683594,
      "learning_rate": 6.731553056921996e-05,
      "loss": -8.61,
      "step": 4660
    },
    {
      "epoch": 3.2817990161630357,
      "grad_norm": 138.380126953125,
      "learning_rate": 6.724525650035138e-05,
      "loss": -9.2861,
      "step": 4670
    },
    {
      "epoch": 3.2888264230498945,
      "grad_norm": 49.7448616027832,
      "learning_rate": 6.71749824314828e-05,
      "loss": -9.2808,
      "step": 4680
    },
    {
      "epoch": 3.2958538299367532,
      "grad_norm": 92.5799560546875,
      "learning_rate": 6.71047083626142e-05,
      "loss": -9.2037,
      "step": 4690
    },
    {
      "epoch": 3.302881236823612,
      "grad_norm": 459.4388122558594,
      "learning_rate": 6.70344342937456e-05,
      "loss": -9.3043,
      "step": 4700
    },
    {
      "epoch": 3.3099086437104708,
      "grad_norm": 269.48260498046875,
      "learning_rate": 6.696416022487703e-05,
      "loss": -9.2566,
      "step": 4710
    },
    {
      "epoch": 3.3169360505973295,
      "grad_norm": 139.7371063232422,
      "learning_rate": 6.689388615600843e-05,
      "loss": -9.3661,
      "step": 4720
    },
    {
      "epoch": 3.3239634574841883,
      "grad_norm": 201.1990966796875,
      "learning_rate": 6.682361208713985e-05,
      "loss": -9.2945,
      "step": 4730
    },
    {
      "epoch": 3.330990864371047,
      "grad_norm": 175.8166046142578,
      "learning_rate": 6.675333801827127e-05,
      "loss": -8.7199,
      "step": 4740
    },
    {
      "epoch": 3.338018271257906,
      "grad_norm": 99.28744506835938,
      "learning_rate": 6.668306394940267e-05,
      "loss": -9.4768,
      "step": 4750
    },
    {
      "epoch": 3.3450456781447646,
      "grad_norm": 207.19703674316406,
      "learning_rate": 6.661278988053409e-05,
      "loss": -9.2613,
      "step": 4760
    },
    {
      "epoch": 3.3520730850316234,
      "grad_norm": 401.6954345703125,
      "learning_rate": 6.654251581166549e-05,
      "loss": -9.3727,
      "step": 4770
    },
    {
      "epoch": 3.359100491918482,
      "grad_norm": 271.2758483886719,
      "learning_rate": 6.647224174279692e-05,
      "loss": -9.4398,
      "step": 4780
    },
    {
      "epoch": 3.366127898805341,
      "grad_norm": 143.84329223632812,
      "learning_rate": 6.640196767392832e-05,
      "loss": -9.337,
      "step": 4790
    },
    {
      "epoch": 3.3731553056921997,
      "grad_norm": 275.751708984375,
      "learning_rate": 6.633169360505974e-05,
      "loss": -9.326,
      "step": 4800
    },
    {
      "epoch": 3.3801827125790584,
      "grad_norm": 216.58729553222656,
      "learning_rate": 6.626141953619116e-05,
      "loss": -9.2935,
      "step": 4810
    },
    {
      "epoch": 3.387210119465917,
      "grad_norm": 328.886962890625,
      "learning_rate": 6.619114546732256e-05,
      "loss": -9.2555,
      "step": 4820
    },
    {
      "epoch": 3.394237526352776,
      "grad_norm": 309.67059326171875,
      "learning_rate": 6.612087139845397e-05,
      "loss": -9.4679,
      "step": 4830
    },
    {
      "epoch": 3.4012649332396347,
      "grad_norm": 386.97467041015625,
      "learning_rate": 6.605059732958538e-05,
      "loss": -9.3766,
      "step": 4840
    },
    {
      "epoch": 3.4082923401264935,
      "grad_norm": 88.3870620727539,
      "learning_rate": 6.598032326071681e-05,
      "loss": -9.2959,
      "step": 4850
    },
    {
      "epoch": 3.415319747013352,
      "grad_norm": 80.30619812011719,
      "learning_rate": 6.591004919184821e-05,
      "loss": -9.2769,
      "step": 4860
    },
    {
      "epoch": 3.422347153900211,
      "grad_norm": 206.01023864746094,
      "learning_rate": 6.583977512297961e-05,
      "loss": -9.3775,
      "step": 4870
    },
    {
      "epoch": 3.4293745607870694,
      "grad_norm": 94.6819076538086,
      "learning_rate": 6.576950105411104e-05,
      "loss": -9.5197,
      "step": 4880
    },
    {
      "epoch": 3.436401967673928,
      "grad_norm": 220.16549682617188,
      "learning_rate": 6.569922698524245e-05,
      "loss": -9.405,
      "step": 4890
    },
    {
      "epoch": 3.443429374560787,
      "grad_norm": 323.34552001953125,
      "learning_rate": 6.562895291637386e-05,
      "loss": -9.5317,
      "step": 4900
    },
    {
      "epoch": 3.4504567814476457,
      "grad_norm": 49.12788391113281,
      "learning_rate": 6.555867884750528e-05,
      "loss": -9.3693,
      "step": 4910
    },
    {
      "epoch": 3.4574841883345044,
      "grad_norm": 136.66146850585938,
      "learning_rate": 6.54884047786367e-05,
      "loss": -9.4497,
      "step": 4920
    },
    {
      "epoch": 3.464511595221363,
      "grad_norm": 151.87098693847656,
      "learning_rate": 6.54181307097681e-05,
      "loss": -9.348,
      "step": 4930
    },
    {
      "epoch": 3.471539002108222,
      "grad_norm": 188.59600830078125,
      "learning_rate": 6.53478566408995e-05,
      "loss": -9.4498,
      "step": 4940
    },
    {
      "epoch": 3.4785664089950807,
      "grad_norm": 74.43659973144531,
      "learning_rate": 6.527758257203093e-05,
      "loss": -9.4019,
      "step": 4950
    },
    {
      "epoch": 3.4855938158819395,
      "grad_norm": 317.06658935546875,
      "learning_rate": 6.520730850316233e-05,
      "loss": -9.4068,
      "step": 4960
    },
    {
      "epoch": 3.4926212227687983,
      "grad_norm": 344.8978271484375,
      "learning_rate": 6.513703443429375e-05,
      "loss": -9.4245,
      "step": 4970
    },
    {
      "epoch": 3.499648629655657,
      "grad_norm": 366.8873596191406,
      "learning_rate": 6.506676036542517e-05,
      "loss": -9.5508,
      "step": 4980
    },
    {
      "epoch": 3.506676036542516,
      "grad_norm": 116.49455261230469,
      "learning_rate": 6.499648629655657e-05,
      "loss": -9.5394,
      "step": 4990
    },
    {
      "epoch": 3.5137034434293746,
      "grad_norm": 504.4307861328125,
      "learning_rate": 6.492621222768799e-05,
      "loss": -8.7171,
      "step": 5000
    },
    {
      "epoch": 3.5207308503162333,
      "grad_norm": 222.0172576904297,
      "learning_rate": 6.485593815881939e-05,
      "loss": -9.5121,
      "step": 5010
    },
    {
      "epoch": 3.527758257203092,
      "grad_norm": 273.6445007324219,
      "learning_rate": 6.478566408995082e-05,
      "loss": -9.4716,
      "step": 5020
    },
    {
      "epoch": 3.534785664089951,
      "grad_norm": 313.3985290527344,
      "learning_rate": 6.471539002108222e-05,
      "loss": -9.392,
      "step": 5030
    },
    {
      "epoch": 3.5418130709768096,
      "grad_norm": 88.73362731933594,
      "learning_rate": 6.464511595221364e-05,
      "loss": -9.4257,
      "step": 5040
    },
    {
      "epoch": 3.5488404778636684,
      "grad_norm": 129.03028869628906,
      "learning_rate": 6.457484188334506e-05,
      "loss": -9.6039,
      "step": 5050
    },
    {
      "epoch": 3.555867884750527,
      "grad_norm": 351.9305725097656,
      "learning_rate": 6.450456781447646e-05,
      "loss": -9.5509,
      "step": 5060
    },
    {
      "epoch": 3.562895291637386,
      "grad_norm": 196.45486450195312,
      "learning_rate": 6.443429374560788e-05,
      "loss": -9.5462,
      "step": 5070
    },
    {
      "epoch": 3.5699226985242447,
      "grad_norm": 97.27666473388672,
      "learning_rate": 6.436401967673928e-05,
      "loss": -9.6162,
      "step": 5080
    },
    {
      "epoch": 3.576950105411103,
      "grad_norm": 124.7396240234375,
      "learning_rate": 6.429374560787071e-05,
      "loss": -9.6372,
      "step": 5090
    },
    {
      "epoch": 3.5839775122979622,
      "grad_norm": 123.9557113647461,
      "learning_rate": 6.422347153900211e-05,
      "loss": -9.3242,
      "step": 5100
    },
    {
      "epoch": 3.5910049191848206,
      "grad_norm": 117.28260803222656,
      "learning_rate": 6.415319747013351e-05,
      "loss": -8.7168,
      "step": 5110
    },
    {
      "epoch": 3.5980323260716798,
      "grad_norm": 101.74256896972656,
      "learning_rate": 6.408292340126494e-05,
      "loss": -9.5337,
      "step": 5120
    },
    {
      "epoch": 3.605059732958538,
      "grad_norm": 77.04703521728516,
      "learning_rate": 6.401264933239635e-05,
      "loss": -9.5764,
      "step": 5130
    },
    {
      "epoch": 3.6120871398453973,
      "grad_norm": 108.3470458984375,
      "learning_rate": 6.394237526352776e-05,
      "loss": -9.5225,
      "step": 5140
    },
    {
      "epoch": 3.6191145467322556,
      "grad_norm": 219.52955627441406,
      "learning_rate": 6.387210119465917e-05,
      "loss": -9.5824,
      "step": 5150
    },
    {
      "epoch": 3.6261419536191144,
      "grad_norm": 152.79986572265625,
      "learning_rate": 6.380182712579058e-05,
      "loss": -9.4746,
      "step": 5160
    },
    {
      "epoch": 3.633169360505973,
      "grad_norm": 97.82615661621094,
      "learning_rate": 6.3731553056922e-05,
      "loss": -9.5639,
      "step": 5170
    },
    {
      "epoch": 3.640196767392832,
      "grad_norm": 178.31402587890625,
      "learning_rate": 6.36612789880534e-05,
      "loss": -9.5452,
      "step": 5180
    },
    {
      "epoch": 3.6472241742796907,
      "grad_norm": 121.54655456542969,
      "learning_rate": 6.359100491918483e-05,
      "loss": -9.6187,
      "step": 5190
    },
    {
      "epoch": 3.6542515811665495,
      "grad_norm": 220.9017791748047,
      "learning_rate": 6.352073085031624e-05,
      "loss": -9.6298,
      "step": 5200
    },
    {
      "epoch": 3.6612789880534082,
      "grad_norm": 144.2965545654297,
      "learning_rate": 6.345045678144765e-05,
      "loss": -9.6157,
      "step": 5210
    },
    {
      "epoch": 3.668306394940267,
      "grad_norm": 155.0948486328125,
      "learning_rate": 6.338018271257907e-05,
      "loss": -9.6214,
      "step": 5220
    },
    {
      "epoch": 3.6753338018271258,
      "grad_norm": 163.47679138183594,
      "learning_rate": 6.330990864371047e-05,
      "loss": -9.5478,
      "step": 5230
    },
    {
      "epoch": 3.6823612087139845,
      "grad_norm": 115.3165283203125,
      "learning_rate": 6.323963457484189e-05,
      "loss": -9.6866,
      "step": 5240
    },
    {
      "epoch": 3.6893886156008433,
      "grad_norm": 531.2682495117188,
      "learning_rate": 6.316936050597329e-05,
      "loss": -9.4788,
      "step": 5250
    },
    {
      "epoch": 3.696416022487702,
      "grad_norm": 161.53216552734375,
      "learning_rate": 6.309908643710472e-05,
      "loss": -9.5981,
      "step": 5260
    },
    {
      "epoch": 3.703443429374561,
      "grad_norm": 399.5056457519531,
      "learning_rate": 6.302881236823612e-05,
      "loss": -9.4757,
      "step": 5270
    },
    {
      "epoch": 3.7104708362614196,
      "grad_norm": 213.628173828125,
      "learning_rate": 6.295853829936753e-05,
      "loss": -9.6372,
      "step": 5280
    },
    {
      "epoch": 3.7174982431482784,
      "grad_norm": 174.0318603515625,
      "learning_rate": 6.288826423049896e-05,
      "loss": -9.7072,
      "step": 5290
    },
    {
      "epoch": 3.724525650035137,
      "grad_norm": 93.0772933959961,
      "learning_rate": 6.281799016163036e-05,
      "loss": -9.5237,
      "step": 5300
    },
    {
      "epoch": 3.731553056921996,
      "grad_norm": 177.47023010253906,
      "learning_rate": 6.274771609276178e-05,
      "loss": -9.5806,
      "step": 5310
    },
    {
      "epoch": 3.7385804638088547,
      "grad_norm": 144.05809020996094,
      "learning_rate": 6.267744202389318e-05,
      "loss": -9.6725,
      "step": 5320
    },
    {
      "epoch": 3.7456078706957134,
      "grad_norm": 205.9955596923828,
      "learning_rate": 6.260716795502461e-05,
      "loss": -9.6034,
      "step": 5330
    },
    {
      "epoch": 3.7526352775825718,
      "grad_norm": 265.2481689453125,
      "learning_rate": 6.253689388615601e-05,
      "loss": -9.6457,
      "step": 5340
    },
    {
      "epoch": 3.759662684469431,
      "grad_norm": 131.41285705566406,
      "learning_rate": 6.246661981728741e-05,
      "loss": -9.7314,
      "step": 5350
    },
    {
      "epoch": 3.7666900913562893,
      "grad_norm": 252.281982421875,
      "learning_rate": 6.239634574841884e-05,
      "loss": -8.8322,
      "step": 5360
    },
    {
      "epoch": 3.7737174982431485,
      "grad_norm": 185.1199188232422,
      "learning_rate": 6.232607167955025e-05,
      "loss": -9.673,
      "step": 5370
    },
    {
      "epoch": 3.780744905130007,
      "grad_norm": 161.00546264648438,
      "learning_rate": 6.225579761068166e-05,
      "loss": -9.7307,
      "step": 5380
    },
    {
      "epoch": 3.787772312016866,
      "grad_norm": 148.48870849609375,
      "learning_rate": 6.218552354181307e-05,
      "loss": -9.5964,
      "step": 5390
    },
    {
      "epoch": 3.7947997189037244,
      "grad_norm": 98.59886932373047,
      "learning_rate": 6.211524947294448e-05,
      "loss": -9.7042,
      "step": 5400
    },
    {
      "epoch": 3.801827125790583,
      "grad_norm": 44.2283821105957,
      "learning_rate": 6.20449754040759e-05,
      "loss": -9.7054,
      "step": 5410
    },
    {
      "epoch": 3.808854532677442,
      "grad_norm": 376.626708984375,
      "learning_rate": 6.19747013352073e-05,
      "loss": -9.5873,
      "step": 5420
    },
    {
      "epoch": 3.8158819395643007,
      "grad_norm": 392.6363830566406,
      "learning_rate": 6.190442726633873e-05,
      "loss": -9.5889,
      "step": 5430
    },
    {
      "epoch": 3.8229093464511594,
      "grad_norm": 147.71983337402344,
      "learning_rate": 6.183415319747014e-05,
      "loss": -9.6535,
      "step": 5440
    },
    {
      "epoch": 3.829936753338018,
      "grad_norm": 105.8662338256836,
      "learning_rate": 6.176387912860155e-05,
      "loss": -9.6288,
      "step": 5450
    },
    {
      "epoch": 3.836964160224877,
      "grad_norm": 98.40209197998047,
      "learning_rate": 6.169360505973296e-05,
      "loss": -9.0204,
      "step": 5460
    },
    {
      "epoch": 3.8439915671117357,
      "grad_norm": 76.27021789550781,
      "learning_rate": 6.162333099086437e-05,
      "loss": -9.6059,
      "step": 5470
    },
    {
      "epoch": 3.8510189739985945,
      "grad_norm": 225.92852783203125,
      "learning_rate": 6.155305692199579e-05,
      "loss": -9.7225,
      "step": 5480
    },
    {
      "epoch": 3.8580463808854533,
      "grad_norm": 88.95600891113281,
      "learning_rate": 6.148278285312719e-05,
      "loss": -9.6925,
      "step": 5490
    },
    {
      "epoch": 3.865073787772312,
      "grad_norm": 88.00982666015625,
      "learning_rate": 6.141250878425862e-05,
      "loss": -9.6889,
      "step": 5500
    },
    {
      "epoch": 3.872101194659171,
      "grad_norm": 113.57298278808594,
      "learning_rate": 6.134223471539002e-05,
      "loss": -9.7592,
      "step": 5510
    },
    {
      "epoch": 3.8791286015460296,
      "grad_norm": 324.0041809082031,
      "learning_rate": 6.127196064652143e-05,
      "loss": -9.6139,
      "step": 5520
    },
    {
      "epoch": 3.8861560084328883,
      "grad_norm": 98.5216064453125,
      "learning_rate": 6.120168657765286e-05,
      "loss": -9.7647,
      "step": 5530
    },
    {
      "epoch": 3.893183415319747,
      "grad_norm": 132.6011199951172,
      "learning_rate": 6.113141250878426e-05,
      "loss": -9.7655,
      "step": 5540
    },
    {
      "epoch": 3.900210822206606,
      "grad_norm": 266.7753601074219,
      "learning_rate": 6.106113843991568e-05,
      "loss": -9.7442,
      "step": 5550
    },
    {
      "epoch": 3.9072382290934646,
      "grad_norm": 168.3669891357422,
      "learning_rate": 6.0990864371047086e-05,
      "loss": -9.8447,
      "step": 5560
    },
    {
      "epoch": 3.9142656359803234,
      "grad_norm": 402.8240966796875,
      "learning_rate": 6.09205903021785e-05,
      "loss": -9.7814,
      "step": 5570
    },
    {
      "epoch": 3.921293042867182,
      "grad_norm": 417.2120361328125,
      "learning_rate": 6.085031623330991e-05,
      "loss": -9.0395,
      "step": 5580
    },
    {
      "epoch": 3.9283204497540405,
      "grad_norm": 257.4863586425781,
      "learning_rate": 6.078004216444132e-05,
      "loss": -9.726,
      "step": 5590
    },
    {
      "epoch": 3.9353478566408997,
      "grad_norm": 262.6522521972656,
      "learning_rate": 6.070976809557274e-05,
      "loss": -9.714,
      "step": 5600
    },
    {
      "epoch": 3.942375263527758,
      "grad_norm": 238.28512573242188,
      "learning_rate": 6.063949402670415e-05,
      "loss": -9.8476,
      "step": 5610
    },
    {
      "epoch": 3.9494026704146172,
      "grad_norm": 408.6724853515625,
      "learning_rate": 6.0569219957835565e-05,
      "loss": -9.7787,
      "step": 5620
    },
    {
      "epoch": 3.9564300773014756,
      "grad_norm": 239.36618041992188,
      "learning_rate": 6.0498945888966974e-05,
      "loss": -9.0823,
      "step": 5630
    },
    {
      "epoch": 3.9634574841883348,
      "grad_norm": 98.94427490234375,
      "learning_rate": 6.0428671820098384e-05,
      "loss": -9.7042,
      "step": 5640
    },
    {
      "epoch": 3.970484891075193,
      "grad_norm": 269.2579650878906,
      "learning_rate": 6.03583977512298e-05,
      "loss": -9.1015,
      "step": 5650
    },
    {
      "epoch": 3.977512297962052,
      "grad_norm": 115.36185455322266,
      "learning_rate": 6.028812368236121e-05,
      "loss": -9.7403,
      "step": 5660
    },
    {
      "epoch": 3.9845397048489106,
      "grad_norm": 142.47499084472656,
      "learning_rate": 6.021784961349263e-05,
      "loss": -9.0063,
      "step": 5670
    },
    {
      "epoch": 3.9915671117357694,
      "grad_norm": 179.82542419433594,
      "learning_rate": 6.0147575544624036e-05,
      "loss": -9.8412,
      "step": 5680
    },
    {
      "epoch": 3.998594518622628,
      "grad_norm": 155.80848693847656,
      "learning_rate": 6.007730147575545e-05,
      "loss": -9.8168,
      "step": 5690
    },
    {
      "epoch": 4.0,
      "eval_runtime": 10.6758,
      "eval_samples_per_second": 64201.45,
      "eval_steps_per_second": 15.736,
      "step": 5692
    }
  ],
  "logging_steps": 10,
  "max_steps": 14230,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2198122548592640.0,
  "train_batch_size": 1024,
  "trial_name": null,
  "trial_params": null
}
